<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DevOps on Funky Si's Blog</title><link>https://www.funkysi1701.com/tags/devops/</link><description>Recent content in DevOps on Funky Si's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Mon, 10 Jan 2022 20:00:45 +0000</lastBuildDate><atom:link href="https://www.funkysi1701.com/tags/devops/index.xml" rel="self" type="application/rss+xml"/><item><title>Using GitHub Actions</title><link>https://www.funkysi1701.com/posts/2022/using-github-actions/</link><pubDate>Mon, 10 Jan 2022 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2022/using-github-actions/</guid><description>&lt;p>I&amp;rsquo;ve been running my website on Azure Static Web Apps for a while and it is pretty cool.&lt;/p>
&lt;p>When you create a Static Web App on Azure you get asked for the github repo of your source code and even the branch to use.
&lt;img class="img-fluid" alt="GitHub Repo for my Static Web App" src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/off7ur2tgsla4smkrhhi.png" loading="lazy"
/>
&lt;/p>
&lt;p>Once you have selected this, you get asked for the type of code to deploy, mine is Blazor Web Assembly but you can use Angular, React or Vue.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="GitHub Actions workflow creation" src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ruhzjeujgl1yjxx5lng8.png" loading="lazy"
/>
You now have three variables to fill in the location in your code of the Website, the location of your Azure Functions and the output location usually wwwroot. Once you have set these three you can preview the GitHub Actions file that will be created and added to your repository.&lt;/p>
&lt;p>I get something like this&lt;/p>
&lt;pre tabindex="0">&lt;code>name: Azure Static Web Apps CI/CD
on:
push:
branches:
- feature/tempbranch
pull_request:
types: [opened, synchronize, reopened, closed]
branches:
- feature/tempbranch
jobs:
build_and_deploy_job:
if: github.event_name == &amp;#39;push&amp;#39; || (github.event_name == &amp;#39;pull_request&amp;#39; &amp;amp;&amp;amp; github.event.action != &amp;#39;closed&amp;#39;)
runs-on: ubuntu-latest
name: Build and Deploy Job
steps:
- uses: actions/checkout@v2
with:
submodules: true
- name: Build And Deploy
id: builddeploy
uses: Azure/static-web-apps-deploy@v1
with:
azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_&amp;lt;GENERATED_HOSTNAME&amp;gt; }}
repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)
action: &amp;#34;upload&amp;#34;
###### Repository/Build Configurations - These values can be configured to match your app requirements. ######
# For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig
app_location: &amp;#34;Client&amp;#34; # App source code path
api_location: &amp;#34;Api&amp;#34; # Api source code path - optional
output_location: &amp;#34;wwwroot&amp;#34; # Built app content directory - optional
###### End of Repository/Build Configurations ######
close_pull_request_job:
if: github.event_name == &amp;#39;pull_request&amp;#39; &amp;amp;&amp;amp; github.event.action == &amp;#39;closed&amp;#39;
runs-on: ubuntu-latest
name: Close Pull Request Job
steps:
- name: Close Pull Request
id: closepullrequest
uses: Azure/static-web-apps-deploy@v1
with:
azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_&amp;lt;GENERATED_HOSTNAME&amp;gt; }}
action: &amp;#34;close&amp;#34;
&lt;/code>&lt;/pre>&lt;p>This github action will run when you create a Pull Request to the branch mentioned in the file, or if you push code into the branch. This code get added into the .github/workflows/ folder and is the location that all github action workflows live.&lt;/p>
&lt;p>I haven&amp;rsquo;t done much with github actions, however I have used Azure DevOps quite a bit. Over on the Azure DevOps side I have created a pipeline that deploys to a Dev environment, then a Test environment and finally a production environment.&lt;/p>
&lt;p>Lets have a look at the workflow that I ended up with and with can break down how it all works. Note I am new to Github actions so if there is a better way of doing this do let me know.&lt;/p>
&lt;pre tabindex="0">&lt;code>name: Azure Static Web Apps
on:
push:
branches:
- main
- develop
- feature/*
jobs:
dev:
runs-on: ubuntu-latest
environment:
name: Dev
name: Dev
steps:
- uses: actions/checkout@v2
with:
submodules: true
- name: Build And Deploy
id: builddeploy
uses: Azure/static-web-apps-deploy@v1
with:
azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_ORANGE_POND_09B18B903 }}
repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)
action: &amp;#34;upload&amp;#34;
###### Repository/Build Configurations - These values can be configured to match your app requirements. ######
# For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig
app_location: &amp;#34;Blog&amp;#34; # App source code path
api_location: &amp;#34;Blog.Func&amp;#34; # Api source code path - optional
output_location: &amp;#34;wwwroot&amp;#34; # Built app content directory - optional
###### End of Repository/Build Configurations ######
test:
if: github.ref == &amp;#39;refs/heads/develop&amp;#39;
runs-on: ubuntu-latest
environment:
name: Test
name: Test
steps:
- uses: actions/checkout@v2
with:
submodules: true
- name: Build And Deploy
id: builddeploy
uses: Azure/static-web-apps-deploy@v1
with:
azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_WITTY_DUNE_0A1A77903 }}
repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)
action: &amp;#34;upload&amp;#34;
###### Repository/Build Configurations - These values can be configured to match your app requirements. ######
# For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig
app_location: &amp;#34;Blog&amp;#34; # App source code path
api_location: &amp;#34;Blog.Func&amp;#34; # Api source code path - optional
output_location: &amp;#34;wwwroot&amp;#34; # Built app content directory - optional
###### End of Repository/Build Configurations ######
prod:
if: github.ref == &amp;#39;refs/heads/main&amp;#39;
runs-on: ubuntu-latest
environment:
name: Prod
name: Prod
steps:
- uses: actions/checkout@v2
with:
submodules: true
- name: Build And Deploy
id: builddeploy
uses: Azure/static-web-apps-deploy@v1
with:
azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_BRAVE_ROCK_0AAC63D03 }}
repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)
action: &amp;#34;upload&amp;#34;
###### Repository/Build Configurations - These values can be configured to match your app requirements. ######
# For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig
app_location: &amp;#34;Blog&amp;#34; # App source code path
api_location: &amp;#34;Blog.Func&amp;#34; # Api source code path - optional
output_location: &amp;#34;wwwroot&amp;#34; # Built app content directory - optional
###### End of Repository/Build Configurations ######
&lt;/code>&lt;/pre>&lt;p>The first thing I did was create three Azure Static Web Apps, I am using the free tier so while this is trippling my costs it is all still free! Doing this created three github action workflow files, I deleted two and edited the third, but before I deleted them I made a note of the AZURE_STATIC_WEB_APPS_API_TOKEN. If you look in your settings -&amp;gt; secrets for your repo you will see secrets have been created, this is the secure token that github uses to update your static web app.&lt;/p>
&lt;p>While we are in settings we might as well look at environments. I created a Prod, Test and Dev environment that I was going to use in my github actions.&lt;/p>
&lt;p>Environments can have various rules setup on them.&lt;/p>
&lt;ul>
&lt;li>Required reviewers - this is like an approver, a user specified here must aprove for the workflow to be deployed&lt;/li>
&lt;li>Wait time - I didn&amp;rsquo;t use this, but it looks like a certain amount of time can be set to pause the deployment. (I assume to do some kind of manual check)&lt;/li>
&lt;li>Deployment Branch - specify what branch are allowed to be deployed to what environments. I specified develop, main and feature branches could be deployed to the Dev environment, develop and main could go on Test and main could go on Prod&lt;/li>
&lt;li>Environment secrets - I didn&amp;rsquo;t use this as my secrets were already created, however it looks like your secrets can be associated with a specific environment&lt;/li>
&lt;/ul>
&lt;p>Now that we have the static web apps setup and the environments lets look at the github action file.&lt;/p>
&lt;p>First of all I removed the PR stuff and just concentrated on pushes. I wanted my workflow to be.&lt;/p>
&lt;ol>
&lt;li>Push to feature branch&lt;/li>
&lt;li>Deploys to Dev env&lt;/li>
&lt;li>PR feature branch to develop&lt;/li>
&lt;li>Once merged code gets pushed into develop&lt;/li>
&lt;li>Deploys to Test env&lt;/li>
&lt;li>PR develop to main&lt;/li>
&lt;li>Once merged code gets pushed into main&lt;/li>
&lt;li>Deploys to Prod env (after approval)&lt;/li>
&lt;/ol>
&lt;p>The approval on deploying to production I think is probably overkill, but I still have it setup like that for now.&lt;/p>
&lt;p>My gh action has three jobs defined as dev: test: and prod: they are all the same except they have the azure_static_web_apps_api_token that is correct for their environment.&lt;/p>
&lt;p>They also each have a environment defined eg&lt;/p>
&lt;pre tabindex="0">&lt;code>environment:
name: Prod
&lt;/code>&lt;/pre>&lt;p>Lastly Test and Prod have an if test setup, if the test is false the job won&amp;rsquo;t run. Importantly it won&amp;rsquo;t fail it just won&amp;rsquo;t run.&lt;/p>
&lt;p>For Prod this needs to only run on main branch so we have&lt;/p>
&lt;p>if: github.ref == &amp;lsquo;refs/heads/main&amp;rsquo;&lt;/p>
&lt;p>For Test this needs to only run on develop so&lt;/p>
&lt;p>if: github.ref == &amp;lsquo;refs/heads/develop&amp;rsquo;&lt;/p>
&lt;p>I could have a test for develop to only run on feature/* but I have allowed it to run everytime.&lt;/p>
&lt;p>There is loads more you can do with github actions, but hopefully this gives you a taste of some of the things you can do. I currently have a mix of Azure DevOps and github actions so I will be working on getting github actions to do more.&lt;/p></description></item><item><title>Azure DevOps Release Pipelines Pre and Post Approval</title><link>https://www.funkysi1701.com/posts/2021/azure-devops-release-pipelines-pre-and-post-approval/</link><pubDate>Sun, 14 Feb 2021 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2021/azure-devops-release-pipelines-pre-and-post-approval/</guid><description>&lt;p>Azure DevOps release pipelines have lots of options to do things how you want. One of my favourites is the option for approval.&lt;/p>
&lt;p>There are two ways you can do approvals Pre and Post deployment. Lets look at both.&lt;/p>
&lt;h2 id="pre-deployment-approval">Pre Deployment Approval&lt;a class="anchor ms-1" href="#pre-deployment-approval">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>&lt;img class="img-fluid" alt="image" src="https://dev-to-uploads.s3.amazonaws.com/i/9k6vo6pfv434u7yq3mt4.png" loading="lazy"
/>
&lt;/p>
&lt;p>Lets imagine you have a simple deployment pipeline that deploys to a test/development environment before deploying to a production environment.&lt;/p>
&lt;p>Pre Deployment Approval happens immediately before the release so in this example, click in the ellipse before the Prod release step.&lt;/p>
&lt;p>You will get a screen like the above, you can select what users need to approve it and how long approval waits before timing out, the default is 30 days, but I tend to use a shorter time out of 3 days.&lt;/p>
&lt;h2 id="post-deployment-approval">Post Deployment Approval&lt;a class="anchor ms-1" href="#post-deployment-approval">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>&lt;img class="img-fluid" alt="image" src="https://dev-to-uploads.s3.amazonaws.com/i/reiulrhinzqyyon6mrqi.png" loading="lazy"
/>
&lt;/p>
&lt;p>Post Deployment Approval happens immediately after the release so in this example, click in the circle after the Test release step.&lt;/p>
&lt;p>You will get a screen like the above, with the same settings as before.&lt;/p>
&lt;p>That is pretty much all there is to approvals so either option will prompt you to approve before anything gets deployed to your production environment.&lt;/p>
&lt;h2 id="deployment-hours">Deployment Hours&lt;a class="anchor ms-1" href="#deployment-hours">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>To complicate matters I make use of the following setting to define deployment hours.
&lt;img class="img-fluid" alt="image" src="https://dev-to-uploads.s3.amazonaws.com/i/aku2z0dl3m3xkvfvh7wd.png" loading="lazy"
/>
&lt;/p>
&lt;p>This setting will start the Prod deployment at 3am Mon-Fri.&lt;/p>
&lt;p>If I configure Post Deployment Approval, as soon as my deploy to Test has completed a request for Approval is sent.&lt;/p>
&lt;p>If I configure Pre Deployment Approval, at 3am Mon-Fri a request for Approval is sent (not ideal if you tend to be asleep at 3am)&lt;/p>
&lt;p>So it looks like Post Deployment Approval is more useful for my use case. However if you deny approval either in Pre or Post approval this will mark the deployment as failed and show Red in your list of deployments.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="image" src="https://dev-to-uploads.s3.amazonaws.com/i/vichyb1srgc1ln85hj0o.png" loading="lazy"
/>
&lt;/p>
&lt;p>From a casual glance it looks like the deployment to Test is failing, it isn&amp;rsquo;t I am just opting to not continue my deployment to production.&lt;/p>
&lt;h2 id="my-pipeline">My Pipeline&lt;a class="anchor ms-1" href="#my-pipeline">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>&lt;img class="img-fluid" alt="image" src="https://dev-to-uploads.s3.amazonaws.com/i/9kprp90t59owfmsmqkcp.png" loading="lazy"
/>
&lt;/p>
&lt;p>This is how I have my pipeline setup. Deployment happens on Test and doesn&amp;rsquo;t have a post approval step.&lt;/p>
&lt;p>After Test an empty stage called Approval runs and that has a post deployment approval, this happens immediately after Test so you get asked straight away for approval.&lt;/p>
&lt;p>Prod does not start as I have my deployment hours configured. Once it is time for deployment to Prod to start it executes.&lt;/p>
&lt;p>Now a casual look at my past releases, you can easily see which have been stopped by approval and which have failed due to whatever issue, and which have run all the way through to Prod.&lt;/p>
&lt;p>And deployments to Prod can only ever run during my defined deployment window.&lt;/p>
&lt;p>I am interested to hear how you have your deployment pipeline setup. Do you make use of Pre or Post Approvals? Do you ensure deployments always happen at specific times?&lt;/p></description></item><item><title>Gated Release</title><link>https://www.funkysi1701.com/posts/2019/gated-release/</link><pubDate>Fri, 05 Apr 2019 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2019/gated-release/</guid><description>&lt;p>Automated releases of software are great but how can we add an element of feedback so only good releases go live.&lt;/p>
&lt;p>I have been using Azure DevOps to release my &lt;a href="https://www.funkysi1701.com/pwned-pass/">PwnedPass&lt;/a>
android app to the Google Play Store for a while now. There are options to deploy to the alpha, Beta or Production tracks and even to set % of users to target. For the full range of options check out the Google Play &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vsclient.google-play" target="_blank" rel="noopener noreferrer">extension&lt;/a>
for Azure DevOps.&lt;/p>
&lt;p>My release starts by publishing to 10% of users on the production track, my next step makes use of the increase rollout option to increase this %, you can have as many of these additional steps as you want until you reach 100% of your users.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="Image" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/04/image.png?fit=662%2C116&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>Now if you run this release now it will just run through each of the steps one after the other. Now of course you can add a pre or post approval to your pipeline but this just adds a manual dependency to your release. Whoever does the approving needs to check things are working before approving or worse just approves regardless.&lt;/p>
&lt;p>Azure DevOps has the concept of &lt;a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/release/deploy-using-approvals?view=azure-devops" target="_blank" rel="noopener noreferrer">gated releases&lt;/a>
which allows you to add automated checks before or after a release happens. These automated checks can be any of the following:&lt;/p>
&lt;ul>
&lt;li>An Azure Function&lt;/li>
&lt;li>A Rest API call&lt;/li>
&lt;li>Azure Monitor Alert&lt;/li>
&lt;li>Query Work Items&lt;/li>
&lt;li>Security and Compliance Assessment&lt;/li>
&lt;/ul>
&lt;p>We are going to make use of the Azure Monitor Alert, to create an alert from your Application Insights data and only continue the rollout if no failures are detected.&lt;/p>
&lt;p>Open up your application insights resource in the Azure portal and look in alerts. Click add new alert rule.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="Image" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/04/image-1.png?fit=662%2C552&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>Select your application insights resource in Resource, In Condition choose a condition to check, I chose Failed Requests, so every time a failure is registered in my API I can stop the deployment. The exact criteria you want to use is entirely up to you.&lt;/p>
&lt;p>Create an action group, I just set my alert to send an email to myself but there are other alert actions you may want to try. Give your alert a name and description and click save.&lt;/p>
&lt;p>Now all we need to do is make Azure DevOps make use of this alert. In your release pipeline select the pre-deployment conditions of your second step and open up the Gates section.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="Image" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/04/image-2.png?fit=662%2C498&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>Choose a suitable time to evaluate, I have been using something long like 12 or 24 hours so if there are problems there is time for it to be noticed. Choose Version 1 of the task (I was not able to get it to work with Version 0)&lt;/p>
&lt;p>Now select your Azure subscription and Resource Group and leave the rest of the settings as they are. Now your Deployment will stop and analyse application insights for any Failed requests and will halt if it finds any.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="Image" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/04/image-3.png?fit=662%2C88&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>I am still testing this out but it will take a few days to figure out if this what I want due to the large time scales involved. I feel this is going to be an improvement of manually approving release steps.&lt;/p></description></item><item><title>Microsoft Ignite | The Tour – London</title><link>https://www.funkysi1701.com/posts/2019/microsoft-ignite-the-tour-london/</link><pubDate>Tue, 26 Feb 2019 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2019/microsoft-ignite-the-tour-london/</guid><description>&lt;p>I have just spent the first day at the conference Microsoft Ignite | The Tour.
&lt;img class="img-fluid" alt="Alt Text" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/02/D0UTj08XgAEo1YJ.jpg?fit=662%2C440&amp;amp;ssl=1" loading="lazy"
/>
The conference was free I just needed to arrange travel and accommodation. Microsoft have really looked after me and all the other guests, breakfast and lunch has been provided, I got a free t-shirt, coffee (or tea) all day plus a beer or glass of wine to end the day. And that’s before you collect any free stuff the vendors are giving away.&lt;/p>
&lt;p>The first session was Designing resilient cloud applications with &lt;a href="https://twitter.com/CodeMillMatt" target="_blank" rel="noopener noreferrer">Matt Soucoup&lt;/a>
and talked about some cloud technologies like Azure key vault and serving static files from blob storage. Unfortunately this had a few technical issues with the demos. I think it was just connectivity with the MongoDB backend but this slightly spoiled the session. As this was the only technical problem I noticed all day I can let it pass.&lt;/p>
&lt;p>Next was a session on Azure DevOps mainly build and release pipelines called Deploying your application faster and safer with &lt;a href="https://twitter.com/bbenz" target="_blank" rel="noopener noreferrer">Brian Benz&lt;/a>
. A lot of this I knew but good to reinforce I am doing things correctly.&lt;/p>
&lt;p>Next was a session on Application Insights called Detecting application anomalies with Telemetry with Matt Soucoup.&lt;/p>
&lt;p>Probably the most useful session was on Docker and Kubernetes called Integrate containers and Kubernetes into your Azure DevOps build and release model with &lt;a href="https://twitter.com/crad77" target="_blank" rel="noopener noreferrer">Marco De Sanctis&lt;/a>
. Going to spend some time looking through the examples from this session.&lt;/p>
&lt;p>A session on Serverless covered Azure Functions, Azure Logic apps and the other Azure Serverless offerings. Investing in Serverless: less servers, more code with &lt;a href="https://twitter.com/simona_cotin" target="_blank" rel="noopener noreferrer">Simona Cotin&lt;/a>
.&lt;/p>
&lt;p>Lastly was a panel discussion on the changes facing IT Pros and SysAdmins. What is the future of the IT Pro in a DevOps &amp;amp; Cloudy world? with &lt;a href="https://twitter.com/jenstirrup" target="_blank" rel="noopener noreferrer">Jennifer Stirrup&lt;/a>
, &lt;a href="https://twitter.com/bakionur" target="_blank" rel="noopener noreferrer">Baki Onur Okutucu&lt;/a>
, &lt;a href="https://twitter.com/AmyKateNicho" target="_blank" rel="noopener noreferrer">Amy Boyd&lt;/a>
and &lt;a href="https://twitter.com/TheOpsMgr" target="_blank" rel="noopener noreferrer">Stephen Thair&lt;/a>
. Should they learn to code, how should they make sure they keep up.&lt;/p>
&lt;p>Tomorrow I have a loads more sessions, including ones about mental health, Azure pipelines, more Kubernetes stuff and dealing with failure.&lt;/p></description></item><item><title>Yaml Builds on Azure DevOps</title><link>https://www.funkysi1701.com/posts/2019/yaml-builds-on-azure-devops/</link><pubDate>Thu, 31 Jan 2019 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2019/yaml-builds-on-azure-devops/</guid><description>&lt;p>I have been using Azure DevOps (Or VSTS or VSO etc) for a while now and one of the great features is doing automatic builds with every check-in. This is more commonly known as a CI (continuous integration) build.&lt;/p>
&lt;p>More recently I have started playing about with creating my build using YAML files instead of using the web user interface to create my build.&lt;/p>
&lt;h2 id="why">Why?&lt;a class="anchor ms-1" href="#why">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>You may wonder why go to the effort of learning the YAML syntax when you can just create the build in Azure DevOps and then forget about it.&lt;/p>
&lt;p>Mostly it is because the build changes over time and you shouldn’t just forget about it. If something changes over time then you might want to version control it, or look at a previous version.&lt;/p>
&lt;p>Lets say you create a pull request that replaces a .net 4.7 web service with a .net core web service. If you have a CI build this PR will fail because it won’t build. If you change the build first any other builds going on will fail. What you want in this case is the build to be associated with that branch or PR. Any builds before you merge this change in will continue to work and any after this change will also work.&lt;/p>
&lt;h2 id="how">How?&lt;a class="anchor ms-1" href="#how">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>How do you get started with YAML builds? Well the first thing is to make sure that YAML builds are turned on, as I write this I believe they are still a feature you can turn on or off. Have a look in Preview features and make sure they are turned on.&lt;/p>
&lt;p>Next look at any of your existing builds and click the View YAML link. This will show you an example YAML file of your existing build. You could just save this as azure-pipelines.yml and checkin to the root of your project. You can also click the add new build pipeline option, this will give you some templates to start you off.&lt;/p>
&lt;p>The YAML file consists of a series of build steps usually called tasks, with a few settings before to configure things like parameters or build agents. Detailed docs about the syntax of the file can be found &lt;a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&amp;amp;tabs=schema" target="_blank" rel="noopener noreferrer">here&lt;/a>
.&lt;/p>
&lt;p>For my mobile app my YAML files consist of downloading nuget packages, building the solution, building specific projects with desired settings, running powershell or other scripts to set things up and finally publishing the results of the build as artifacts so that they can be used in any releases.&lt;/p>
&lt;h2 id="secure-it">Secure It!&lt;a class="anchor ms-1" href="#secure-it">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>Avoid committing passwords and secure keys into source control. I have found you can upload secure files via Azure DevOps and then add a download secure files step at the start of your build. This allows the secure file to be used during the build but the contents of the file can’t be viewed by anyone with access to the source code.&lt;/p>
&lt;p>I find it often takes a bit of thinking about how to achieve this, but it is usually possible to keep keys and secrets secure.&lt;/p></description></item><item><title>Let’s Encrypt is awesome</title><link>https://www.funkysi1701.com/posts/2018/lets-encrypt-is-awesome/</link><pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2018/lets-encrypt-is-awesome/</guid><description>&lt;p>Let’s Encrypt is a free way to get a SSL certificate onto your website and until recently I had never tried it. It is very easy and I think it is awesome.&lt;/p>
&lt;p>IIS is the web server software the Microsoft include with Windows 10 and Windows Server. I have it installed on my laptop and it displays the default IIS page.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2018/04/iis.jpg?resize=768%2C464&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>It is not really a good idea to host websites on your laptop, use a dedicated web server, or host with a hosting company, however the techniques are the same and it gives me something to write about!&lt;/p>
&lt;p>In order to point a domain name at what IIS on my machine was serving up I did the following:&lt;/p>
&lt;ul>
&lt;li>Do a google search for “whats my IP”, this will return your public IP. Most residential ISPs use dynamic IPs so it may change over time, (which is another reason not to host a website on your laptop!)&lt;/li>
&lt;li>Add an A record on a domain with the IP address you have just got&lt;/li>
&lt;li>Your public IP most likely points at your router not your laptop so enable port forwarding of port 80 and port 443 to the internal IP of your laptop (something like 192.168.0.11 etc)&lt;/li>
&lt;/ul>
&lt;p>Now comes the fun Let’s Encrypt stuff!&lt;/p>
&lt;p>First you need a Let’s Encrypt client, there are a lot of them out there mostly for linux flavours, however a bit of googling found a windows one. Go to &lt;a href="https://github.com/PKISharp/win-acme/releases" target="_blank" rel="noopener noreferrer">https://github.com/PKISharp/win-acme/releases&lt;/a>
and download the zip file and unzip it.&lt;/p>
&lt;p>Run the executable from the zip file and follow the onscreen prompts.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2018/04/letsencrypt.jpg?resize=768%2C480&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>Press N to create a new certificate.&lt;/p>
&lt;p>Then press 1 to bind to single website found in your IIS setup&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2018/04/letsencrypt2.jpg?resize=768%2C686&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>And now magically Let’s Encrypt knows what you have setup in IIS.&lt;/p>
&lt;p>Now all you need to do is enter an email address incase a renewal fails and agree to the let’s encrypt terms and you are all setup.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2018/04/letsencrypt3.jpg?resize=768%2C920&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>How awesome and easy is that for getting your websites working with a SSL certificate. If you have IIS configured on a server, give it a try and you can SSL all your things.&lt;/p></description></item><item><title>DNS for Developers</title><link>https://www.funkysi1701.com/posts/2018/dns-for-developers/</link><pubDate>Mon, 09 Apr 2018 00:00:00 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2018/dns-for-developers/</guid><description>&lt;p>DNS is the backbone of the internet and as such I believe every developer should know something about the basics and not just leave it for the sysadmin to sort.&lt;/p>
&lt;h3 id="what-is-dns">What is DNS?&lt;a class="anchor ms-1" href="#what-is-dns">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>DNS or Domain Name System is what translates Domain names to IP addresses and vice versa.&lt;/p>
&lt;h3 id="wait-what-is-an-ip-address-and-what-are-domain-names">Wait what is an IP address and what are domain names?&lt;a class="anchor ms-1" href="#wait-what-is-an-ip-address-and-what-are-domain-names">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>You do realise this is a developer blog? An IP address is a unique address on the internet and a domain name is a user friendly label for one or more of these.&lt;/p>
&lt;p>An example might be google.com which for me resolves to 216.58.204.14&lt;/p>
&lt;h3 id="how-it-works">How it works&lt;a class="anchor ms-1" href="#how-it-works">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2018/04/dns-rev-1.gif?resize=360%2C320&amp;amp;ssl=1" loading="lazy"
/>
When your browser makes a request to google.com it makes a request to your ISPs DNS Servers. This resolves google.com to 216.58.204.14&lt;/p>
&lt;p>In more detail your ISPs DNS server will forward the DNS query to another DNS server and will cache the results for a set amount of time. This is the TTL or Time To Live. Next time the ISP DNS Server will be able to reply directly without needing to forward requests.&lt;/p>
&lt;p>This forwarding and caching is what makes making a DNS change not instantaneous. The TTL needs to be reached so that no results are still being fetched from the cache of DNS servers across the globe.&lt;/p>
&lt;h3 id="dns-records">DNS Records&lt;a class="anchor ms-1" href="#dns-records">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>Now we know roughly how DNS works let’s look at the most common type of records&lt;/p>
&lt;h4 id="a">A&lt;a class="anchor ms-1" href="#a">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h4>
&lt;p>A (Host) records are the most simple records which translate domain names to IPs&lt;/p>
&lt;p>eg &lt;a href="https://www.google.com" target="_blank" rel="noopener noreferrer">www.google.com&lt;/a>
to 216.58.204.14&lt;/p>
&lt;h4 id="cname">CNAME&lt;a class="anchor ms-1" href="#cname">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h4>
&lt;p>A CNAME (Canonical Name) record is different to an A record in that it maps a domain name to another domain name when no A record exists.&lt;/p>
&lt;p>eg &lt;a href="https://www.google.com" target="_blank" rel="noopener noreferrer">www.google.com&lt;/a>
to somethingelse.google.com&lt;/p>
&lt;p>Typically Azure makes use of CNAMEs for many of its services especially adding a custom domain name&lt;/p>
&lt;h4 id="mx">MX&lt;a class="anchor ms-1" href="#mx">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h4>
&lt;p>MX stands for Mail Exchange and is used for configuring email&lt;/p>
&lt;h4 id="name-server">Name Server&lt;a class="anchor ms-1" href="#name-server">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h4>
&lt;p>Every domain has a number of Name Servers which tells you what servers control the DNS settings for that domain. If you change your Name Servers then the new Name servers will be where you can change your DNS settings.&lt;/p>
&lt;p>If you want to use a service like &lt;a href="https://dnsimple.com/" target="_blank" rel="noopener noreferrer">DNSimple&lt;/a>
instead of &lt;a href="https://www.123-reg.co.uk/" target="_blank" rel="noopener noreferrer">123reg&lt;/a>
or where ever you registered your domain then all you need to do is change your Name servers.&lt;/p>
&lt;h4 id="aaaa">AAAA&lt;a class="anchor ms-1" href="#aaaa">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h4>
&lt;p>Like A record but for ipv6&lt;/p>
&lt;h3 id="what-next">What next?&lt;a class="anchor ms-1" href="#what-next">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>Want to put some different DNS records into practise? Buy a domain name and publish some content to it. Check out my previous post about &lt;a href="https://www.funkysi1701.com/creating-dns-records-programmatically">programmatically adding records&lt;/a>
. Want an SSL certificate? Get a wildcard one and then you can apply it to any subdomain you add to your domain.&lt;/p>
&lt;p>If you have a new website you want to publish consider which of the following is better:&lt;/p>
&lt;p>&lt;a href="https://www.example.com/newsite" target="_blank" rel="noopener noreferrer">https://www.example.com/newsite&lt;/a>
&lt;/p>
&lt;p>&lt;a href="https://newsite.example.com" target="_blank" rel="noopener noreferrer">https://newsite.example.com&lt;/a>
&lt;/p>
&lt;p>I much prefer the second option, it looks cleaner, there is no potential conflict with the parent site, no subfolder issues between production and development.&lt;/p></description></item><item><title>Chrome distrusts SSL Certificates</title><link>https://www.funkysi1701.com/posts/2018/ssl-distrusts/</link><pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2018/ssl-distrusts/</guid><description>&lt;p>One of the websites I have been working on has been displaying an error in the console. The error reads as follows.&lt;/p>
&lt;pre tabindex="0">&lt;code>The SSL certificate used to load resources from https://example.com will be distrusted in M70. Once distrusted, users will be prevented from loading these resources. See https://g.co/chrome/symantecpkicerts for more information.
&lt;/code>&lt;/pre>&lt;p>But what does this mean? Well let’s start by looking at the &lt;a href="https://security.googleblog.com/2017/09/chromes-plan-to-distrust-symantec.html" target="_blank" rel="noopener noreferrer">link&lt;/a>
provided.&lt;/p>
&lt;p>In January 2017 it was revealed that Certificate Authorities run by Symantec which include Thawte, VeriSign, Equifax, GeoTrust, and RapidSSL had been issuing certificates that did not comply with baseline standards.&lt;/p>
&lt;p>Starting with Chrome 66, Google has decided to remove trust for these certificates. Chrome 66 is due for release around 17th April. My error mentions M70 so what does that refer to?&lt;/p>
&lt;p>Chrome 70 which is due to be released in October 2018 will removed the trust for another batch of Symantec certificates.&lt;/p>
&lt;p>If you are getting one of these errors because you are using a certificate that is going to be distrusted what will your site look like in Chrome 66 or Chrome 70?&lt;/p>
&lt;p>Well Chrome 66 is now in the dev channel so we can give it a try. &lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2018/02/tempsnip.png.jpg?resize=662%2C443&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>Not very nice for your users is it? Now is the time to order a new SSL certificate to avoid this happening to your site.&lt;/p>
&lt;p>I first saw this error a few months ago and have been reading up about it and waiting for Chrome 66 to reach the dev channel so I could test what it did to my site. However now that I have Chrome 66 installed I spotted the intranet for the company I work for is also affected. I do not directly work on the intranet so I notified the security team that they may want to look into this.&lt;/p>
&lt;p>Unfortunately the response I received has been that Google needs to fix this before Chrome 66 is released. I am not criticising my employer or the security team, however this isn’t something Google can just “ &lt;strong>fix&lt;/strong> “.&lt;/p>
&lt;p>The certificates issued were issued by a CA that had issues so in order to maintain the trustworthiness of all certificates Google had little choice but to distrust them. Google and security experts need to be making more of a fuss about this and I am joining in on making a fuss by writing this blog. &lt;a href="https://scotthelme.co.uk/are-you-ready-for-the-symantec-distrust/" target="_blank" rel="noopener noreferrer">Scott Helme&lt;/a>
estimates that there are about 7000 websites which may be affected by the M66 and M70 distrusts.&lt;/p></description></item><item><title>Moving files into blob storage</title><link>https://www.funkysi1701.com/posts/2018/moving-blobs-cloud-suppliers/</link><pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2018/moving-blobs-cloud-suppliers/</guid><description>&lt;p>We are in the process of moving our companies websites onto the Azure platform. One of the challenges was to move image files out of the website project into blob storage. This week I have moved 150,000 of them.&lt;/p>
&lt;p>One thing I keep banging on about is that your source code should not contain data. If it does every time you do a deployment you need to consider where these images are located and ensure you don’t overwrite or loose any. It also goes without saying that deployments of a few Mb are a lot quicker than deployments of 100s of Mb.&lt;/p>
&lt;p>Azure blob storage also gives you advantages like distributing storage across multiple datacenters which would be impossible with traditional files on a server.&lt;/p>
&lt;p>So now that we have established that this is a good idea lets look at how we could move large amounts of data. In my case all the filenames are stored in a SQL database so the plan of action was to simply loop through the files in the database, download from current storage (either locally or other cloud storage), upload to Azure and tidy up afterwards. Due to the number of images I am going to update the database and mark when a file has been processed so I can do the move over several days.&lt;/p>
&lt;p>This is my code&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-csharp" data-lang="csharp">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> source = &lt;span style="color:#e6db74">&amp;#34;https://example.com/images/&amp;#34;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> tmp = Server.MapPath(&lt;span style="color:#e6db74">&amp;#34;~/tmp/&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> (!Directory.Exists(tmp))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Directory.CreateDirectory(tmp);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> fixturePhotos = db.Images.Where(x =&amp;gt; x.Moved == &lt;span style="color:#66d9ef">null&lt;/span> || x.Moved == &lt;span style="color:#ae81ff">0&lt;/span>).Take(id);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">foreach&lt;/span> (&lt;span style="color:#66d9ef">var&lt;/span> photo &lt;span style="color:#66d9ef">in&lt;/span> fixturePhotos)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">try&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">string&lt;/span> path = getFilePath(photo.FileName);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> (!Directory.Exists(Server.MapPath(&lt;span style="color:#e6db74">&amp;#34;~/tmp/&amp;#34;&lt;/span> + path)))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Directory.CreateDirectory(Server.MapPath(&lt;span style="color:#e6db74">&amp;#34;~/tmp/&amp;#34;&lt;/span> + path));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> WebClient WebClient = &lt;span style="color:#66d9ef">new&lt;/span> WebClient();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> WebClient.DownloadFile(source + photo.FileName, tmp + photo.FileName);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> FileUploader f = &lt;span style="color:#66d9ef">new&lt;/span> FileUploader(tmp + photo.FileName, photo.FileName);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> System.IO.File.Delete(tmp + photo.FileName);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> photo.Moved = &lt;span style="color:#ae81ff">1&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">catch&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> photo.Moved = &lt;span style="color:#ae81ff">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>db.SaveChanges();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> (Directory.Exists(tmp))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Directory.Delete(tmp, &lt;span style="color:#66d9ef">true&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>First of all I create a tmp folder in the root of my website if it doesn’t exist to store my images temporarily.&lt;/p>
&lt;p>I then use an entity framework model to query the database that haven’t been moved, and I use the take() method to limit how many results I process. (I have been passing in 1000 at a time)&lt;/p>
&lt;p>I then use a foreach loop over all these files to perform the following actions.&lt;/p>
&lt;ol>
&lt;li>Create additional subfolders if the filename variable stored in the database isn’t actually a filename but a filepath, note you will have to split filename and filepath which I haven’t included code for here.&lt;/li>
&lt;li>Download file from the original url and save into the temporary folder&lt;/li>
&lt;li>Upload to Azure&lt;/li>
&lt;li>Delete temporary file&lt;/li>
&lt;li>Update database giving a success or fail&lt;/li>
&lt;/ol>
&lt;p>Once the foreach is finished I commit the database changes and delete the temporary folder. I am sure there must be other ways to do this transfer but this was quick and easy to setup and now I have a copy of all the files in Azure storage so I can test out other issues with my website.&lt;/p>
&lt;p>One last tip about how to schedule this code. I called the above code from a MVC controller and then wrote a Azure Function to call this code on a schedule.&lt;/p></description></item><item><title>Azure Functions</title><link>https://www.funkysi1701.com/posts/2017/azure-functions/</link><pubDate>Tue, 12 Sep 2017 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2017/azure-functions/</guid><description>&lt;p>I recently blogged about using &lt;a href="https://www.funkysi1701.com/posts/using-azure-webjobs-to-automate-stuff">Azure Web Jobs&lt;/a>
, Azure Function is another way of doing the same thing, lets look at how they work.&lt;/p>
&lt;p>(Sorry its been a while since I blogged but I suspect an erratic schedule will continue for the next few months.)&lt;/p>
&lt;p>To create an Azure Function go to the Azure portal and click add new and search for &amp;ldquo;Function App&amp;rdquo;&lt;/p>
&lt;p>Give your app a name and select the usual resource group and location settings.&lt;/p>
&lt;p>Now when you open Function Apps you should see your new app.&lt;/p>
&lt;p>I want my Function App to run on a schedule so I clicked the + next to functions and selected TimerTrigger. I am a c# programmer so I selected this option as well.&lt;/p>
&lt;p>Give your function a name and specify using the usual cron notation how often it should run. I want mine to run at 9.30pm each night so use 0 30 21 * * *&lt;/p>
&lt;p>Now comes the code bit. By default you get a window with the following code in it&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-csharp" data-lang="csharp">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">using&lt;/span> System;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">public&lt;/span> &lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">void&lt;/span> Run(TimerInfo myTimer, TraceWriter log)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> log.Info(&lt;span style="color:#e6db74">$&amp;#34;C# Timer trigger function executed at: {DateTime.Now}&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is entirely up to you what you get your function to do. In my case I just wanted to call a URL on a schedule so I created some code that used httpclient.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-csharp" data-lang="csharp">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">using&lt;/span> System;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">using&lt;/span> System.Net.Http;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">public&lt;/span> &lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">async&lt;/span> Task Run(TimerInfo myTimer, TraceWriter log)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> log.Info(&lt;span style="color:#e6db74">$&amp;#34;Buffer 0 function executed at: {DateTime.Now}&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> HttpClient client = &lt;span style="color:#66d9ef">new&lt;/span> HttpClient();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">var&lt;/span> result = &lt;span style="color:#66d9ef">await&lt;/span> client.GetAsync(&lt;span style="color:#e6db74">&amp;#34;URL&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">string&lt;/span> resultContent = &lt;span style="color:#66d9ef">await&lt;/span> result.Content.ReadAsStringAsync();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> log.Info(resultContent);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once you have created your app and it has run you can use the monitor section to view success and failures.&lt;/p>
&lt;p>There is loads more you can do with Azure Function but this is a good place to start.&lt;/p></description></item><item><title>Blame</title><link>https://www.funkysi1701.com/posts/2017/blame/</link><pubDate>Mon, 10 Apr 2017 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2017/blame/</guid><description>&lt;p>I recently came home from a busy day of work to my wife blaming me for allowing our 18 month old son to reach some paint and get it all over the carpet. She was concentrating on the fact that he could reach the paint not on the fact that she had brilliantly got the paint out of the carpet.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/04/blame-shifting.jpg?resize=300%2C300&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>My instinct on being blamed is to deny it, or start throwing blame back at her. It is possible I put the paint in his reach, it is also possible it was someone else.&lt;/p>
&lt;p>It is very easy to blame someone else. For example, looking through the git history to find out who changed a specific file is only a few clicks.&lt;/p>
&lt;p>But is it ever productive to blame someone? Is it not better to focus our energy on fixing the issue at hand?&lt;/p>
&lt;p>In the world of business it is easy to go from blaming other people until you have a blame culture. When you have a blame culture everyone starts looking out for themselves so it’s not them that gets the blame and productivity will suffer.&lt;/p>
&lt;p>I think it is more important to put in place​ processes to minimise issues happening again. If a deployment causes downtime, don’t ask who’s fault is it and sack them. Instead what can we do to reduce the chance of it happening again?&lt;/p>
&lt;p>As someone who has an interest in DevOps, I often break things and don’t want to get blamed for that, I do want to improve my processes so I am not always breaking the same things.&lt;/p>
&lt;p>What do you think about Blame? Is what can we do differently always better than who did this?&lt;/p></description></item><item><title>Github Vs Bitbucket Vs Visual Studio Team Services</title><link>https://www.funkysi1701.com/posts/2017/github-vs-bitbucket-vs-vsts/</link><pubDate>Mon, 06 Mar 2017 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2017/github-vs-bitbucket-vs-vsts/</guid><description>&lt;p>As a developer using source control and git is bread and butter of what we do. Github is probably the most popular and widely known hosting service for source control but I have also used Bitbucket and Visual Studio Team Services. Lets have a look at each one and what they offer. Note while I have included prices I have only tried out the free versions.&lt;/p>
&lt;h3 id="github">Github&lt;a class="anchor ms-1" href="#github">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/03/github-octocat.png?resize=300%2C158&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;ul>
&lt;li>URL: &lt;a href="https://github.com/" target="_blank" rel="noopener noreferrer">https://github.com/&lt;/a>
&lt;/li>
&lt;li>Private Repositories: Not Available for free&lt;/li>
&lt;li>Public Repositories: Unlimited&lt;/li>
&lt;li>Team Size: Unlimited&lt;/li>
&lt;li>Prices: $7 per month for unlimited private repositories, $25 per month for 5 users then $9 per month per user&lt;/li>
&lt;/ul>
&lt;p>This is probably the most widely used service for hosting code. Over 13 million repositories of code. This is an ideal solution if you want your code to be publicly viewable, but take care not to publish passwords, private keys or your companies trade secrets. Every developer should have a Github account for displaying bits of code they are proud of.&lt;/p>
&lt;p>There are a number of externally built APIs that link into Github for doing extra features, like building, code coverage etc&lt;/p>
&lt;h3 id="bitbucket">Bitbucket&lt;a class="anchor ms-1" href="#bitbucket">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/03/d8TRzzL.png?resize=150%2C150&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;ul>
&lt;li>URL: &lt;a href="https://bitbucket.org/" target="_blank" rel="noopener noreferrer">https://bitbucket.org/&lt;/a>
&lt;/li>
&lt;li>Private Repositories: Unlimited&lt;/li>
&lt;li>Public Repositories: Unlimited&lt;/li>
&lt;li>Team Size: Less than 5&lt;/li>
&lt;li>Prices: $10 per month for 10 Users, $100 per month for 100 Users, $200 per month for Unlimited Users&lt;/li>
&lt;/ul>
&lt;p>At my last job we used Bitbucket extensively for all our projects. All the code was private so only the team could access it, however before I left we were approaching the 5 user limit (but looking at these prices cost seems very reasonable)&lt;/p>
&lt;h3 id="visual-studio-team-services">Visual Studio Team Services&lt;a class="anchor ms-1" href="#visual-studio-team-services">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/11/Visual-Studio-Team-Services.png?resize=300%2C136&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;ul>
&lt;li>URL: &lt;a href="https://www.visualstudio.com/team-services/" target="_blank" rel="noopener noreferrer">https://www.visualstudio.com/team-services/&lt;/a>
&lt;/li>
&lt;li>Private Repositories: Unlimited&lt;/li>
&lt;li>Public Repositories: Not Available&lt;/li>
&lt;li>Team Size: Less than 5&lt;/li>
&lt;li>Prices: $30 per month for 10 users, and other features paid for via Azure Invoices&lt;/li>
&lt;/ul>
&lt;p>Visual Studio Team Services or VSTS is Microsoft’s version control solution and I have only just started using it, however what I have seen I like. There are lots of options for building your code so VSTS is more than just hosting your code it is verging on a Continuous Integration/Delivery solution. Being a Microsoft product there are numerous links to Azure and it is easy to deploy stuff to that platform.&lt;/p>
&lt;p>All three have options for tracking issues but VSTS have options to add Stakeholder users which would allow none developers to add and keep track of issues and progress with them.&lt;/p>
&lt;p>If I want to run tests, look at code coverage VSTS is probably the solution I would go for, if I want something that is public I would go for Github. What do you think which of these is your favourite?&lt;/p></description></item><item><title>Looking back Ten years</title><link>https://www.funkysi1701.com/posts/2016/looking-back-ten-years/</link><pubDate>Thu, 29 Sep 2016 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2016/looking-back-ten-years/</guid><description>&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/09/10-year-term-life-insurance.gif?resize=471%2C314" loading="lazy"
/>
&lt;/p>
&lt;p>Next month I will celebrate ten years working at my current job, two weeks after that I will start a new chapter of my life at a new company. Lets take this opportunity to look back ten years at some of the great stuff I have achieved.&lt;/p>
&lt;h3 id="2006">2006&lt;a class="anchor ms-1" href="#2006">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>In 2006 I had no IT experience. If you were to ask me where Event Logs could be found I wouldn’t know, and the letters DNS meant nothing to me. I was quiet and shy but hard working. The idea of making IT my career hadn’t occurred to me, this was just a job.&lt;/p>
&lt;p>I started off life doing first line support and compiling health and safety files onto CD-ROMs (known by the company as Eurofiles) with HTML. At the time the company had a mix of Windows 2000 and Windows XP, using MS Office 2003. Server wise I can’t remember exactly but I think two domain controllers, a database server and a backup server using a tape drive. At this point in time I don’t believe I did a lot with the servers but I think the servers were running Windows Server 2005.&lt;/p>
&lt;p>As time went on I learnt more and more about what the company did and how stuff worked.&lt;/p>
&lt;h3 id="nagios">Nagios&lt;a class="anchor ms-1" href="#nagios">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>One of my early achievements was setting up a nagios server. I have blogged about &lt;a href="https://www.funkysi1701.com/posts/2014/i-love-nagios/">Nagios&lt;/a>
before, but it is a server monitoring system that runs on linux. I am extremely proud of what I have achieved here, no one else in the company had my knowledge of how nagios worked. Initially I even experimented with using a cheap mobile phone connected to the serial port (remember when PCs had these?) to send SMS messages to alert of down time. However this was abandoned when I burned through all the credit on this phone.&lt;/p>
&lt;p>Nagios is still used today, and my current team have been shown how to extend the system as the business changes. My philosophy has always been if we have a problem that Nagios didn’t warn us about we need an extra nagios test. These days the nagios web interface is publicly available on the internet so can be checked from anywhere (assuming the office internet is up!) and a mobile app replaces the SMS idea we originally had.&lt;/p>
&lt;h3 id="2011">2011&lt;a class="anchor ms-1" href="#2011">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>At the end of 2010 the IT Manager resigned, then the Developer resigned and finally the Lead Developer resigned. I was alone. I was IT Manager. I negotiated my first pay rise since joining the company. I was IT.&lt;/p>
&lt;p>Wow things were stressful back then. I had so much to learn but somehow I managed. I learnt how to Interview, yes I have employed some clangers along the way, but also some great staff. As well as internal staff I learnt to deal with contractors. We had contractors to help with our internal systems and also contractors to do development work. One thing I have learnt about contractors is that you will always have to chase them at some point to deliver what you are paying them for.&lt;/p>
&lt;h3 id="server-migration">Server Migration&lt;a class="anchor ms-1" href="#server-migration">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>Around the start of 2011 our email server started to show its age. Exchange 2003 had a hard limit on the size of its information store and we were rapidly approaching this. Along with an IT contractor I worked to migrate to Exchange 2010, this was a huge project and caused all sorts of issues which we just worked through. Since then I have done other migrations so Windows Server 2012 and also virtualized many servers in 2013.&lt;/p>
&lt;h3 id="bandwidth">Bandwidth&lt;a class="anchor ms-1" href="#bandwidth">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>A major limit with our infrastructure has always been the internet connection coming in. For a while we tried to load balance three ADSL connections, but the upload speed was always a limiting factor. It was a major victory for the simplification of our network when we got a leased line installed into our head office, helped by a government grant we gave us free installation. Not content with this I did the same again for our second York office, including the free installation.&lt;/p>
&lt;h3 id="learning-development">Learning Development&lt;a class="anchor ms-1" href="#learning-development">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>Since I started I have always been learning development stuff. But in the past few years I have learnt a massive amount helped largely by two main things. My boss passing lots of my responsibility onto others and dedicating lots of my time to development tasks, and also the opportunity to learn with our outsourced development team.&lt;/p>
&lt;h3 id="finishing-off">Finishing off&lt;a class="anchor ms-1" href="#finishing-off">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>There are a few things that have been ongoing that we have wanted to change since I started which I can’t really take the credit for as they are not complete yet, but I am proud as they may be finished in the next few months.&lt;/p>
&lt;p>The company stores calendar information in one giant excel spreadsheet, this is being replaced by outlook calendars.&lt;/p>
&lt;p>Our last Windows Server 2003 server and tape drive is being decommissioned. This is the only server that has been running for the entire time I have been here.&lt;/p>
&lt;p>The way the company keeps track of work coming in and who does what is being reviewed. Over the years many people have tried to adapt the old database that one of the directors created in Access many years ago. I did a major overhaul recently to delete unused columns and added extra invoicing functionality. What is needed is a fresh system, maybe a CRM can do everything they want. I wish them lots of luck in doing this as I know it won’t be easy but it has to be done.&lt;/p>
&lt;h3 id="2016">2016&lt;a class="anchor ms-1" href="#2016">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h3>
&lt;p>There are many other projects and pieces of work that I have done that I am proud of. In 2016 I will leave a stable IT department. All clients are on Windows 10 and using Office 2010. We have virtual servers running Server 2012 and are using some great services from Azure to run our SaaS websites. There are of course things I would have liked to achieve, and things I wish I had done differently but on the whole it feels like the right time to move on, especially now long running projects are starting to conclude.&lt;/p></description></item><item><title>10 Ways to Survive as an IT Manager</title><link>https://www.funkysi1701.com/posts/2016/10-ways-to-survive-as-an-it-manager/</link><pubDate>Thu, 22 Sep 2016 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2016/10-ways-to-survive-as-an-it-manager/</guid><description>&lt;p>So after five and a bit years of being an IT Manager here is some advice I have learned along the way in no particular order. On the whole I have enjoyed myself but it has been a real challenge at times.&lt;/p>
&lt;h2 id="1-figure-out-what-plates-are-still-spinning">1. Figure out what plates are still spinning&lt;a class="anchor ms-1" href="#1-figure-out-what-plates-are-still-spinning">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>Being an IT Manager is all about keeping everything running all of the time. A bit like spinning 5 or 6 plates. You have plates for your servers and network infrastructure, you have plates for bespoke databases that you maintain, you have plates for your staff (including any external contractors), you have plates for any websites or apps that you develop. That is a lot of plates to keep spinning and that before you start thinking about what your boss wants you to deliver. Make sure you know what is happening with all these plates, which ones are happy, which ones are on the way to the floor and which ones you need to get the glue out and repair.&lt;/p>
&lt;h2 id="2-make-it-someone-elses-problem">2. Make it someone else’s problem&lt;a class="anchor ms-1" href="#2-make-it-someone-elses-problem">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>If you can blame someone else do so. If your internet goes down it is your ISPs fault. If your website dies its your hosting company’s fault. Take responsibility for problems but if when something goes wrong you can pick up the phone and ask for help, it will make your life easier.&lt;/p>
&lt;h2 id="3-hire-good-staff">3. Hire good staff&lt;a class="anchor ms-1" href="#3-hire-good-staff">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>Hiring poor staff wastes time and money and makes you look bad by others. Demand the highest salary band for new staff that you can afford and don’t agree to hiring anyone that you have doubts about. It is easy to bow to the pressure to get someone quickly but this will always result in worse problems in the long run. Once you have a good team do your best to keep them, and warn upper management of the problems if staff leaves (basically make it their problem not yours!).&lt;/p>
&lt;h2 id="4-learn-learn-learn">4. Learn, Learn, Learn&lt;a class="anchor ms-1" href="#4-learn-learn-learn">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>You may or may not have the opportunity to go on training courses. Whatever your situation spend time learning new stuff that will benefit the company and yourself. You can learn a lot by reading online, you can petition for training from your managers, you can fund training yourself, you can ask for help from your different suppliers. The more you learn, the more you can do and the more useful you can be to the company, plus the more interesting you will find the job.&lt;/p>
&lt;h2 id="5-say-no">5. Say No!&lt;a class="anchor ms-1" href="#5-say-no">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>Don’t be afraid to say no. You will always be asked to do the impossible and if something is impossible say so at the start. It wastes everyone’s time if you spend a lot of time trying to do the impossible. Always give your reasons for saying no, and if you always say no people will think you are unhelpful. A better way to say no is to come up with a better solution. No I can’t do it your way but here is a better solution.&lt;/p>
&lt;h2 id="6-dont-give-estimates">6. Don’t give estimates&lt;a class="anchor ms-1" href="#6-dont-give-estimates">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>If you are asked how long something will take you don’t answer straight away or give an exaggerated estimate. Go away and spend some time thinking of everything that is involved before replying. There will always be something that you forgot to consider when first asked about it and looking at the different components will help plan out the work needed as well as provide an estimate.&lt;/p>
&lt;h2 id="7-know-what-to-tell-your-boss-and-what-not-to">7. Know what to tell your boss, and what not to&lt;a class="anchor ms-1" href="#7-know-what-to-tell-your-boss-and-what-not-to">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>This is a hard one to get the balance right for. You need to tell your boss enough so that they appreciate all that you do, but too much and they will stop listening and accuse you of talking in technobabble. I have never got the balance right with this one. I have always aired on the side of not telling my boss enough, and hence they don’t realize that I saved the day on Sunday night as everything is working again on Monday. Do repeat yourself. If your server is running low on resources start asking for replacement hardware early, and increase the frequency and the panic in line with the problems it is causing.&lt;/p>
&lt;h2 id="8-understand-the-problems-of-the-business">8. Understand the problems of the business&lt;a class="anchor ms-1" href="#8-understand-the-problems-of-the-business">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/09/bhrzpww6aehdx1wvrrug.jpg?w=800&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>Businesses need to make money. If the one you work for isn’t making enough money you will soon be looking for another. If you work for IT you will quickly start to see the problems of the business, think about what simple changes IT could make to improve things that would benefit the whole company. Some of your suggestions won’t go anywhere, but some may have a massive impact. I can think of a few changes that IT have spearheaded that I am very proud of, upgrading our internet connection, simplifying or automating processes and delivering new versions of software.&lt;/p>
&lt;h2 id="9-ask-for-help">9. Ask for help&lt;a class="anchor ms-1" href="#9-ask-for-help">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>Don’t be afraid of asking for help. There are lots of places to look for help. Other departments could take more on, you could recruit extra help, you could hire external contractors. You can ask questions on support forums like &lt;a href="https://serverfault.com/" target="_blank" rel="noopener noreferrer">ServerFault&lt;/a>
or &lt;a href="https://stackoverflow.com/" target="_blank" rel="noopener noreferrer">StackOverflow&lt;/a>
, many software re-sellers or other suppliers are a good point of contact for questions about things they supply. Microsoft Support was also invaluable for a server issue.&lt;/p>
&lt;h2 id="10-think-about-disasters">10. Think about disasters&lt;a class="anchor ms-1" href="#10-think-about-disasters">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>Write a disaster recovery plan or backup policy. Yes there will always be something more important that needs doing, but just stop for a moment to think how you would feel if everything died on your watch. The one thing you can rely on with technology is that it will fail at some point. A back of the envelope plan of action is better than no plan at all, even better is a detailed plan of what to do when each and every service you rely on fails. Plan additional services with an idea of adding extra redundancy. Always have multiple Domain Controllers, think about what data you could run from the Cloud. VMs could be replicated to the Cloud, and servers could be run from there.&lt;/p></description></item><item><title>DZone</title><link>https://www.funkysi1701.com/posts/2016/dzone/</link><pubDate>Thu, 25 Aug 2016 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2016/dzone/</guid><description>&lt;p>For a while now I have been sharing some of my blog posts on the &lt;a href="https://dzone.com/" target="_blank" rel="noopener noreferrer">Dzone&lt;/a>
website.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/08/dzone_02.png?w=400&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>The Dzone website allows users to submit links to content and I have been submitting the content I have created on this website. This is how Dzone describes themselves:&lt;/p>
&lt;p>With over 1 Million members, DZone.com is one of the web’s largest communities and publishers of technical content for software professionals. Developers from all over the world come to DZone for the latest and best content to hone their skills and advance their careers.&lt;/p>
&lt;p>Well this week I have been invited to join the &lt;a href="https://dzone.com/pages/mvb" target="_blank" rel="noopener noreferrer">MVB&lt;/a>
(Most Valuable Blogger) programme. The hope is that more of my readers will find my content from the DZone website.&lt;/p>
&lt;p>The DZone team will soon start sharing my content on the DZone website and I hope this will result in even more people reading what I have to say.&lt;/p>
&lt;p>If nothing else this is clearly telling me that my blog is making a difference, people are taking note of me and what I have to say. I need to keep going and keep writing articles and be more consistent.&lt;/p></description></item><item><title>Amazon Web Services Pt 2</title><link>https://www.funkysi1701.com/posts/2016/amazon-web-services-pt-2/</link><pubDate>Thu, 04 Aug 2016 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2016/amazon-web-services-pt-2/</guid><description>&lt;p>Last time I started looking at Amazon Web Services and how it differed from Azure. I am going to continue looking at what it can do.&lt;/p>
&lt;h2 id="virtual-machines">Virtual Machines&lt;a class="anchor ms-1" href="#virtual-machines">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>Lets look at what you can do with Virtual Machines. I selected to create a new Virtual Machine (or as AWS calls them an EC2 Instance)&lt;/p>
&lt;p>First you choose a name for your VM and then the OS that runs on it. There are 5 main OSes to choose from Windows Server, Amazon Linux and a selection of the most common Linux flavours.&lt;/p>
&lt;p>You can then download a certificate to secure your VM.&lt;/p>
&lt;p>Like Azure, AWS takes a few moments to create your VM. While I wait I can see that AWS has configured a firewall so only my current IP can connect to it.&lt;/p>
&lt;p>Once the VM is ready you can download an RDP file. However to get the login details you need to decrypt the password using the certificate you downloaded when you created the VM.&lt;/p>
&lt;p>It is interesting to compare the difference in security between Azure and AWS. Azure allows the resetting of passwords of VMs directly from its console, however I suspect that in AWS if you loose your certificate (AWS states they don’t keep a copy of this) you would have to recreate your VM.&lt;/p>
&lt;p>Like with the Websites the default name of the VM is much less user-friendly than what you get from Azure. However I suspect there are other options I haven’t spotted that may customise these.&lt;/p>
&lt;h2 id="azure-portal-vs-aws-console">Azure Portal vs AWS Console&lt;a class="anchor ms-1" href="#azure-portal-vs-aws-console">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>I really like the Azure Portal. It feels like something that has been designed so you can easily access all the options for a specific Azure feature.&lt;/p>
&lt;p>The AWS Console probably has all the same options as with Azure however I don’t think it looks half as good, and will take me a while looking through menus to find the equivalent options. Part of this is due to my unfamiliarity with AWS, so will get easier with time.&lt;/p></description></item><item><title>Clever things with MS Access</title><link>https://www.funkysi1701.com/posts/2016/clever-things-ms-access/</link><pubDate>Thu, 07 Jul 2016 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2016/clever-things-ms-access/</guid><description>&lt;p>I hate MS Access and especially developing with it as you can’t do any thing clever with it.&lt;/p>
&lt;p>&lt;strong>Wrong, Wrong, Wrong!&lt;/strong>&lt;/p>
&lt;p>There are a few clever things I have been able to script to make developing with it passable. I still would rather use Visual Studio but this improves the experience a fair bit.&lt;/p>
&lt;h2 id="source-control">Source Control&lt;a class="anchor ms-1" href="#source-control">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>Access files are binary (I use the ADP/ADE file format but I assume other Access file formats have the same problem) and so you can’t diff them to see what has changed. This is bad.&lt;/p>
&lt;p>However there is a solution to this. A tool called Access SVN and can be downloaded from &lt;a href="https://accesssvn.codeplex.com/" target="_blank" rel="noopener noreferrer">https://accesssvn.codeplex.com/&lt;/a>
, this gives you a way to extract to text files all the forms and reports that are in Access. Before every commit I would manually run this tool on my ADP file and extract to text files, then I would commit these text files to source control and could easily see what had changed in each commit.&lt;/p>
&lt;p>Despite the name Access SVN, the tool is not tied to subversion, you can use any source control system, I use git.&lt;/p>
&lt;p>Also included in this tool is a way to do this with the command line, so you can make this a build step on your build server. I have not used this extensively yet, but the syntax is fairly simple&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>asvn.exe e &lt;span style="color:#e6db74">&amp;#34;path to Access file&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;path to txt files&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;*.*&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The filter at the end &lt;em>.&lt;/em> allows you to specify what to extract so you could extract all forms/reports beginning with D with &amp;ldquo;&lt;em>.D&lt;/em>&amp;rdquo;. I had trouble using &lt;em>.&lt;/em> because the names of my forms/reports contain characters not allowed in a windows file name. I am sure there is a way round this but I haven’t had chance to look into it further yet.&lt;/p>
&lt;h2 id="testing-ms-access">Testing MS Access&lt;a class="anchor ms-1" href="#testing-ms-access">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>Surely testing is not possible with MS Access? I would have agreed with that statement the other day until I found a neat way of testing if a feature is enabled.&lt;/p>
&lt;p>Firstly a bit of background. I develop using MS Access 2003 because the design view is far easier to use, however because it is out of support all my users use MS Access 2010. MS Access 2010 has a feature called Tabbed Documents which allows all forms and reports to open in new tabs so you can easily switch between them. This feature can only be enabled in MS Access 2010 and has no effect if opening with MS Access 2003.&lt;/p>
&lt;p>If you use Access SVN on your Access file with tabbed documents turned on and off you will see UseMDIMode: 0 and UseMDIMode: 1 show up in the Database properties file. UseMDIMode: 0 means that tabbed documents is turned on.&lt;/p>
&lt;p>In powershell I can now write a test to see if UseMDIMode: 0 can be found in the database properties file&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-powershell" data-lang="powershell">&lt;span style="display:flex;">&lt;span>Get-Content &lt;span style="color:#e6db74">&amp;#34;General\Database properties.dbp.txt&amp;#34;&lt;/span> | Select-String &lt;span style="color:#e6db74">&amp;#34;UseMDIMode: 0&amp;#34;&lt;/span> -quiet
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If the test passes True will be returned, if it fails null will be returned.&lt;/p>
&lt;p>On my build server I scripted the extraction of Database properties.dbp.txt from the ADP file with asvn.exe before running this test. While not strictly needed as Database properties.dbp.txt should be in source control, it is possible that someone could forget to extract the text files from the ADP, with this step you are always testing what is enabled in the binary file.&lt;/p>
&lt;h2 id="ms-access-connection-strings">MS Access Connection Strings&lt;a class="anchor ms-1" href="#ms-access-connection-strings">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>While developing with MS Access I often swap the database connection to point to my local machine or a build server. I always try and remember to only ever commit with this set to the live database to avoid obvious problems.&lt;/p>
&lt;p>The other day I found on &lt;a href="https://stackoverflow.com/questions/16411430/change-access-server-connection-from-command-line" target="_blank" rel="noopener noreferrer">stackoverflow&lt;/a>
a way to script this. I love this! I can include this step in my deployment process and it will overwrite what ever the connection string is in source control with what your production environment needs.&lt;/p>
&lt;p>All you need to run this step is, (note it is spaces between the parameters not commas)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cscript connect.vbs Project.adp &lt;span style="color:#e6db74">&amp;#34;ServerName&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;DatabaseName&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The contents of connect.vbs can be found on the &lt;a href="https://stackoverflow.com/questions/16411430/change-access-server-connection-from-command-line" target="_blank" rel="noopener noreferrer">stackoverflow article&lt;/a>
. It is also possible to pass username and password if your environment requires this.&lt;/p>
&lt;h2 id="compiled-ade">Compiled ADE&lt;a class="anchor ms-1" href="#compiled-ade">&lt;i class="fas fa-link">&lt;/i>&lt;/a>&lt;/h2>
&lt;p>The last clever thing I do with MS Access is convert my ADP file into the compiled ADE version. To manually do this there is an option in the tools menu.&lt;/p>
&lt;p>To automate this I run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cscript createADE.vbs &lt;span style="color:#e6db74">&amp;#34;path to ADP&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;path to ADE&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The contents of createADE came from this &lt;a href="https://social.msdn.microsoft.com/Forums/office/en-US/01fd48a9-258e-4405-86f1-adfb2f1057ee/create-an-access-2007-ade-from-a-adp-via-commandline?forum=accessdev" target="_blank" rel="noopener noreferrer">forum post&lt;/a>
, the only change I made was to comment out some of the echo statements so it would run silently as part of my build process. It should be noted that cscript and wscript are almost identical and either will run these scripts however in a command line environment cscript is preferable, and wscript should be used in a windows environment.&lt;/p>
&lt;p>I am quite surprised at how much I have managed to do in terms of scripting the build and deployment process for MS Access. I still don’t like developing with Access but this has definitely improved things.&lt;/p></description></item><item><title>Periodic Table of DevOps</title><link>https://www.funkysi1701.com/posts/2016/periodic-table-devops/</link><pubDate>Thu, 30 Jun 2016 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2016/periodic-table-devops/</guid><description>&lt;p>The periodic table lists all the chemical elements and groups them together based on some key properties. Today I found an article about the &lt;a href="https://xebialabs.com/periodic-table-of-devops-tools/" target="_blank" rel="noopener noreferrer">periodic table of DevOps&lt;/a>
.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/06/https-xebialabs.com-assets-files-infographics-periodic-table-of-devops-v2.png?resize=1024%2C572&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>I am not going to discuss every element but I thought I might go through some that I have heard of or used.&lt;/p>
&lt;ul>
&lt;li>Github – The repository of lots of open source software. My Github can be found at &lt;a href="https://github.com/funkysi1701" target="_blank" rel="noopener noreferrer">https://github.com/funkysi1701&lt;/a>
&lt;/li>
&lt;li>Amazon Web Services – The second most popular cloud computing provider (not sure why this isn’t number 3?)&lt;/li>
&lt;li>Git – The distributed source control system that everyone uses these days.&lt;/li>
&lt;li>Azure – The number one cloud provider, I have used this a lot, mostly with websites but also with some of their other features like Traffic Manager.&lt;/li>
&lt;li>Bitbucket – like Github but allows private repositories. I have used this extensively for work based projects that I don’t want to be public.&lt;/li>
&lt;li>Google Cloud Platform – don’t know much about this one, but no surprise that google wants a piece of the cloud computing pie.&lt;/li>
&lt;li>Selenium – This is a product I want to play about with as allows front end testing with a browser.&lt;/li>
&lt;li>Rackspace – Before we made the jump to Azure we made use of some Rackspace servers.&lt;/li>
&lt;li>Subversion – The first source control system that I used, but been using git so long now not sure I can remember how it worked.&lt;/li>
&lt;li>Visual Studio – The IDE from Microsoft that I use to write code. I am a big fan as it does everything I could want.&lt;/li>
&lt;li>TeamCity – The continuous integration software that I have been using to automate my deployments.&lt;/li>
&lt;li>MSBuild – This is used by Visual Studio to build your software and can also be used by your deployment scripts.&lt;/li>
&lt;li>Trello – A website that allows you to create a board of ideas or things to do.&lt;/li>
&lt;li>Slack – Brings all your communication together in one place. It’s real-time messaging, archiving and search for modern teams.&lt;/li>
&lt;li>New Relic – A software analytics tool suite used by developers, ops, and software companies to understand how your applications are performing. Useful but find myself favouring Application Insights (part of Azure) more now.&lt;/li>
&lt;li>Nagios – Yay nagios is on the list! My favourite server monitoring system.&lt;/li>
&lt;li>Splunk – This application can be used to search, monitor and analyse all your log files to find out what is happening. Don’t currently use it but I have tried it out in the past.&lt;/li>
&lt;/ul>
&lt;p>What is your favourite DevOps tool? Why not leave a comment below?&lt;/p></description></item><item><title>Automatic Git Tagging</title><link>https://www.funkysi1701.com/posts/2016/automatic-git-tagging/</link><pubDate>Thu, 16 Jun 2016 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2016/automatic-git-tagging/</guid><description>&lt;p>One of the features of git is the ability to tag a point in my change history with a tag. For a while now I have been manually tagging my code whenever I do a release, so I can easily work out what has changed by doing a diff between two tags.&lt;/p>
&lt;p>Now that I am automating my release process with TeamCity I am thinking about how to manage my tags better.&lt;/p>
&lt;p>TeamCity has a setting called VCS Labeling which comes in very handy.
&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/06/Untitled.jpg?w=1595&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>Configuring it is fairly simple as it only has three settings.&lt;/p>
&lt;p>&lt;strong>VCS root to label&lt;/strong>: This is obviously the url to your git repository
&lt;strong>Labeling pattern&lt;/strong>: This is the text of the label to be added.
&lt;strong>Label successful builds only&lt;/strong>: Do you really want to add a tag if the build failed?&lt;/p>
&lt;p>A tag needs to have a unique name, so adding a tag just called &lt;strong>deployed&lt;/strong> won’t work. When I used to add tags manually I used the naming convention of &lt;strong>deployedyyyymmdd&lt;/strong>. While this naming convention is possible with TeamCity I use something a bit more complex to provide more information about what has been deployed.&lt;/p>
&lt;p>TeamCity provides lots of parameters that can be used in your build steps and also in the Labeling pattern box. I started off using &lt;strong>deployed-build-%system.build.number%&lt;/strong> as my tag which just marks git with the TeamCity build number.&lt;/p>
&lt;p>When I run a TeamCity deployment I don’t always use the same configuration options, I deploy locally, to a test server or to production and sometimes I just deploy the frontend or the backend. How cool would it be to include this information in the tag text?&lt;/p>
&lt;p>Well the next step was to change my Labeling pattern to &lt;strong>deployed-build-%system.build.number%-%ServerName%-%DatabaseName%-%FrontEndPath%&lt;/strong>, this adds the backend database config settings and the path the frontend was deployed to. Now when looking at git you can see commits marked with multiple tags, one for each deployment that succeeded and the tag will indicate the settings used during that deployment.&lt;/p>
&lt;p>Now I will never forget to add the tag after a release as the adding of a tag is part of the deployment process, if the deployment fails the tag won’t be added. I can test my deployment to test and git will show if this has been successful, and when I deploy live this will also show up.&lt;/p>
&lt;p>How do you use tags? Do you mark successful builds with a tag? Why not let me know or leave a comment below.&lt;/p></description></item><item><title>Revisiting Team City</title><link>https://www.funkysi1701.com/posts/2016/revisiting-teamcity/</link><pubDate>Thu, 24 Mar 2016 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2016/revisiting-teamcity/</guid><description>&lt;p>Last year I blogged about &lt;a href="http://www.funkysi1701.com/2015/04/01/teamcity/">Team City&lt;/a>
, well I have been looking at it again recently. In that time they have even changed their logo!&lt;/p>
&lt;p>Lets start with thinking about what I want my Continuous Integration server to do.&lt;/p>
&lt;ol>
&lt;li>Check out my code from source control (usually master but all feature branches would be even better)&lt;/li>
&lt;li>Configure specific setting for build&lt;/li>
&lt;li>Build my code&lt;/li>
&lt;li>Build my databases&lt;/li>
&lt;li>Run any unit tests&lt;/li>
&lt;li>(Optional) Run deployment to Azure Test/Live site&lt;/li>
&lt;/ol>
&lt;p>There are probably other things I want to achieve but I will start with these six.&lt;/p>
&lt;ol>
&lt;li>Checking out code from source control is something Team City does out of the box, so I can safely say I have done this now. It even monitors a branch for changes and initiates a new check out.&lt;/li>
&lt;li>Team City allows you to create specific build steps so in theory you can have multiple builds for every variation of settings that you want for your code. I have not tried this yet apart from building with the default config, but I don’t expect it will be too difficult.&lt;/li>
&lt;li>I have managed to get my code to build with Team City, it took a bit of tweaking the different build steps but wasn’t too difficult. Team City has a visual studio build agent which takes you solution file and does what it needs to. The one problem I have found with this step is that I get errors with my tests if I select a Debug config instead of Release.&lt;/li>
&lt;li>Databases are always the problem part of the deployment. So far I have manually deployed my databases but I intend on revisit this step. A &lt;a href="https://stackoverflow.com/questions/21555038/how-can-i-execute-sql-scripts-using-teamcity" target="_blank" rel="noopener noreferrer">stackoverflow&lt;/a>
post suggests that I can run SQL code via Team City in the following way by creating a command line executable:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>Command executable: c:&lt;span style="color:#ae81ff">\P&lt;/span>rogram Files&lt;span style="color:#ae81ff">\M&lt;/span>icrosoft SQL Server&lt;span style="color:#ae81ff">\1&lt;/span>00&lt;span style="color:#ae81ff">\T&lt;/span>ools&lt;span style="color:#ae81ff">\B&lt;/span>inn&lt;span style="color:#ae81ff">\s&lt;/span>qlcmd.exe
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Command parameters: -S &lt;span style="color:#f92672">[&lt;/span>ServerName&lt;span style="color:#f92672">]&lt;/span> -i &lt;span style="color:#f92672">[&lt;/span>PathToSQLScript&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I have yet to try this but I am hopefully that it will just work. Dropping a database and restoring a back and then running different SQL scripts is all possible from TSQL, so I should be OK. Watch this space for more details.&lt;/p>
&lt;ol start="5">
&lt;li>Running the unit tests got me stuck for a while. I tried setting it up using VSTest or MSTest neither worked mainly because a config file wasn’t being copied with the test binaries. When I tried using NUnit it just worked. The tests that failed gave me a few config settings to change.&lt;/li>
&lt;li>I have powershell scripts that deploy to Azure websites, I think that these could form the basis of a deployment to Azure. Again the difficult step here may end up being deploying all the different databases to Azure. This is also the riskiest step as I need to connect to live servers which is why I will leave this to last, at the very least I could generate scripts that do a full deployment.&lt;/li>
&lt;/ol>
&lt;p>That’s it for now. Once I have this all working I will revisit again with details of the database steps as I am expecting a few challenges to overcome. What have you used a CI Server for? Are there other things I want to achieve from a project like this? Why not contact me or leave a comment below&lt;/p></description></item><item><title>Trying Out Azure Active Directory</title><link>https://www.funkysi1701.com/posts/2016/trying-out-azure-active-directory/</link><pubDate>Thu, 03 Mar 2016 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2016/trying-out-azure-active-directory/</guid><description>&lt;p>One of my plans is to create new MVC Webapps for my companies databases. Once I publish these I will need to secure them so only staff have access.&lt;/p>
&lt;p>The traditional way to do this would be insert membership tables into my database. The user then has to remember another username and password and I have to secure the storage of these credentials. Lots of work for everyone.&lt;/p>
&lt;p>&lt;img class="img-fluid" alt="" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/03/arch.png?w=600&amp;amp;ssl=1" loading="lazy"
/>
&lt;/p>
&lt;p>There is a better way by using Azure Active Directory. You have probably heard of Active Directory, if you are a SysAdmin you probably use it all the time to manage your corporate users and computers. Azure Active Directory is an extension of this into the Cloud.&lt;/p>
&lt;p>I have blogged in the past about using Azure but this is the first time I have tried connecting my internal domain to Azure. There is a &lt;a href="https://vlabs.holsystems.com/vlabs/technet?eng=VLabs&amp;amp;auth=external&amp;amp;src=vlabs&amp;amp;altadd=true&amp;amp;labid=13535" target="_blank" rel="noopener noreferrer">Virtual Lab&lt;/a>
which helped me try out some of these ideas.&lt;/p>
&lt;p>The first thing I did was to create a new Directory in Azure. I did this via the old portal (manage.windowsazure.com) it might be possible via the new portal but I don’t know how yet.&lt;/p>
&lt;p>Click New, select App Services &amp;gt; Active Directory &amp;gt; Directory and select Custom Create. Select Create new directory, give it a name and a domain name and select a region from the drop down. Then add a Global Admin for this directory.&lt;/p>
&lt;p>There is a tool called &lt;a href="https://www.microsoft.com/en-us/download/details.aspx?id=47594" target="_blank" rel="noopener noreferrer">Azure Active Directory Connect&lt;/a>
. Download and Install this with express settings on one of your domain controllers. You need to specify a domain admin account to access your domain and the Azure Global Admin account you just created.&lt;/p>
&lt;p>At this point I went to bed so I am not sure how long it tool to sync all the domain information but by morning it was all showing in the users list on Azure.&lt;/p>
&lt;p>All my user accounts are showing with a @contoso.onmicrosoft.com, it is possible to use custom domains but I haven’t figured out that step yet. I made a change in my Active Directory and a while later that change was showing in Azure AD.&lt;/p>
&lt;p>So now what? Open up Visual Studio and see if I can use Azure to Authenticate.&lt;/p>
&lt;p>I selected to create a new MVC web project and clicked the change authenticate option. One of the options was Work and School Accounts, I then selected Cloud Single Organization and entered contoso.onmicrosoft.com. I then ran this app and it authenticated using Azure using my domain password. Really impressed at how easy that was.&lt;/p>
&lt;p>The app then shows up on Azure in the old portal. In Applications you can see a list of which users have access to your app and configure few other app related settings.&lt;/p>
&lt;p>This is a long way off being useful in my actual app, but it shows that the basics of what I am trying to do does work. Anyone done anything similar with Azure AD? How did you get on?&lt;/p></description></item><item><title>Azure Traffic Manager</title><link>https://www.funkysi1701.com/posts/2015/azure-traffic-manager/</link><pubDate>Thu, 12 Mar 2015 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2015/azure-traffic-manager/</guid><description>&lt;p>I have spent most of the day tweaking my Azure websites. Lots of fun!&lt;/p>
&lt;p>Last week unfortunately Azure had some problems and many websites that were running in the North Europe data centre were unavailable for several hours. And you guessed it my websites were hosted here.&lt;/p>
&lt;p>All hosting providers are going to have downtime from time to time and this is just something you have to take on the chin. The important thing to do in times like these is communicate with your customers about what is going on and that you are doing everything you can to restore service.&lt;/p>
&lt;p>However Azure has some amazing features that you can configure to help manage when downtime occurs.&lt;/p>
&lt;p>Azure is Microsoft’s global cloud platform. And it really is global, there are data centres in North Europe, West Europe, Brazil, Japan, two more in Asia and five in the US. In the event of problems it is highly unlikely that more than one of these would go down at once. If all of these are unavailable, I expect the planet earth is facing some kind of cataclysmic event and the fact that my website is down is not a priority.&lt;/p>
&lt;p>&lt;a href="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/03/IC750592.jpg" target="_blank" rel="noopener noreferrer">&lt;img class="img-fluid" alt="IC750592" src="https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/03/IC750592.jpg?resize=662%2C347" loading="lazy"
/>
&lt;/a>
To take advantage of these multiple data centres, Azure has something called a Traffic Manager.&lt;/p>
&lt;p>Traffic Manager has various settings but I am using it in failover mode. This means that if one website goes down, the next one is used.&lt;/p>
&lt;p>All you need to do is create a traffic manager, add two or more websites to it (called endpoints) and choose a page that needs to be monitored so Azure knows which websites are up and which are down.&lt;/p>
&lt;p>If you are using SSL or custom domain names, there are a few extra steps you need to do. Your custom domain name needs pointing at the traffic manager, not the individual websites. The websites themselves have three domain names, the traffic manager address, the azure address and the custom domain name. The SSL certificate can then be assigned to each website that you have added to the traffic manager.&lt;/p>
&lt;p>That was easy wasn’t it, and now if a website goes down traffic manager will use the next one. While testing this, the transition to the next website was almost immediate. I did notice that if you had a browser showing the website open during a problem you sometimes got an error page, I think this was probably due to browser caching, reopening a tab or browser fixed this issue.&lt;/p></description></item><item><title>What is the difference between Development and Operations?</title><link>https://www.funkysi1701.com/posts/2014/what-is-the-difference-between-dev-and-ops/</link><pubDate>Sat, 27 Sep 2014 20:00:45 +0000</pubDate><guid>https://www.funkysi1701.com/posts/2014/what-is-the-difference-between-dev-and-ops/</guid><description>&lt;p>I want to learn Development and eventually move into a development role. But what is the difference between development and what I already do?&lt;/p>
&lt;p>Put simply Development or programming is the creation of new programs, websites or databases.&lt;/p>
&lt;p>IT Operations or System Administration is the administration of existing servers, websites or databases.&lt;/p>
&lt;p>I have been doing System Administration since I started my job back in 2006. What you do as part of this role can be very simple like setting up new users or computers to something very complex like upgrading the version of Exchange that the company uses for its emails.&lt;/p>
&lt;p>As part of my role I have done a lot of tasks that could be described as development. I have created databases and built new database structures like tables, views and stored procedures. I have also assisted the development department with testing, setting up visual studio, building websites and databases from code.&lt;/p>
&lt;p>Can I describe myself as a developer? I would say yes, I have done plenty of work that is development work.&lt;/p>
&lt;p>Can I get a job as a developer? Possibly but it would depend on the role. I have many of the skills that developers use, but I have limited knowledge in many areas. What I need to do is concentrate my efforts onto the development work I do and delegate as much administration work as I can so my knowledge can increase.&lt;/p>
&lt;p>A Buzz word in IT at the moment is DevOps. I have lost count of the number of times .NetRocks or RunAsRadio have mentioned it.&lt;/p>
&lt;p>DevOps is the integration of IT Development and Operations, it emphasises the need for the two departments to work closely together to achieve common goals. In big companies the two departments can pull against each other if you are not careful, but in my case as I do both roles, it would be very difficult for the development side of myself to blame the operations side of myself for a problem.&lt;/p>
&lt;p>This I think puts me in a good position, I just need to learn more development and I will make a valuable addition to someone’s development team.&lt;/p></description></item></channel></rss>