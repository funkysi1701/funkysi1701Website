[{"categories":null,"content":"This morning I was listening to a podcast where the new features coming out for SQL Server 2022 were being discussed. This starting me thinking about what would be involved in upgrading.\nUpgrading production environments is complex and there are licensing considerations to take into account. However for non production workloads like development this isn\u0026rsquo;t a problem so lets look at that first.\nIn the past I have installed SQL Server Devloper Edition onto my laptop, this is fine but I have found that unless you are very careful you may end up with multiple different versions of SQL Server sitting around, and it is difficult to cleanly remove them without a fresh install of the OS.\nHowever in this day and age, Docker and Containers are king. My current development environment makes use of Docker and has a docker compose file which sets up SQL Server for this particular application, lets take a look.\n sqlserver:  image: mcr.microsoft.com/mssql/server:2019-latest  container_name: Sql  ports:  - \u0026#34;5432:1433\u0026#34;  networks:  - my-network  volumes:  - sqlvolume:/var/opt/mssql This defines which docker image to use, in this case 2019-latest, sets up ports and the name and saves the data on a docker volume.\nIf we then run SELECT @@VERSION on this instance of SQL Server we get told:\nMicrosoft SQL Server 2019 (RTM-CU13) (KB5005679) - 15.0.4178.1 (X64) Sep 23 2021 16:47:49 Copyright (C) 2019 Microsoft Corporation Developer Edition (64-bit) on Linux (Ubuntu 20.04.3 LTS) \u0026lt;X64\u0026gt; What if we change the docker-compose file to use 2022-latest?\nmanifest for mcr.microsoft.com/mssql/server:2022-latest not found: manifest unknown: manifest tagged by \u0026#34;2022-latest\u0026#34; is not found SQL Server 2022 hasn\u0026rsquo;t been released yet so there is no docker image for it yet. Try this command again in a few months when it is available.\nOK so what else can we try? What about a downgrade to 2017-latest? Will that work?\nSQL Server 2017 starts but the following error gets logged.\n2022-02-23 21:41:15.30 Server Software Usage Metrics is disabled. 2022-02-23 21:41:15.30 spid6s Starting up database \u0026#39;master\u0026#39;. 2022-02-23 21:41:15.34 spid6s Error: 948, Severity: 20, State: 1. 2022-02-23 21:41:15.34 spid6s The database \u0026#39;master\u0026#39; cannot be opened because it is version 904. This server supports version 869 and earlier. A downgrade path is not supported. Doh we can\u0026rsquo;t downgrade the existing database we have. Probably a good thing really.\nMicrosoft release regular updates for SQL Server called CU\u0026rsquo;s (Cumulative Updates), you can see above we are on CU13. Is there a CU14 or CU15 we could try?\nUpdate the docker compose to: mcr.microsoft.com/mssql/server:2019-CU14-ubuntu-20.04\nAt this point I actually got an error\n2022-02-23 21:50:20.71 Server Error: 17058, Severity: 16, State: 1. 2022-02-23 21:50:20.71 Server initerrlog: Could not open error log file \u0026#39;/var/opt/mssql/log/errorlog\u0026#39;. Operating system error = 5(Access is denied.). This is caused by trying to use SQL Server 2017 but it is easy to fix.\nIn docker desktop there is a volumes section, find the volume you are using with SQL Server and delete the errorlog mentioned above.\nNow if you retry SQL will start OK.\nRepeating the SELECT @@VERSION gives us a new CU\nMicrosoft SQL Server 2019 (RTM-CU14) (KB5007182) - 15.0.4188.2 (X64) Nov 3 2021 19:19:51 Copyright (C) 2019 Microsoft Corporation Developer Edition (64-bit) on Linux (Ubuntu 20.04.3 LTS) \u0026lt;X64\u0026gt; Microsoft SQL Server 2019 (RTM-CU15) (KB5008996) - 15.0.4198.2 (X64) Jan 12 2022 22:30:08 Copyright (C) 2019 Microsoft Corporation Developer Edition (64-bit) on Linux (Ubuntu 20.04.3 LTS) \u0026lt;X64\u0026gt; How much easier is this than manually installing updates and rebooting or attempting to uninstall and reinstall SQL Server. As SQL Server 2022 isn\u0026rsquo;t out yet I can\u0026rsquo;t say for certain what issues I may encounter but hopefully it will be as easier as this. And I don\u0026rsquo;t need to backup or restore and databases they are all available as before!\n View on DevTo  View on Hashnode   ","date":"Feb 23, 2022","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2022/header-01.png","permalink":"https://www.funkysi1701.com/posts/2022/updating-sqlserver-with-docker/","series":null,"tags":["Docker","SQL"],"title":"Updating SQL Server with Docker"},{"categories":null,"content":"Today Microsoft celebrated 20 years since the first version of dotnet was released with a special live stream event.\n A lot has happened in the 20 years of dotnet (or .Net). It is my understanding that .Net was created to rival Java. When the .Net Framework was first created it was a windows only thing, but today modern .Net is a modern run anywhere platform, with .net applications running everywhere from PCs and Laptops, Raspberry Pi\u0026rsquo;s, Mobile phones and tablets (via Xamarin Forms), to Websites and Microservice APIs running on every Cloud platform out there.\nIf you want to celebrate this achievment tweet with the hashtag #DotNetLovesMe or download some of the digital swag available from github On Thursday the 17th Feb the first preview of dotnet 7.0 is going to be released, with the latest version 6.0 only being released last November.\nThe above graphic comes from the following tweet.\nThis is 20 years of #dotnet releases. #dotNETLovesMe pic.twitter.com/Zxfe1SdWTq\n\u0026mdash; Khalid ðŸŽŸ (@buhakmeh) February 14, 2022  What do you like about dotnet? When did you first start using it? What are you going to build with it next?\n View on DevTo  View on Hashnode   ","date":"Feb 14, 2022","img":"https://www.funkysi1701.com/images/FLjBrnPXwAQE8BN.jfif","permalink":"https://www.funkysi1701.com/posts/2022/dotnet-is-20-years-old/","series":null,"tags":["DotNet"],"title":"dotnet is 20 years old"},{"categories":null,"content":"Is it to share my ideas? Is it to learn new technologies and techniques? Is it to create a following? Is it to educate others? Is it to build some kind of service? Is it some combination of all of these.\nHistory Back when the web was young and I was first learning HTML. I hand crafted web pages, adding photos I had taken with captions. If I needed a new page I just added a new html page and linked to it from another page.\nAs time went on I started to learn mysql and php and my website became a hand crafted php nightmare. I also applied what I learned to help my father run the website for his camera club.\nAt some point I started playing with WordPress. I have had various WordPress websites or blogs over the years. WordPress is very powerful you can do so many things, install so many plugins. WordPress runs on php and mysql and as my career started to centre around the .net space, I started to want something that was similar, so I could apply things I had learnt to my own website.\nThis has led me to the current state of my website. I have a WordPress blog, with most of my oldest content, my newer content lives on dev.to and I have a Blazor webassembly site that uses the dev.to api to run my new website.\nBlazor webassembly is great, however it has some limitations which I am starting to push against. To host this as cheaply as possible I am using Azure static web apps, so no .net backend all the website is front end. I have some Azure Functions that does the backend bits that I need.\nGoogle and other bots are not able to find any of my pages except index, due to the way Blazor works. I have got round this by pre rendering the content using https://prerender.io/ My next difficulty is how to generate a sitemap.xml or a rss feed for my blog. This has started to make me question my architecture decisions.\nI could use a hosted solution like ghost which is popular with a some of my peers. This would solve many of the problems I am currently facing but I wouldn\u0026rsquo;t be able to play with everything as it is hosted and therefore someone else\u0026rsquo;s problem. How important that is I will look at later.\nAnother option would be to use github pages, there are quite a few ways to publish a github page, Jekyll and Hugo appear to be the most popular. Both produce static content and both are a new for me to learn. Interestingly I could also publish either to Azure Static Web apps if github pages ends up not being suitable.\nSplit in two I think my website needs to be split in two. I need a stable blog platform probably using Hugo and github pages. This is what I want to get indexed by the search engines and be the primary way people find out about what I am doing.\nI then have additional sites, that I use as my playground for learning new tech. I can easily link between them and I can tweak the style so they \u0026ldquo;fit\u0026rdquo; nicely together.\nI am still considering what to do with dev.to. I like that I am using it as the backend for my blog posts, and its API gives me that flexibility to display that content where I want.\n View on DevTo  View on Hashnode   ","date":"Jan 25, 2022","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2022/WHAT-IS-THE-CORPORATE-WEBSITE.jpg","permalink":"https://www.funkysi1701.com/posts/2022/why-do-i-have-a-website/","series":null,"tags":["website",""],"title":"Why do I have a website?"},{"categories":null,"content":"I\u0026rsquo;ve been running my website on Azure Static Web Apps for a while and it is pretty cool.\nWhen you create a Static Web App on Azure you get asked for the github repo of your source code and even the branch to use. Once you have selected this, you get asked for the type of code to deploy, mine is Blazor Web Assembly but you can use Angular, React or Vue.\nYou now have three variables to fill in the location in your code of the Website, the location of your Azure Functions and the output location usually wwwroot. Once you have set these three you can preview the GitHub Actions file that will be created and added to your repository.\nI get something like this\nname: Azure Static Web Apps CI/CD on: push: branches: - feature/tempbranch pull_request: types: [opened, synchronize, reopened, closed] branches: - feature/tempbranch jobs: build_and_deploy_job: if: github.event_name == \u0026#39;push\u0026#39; || (github.event_name == \u0026#39;pull_request\u0026#39; \u0026amp;\u0026amp; github.event.action != \u0026#39;closed\u0026#39;) runs-on: ubuntu-latest name: Build and Deploy Job steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_\u0026lt;GENERATED_HOSTNAME\u0026gt; }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Client\u0026#34; # App source code path api_location: \u0026#34;Api\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### close_pull_request_job: if: github.event_name == \u0026#39;pull_request\u0026#39; \u0026amp;\u0026amp; github.event.action == \u0026#39;closed\u0026#39; runs-on: ubuntu-latest name: Close Pull Request Job steps: - name: Close Pull Request id: closepullrequest uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_\u0026lt;GENERATED_HOSTNAME\u0026gt; }} action: \u0026#34;close\u0026#34; This github action will run when you create a Pull Request to the branch mentioned in the file, or if you push code into the branch. This code get added into the .github/workflows/ folder and is the location that all github action workflows live.\nI haven\u0026rsquo;t done much with github actions, however I have used Azure DevOps quite a bit. Over on the Azure DevOps side I have created a pipeline that deploys to a Dev environment, then a Test environment and finally a production environment.\nLets have a look at the workflow that I ended up with and with can break down how it all works. Note I am new to Github actions so if there is a better way of doing this do let me know.\nname: Azure Static Web Apps on: push: branches: - main - develop - feature/* jobs: dev: runs-on: ubuntu-latest environment: name: Dev name: Dev steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_ORANGE_POND_09B18B903 }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Blog\u0026#34; # App source code path api_location: \u0026#34;Blog.Func\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### test: if: github.ref == \u0026#39;refs/heads/develop\u0026#39; runs-on: ubuntu-latest environment: name: Test name: Test steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_WITTY_DUNE_0A1A77903 }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Blog\u0026#34; # App source code path api_location: \u0026#34;Blog.Func\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### prod: if: github.ref == \u0026#39;refs/heads/main\u0026#39; runs-on: ubuntu-latest environment: name: Prod name: Prod steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_BRAVE_ROCK_0AAC63D03 }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Blog\u0026#34; # App source code path api_location: \u0026#34;Blog.Func\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### The first thing I did was create three Azure Static Web Apps, I am using the free tier so while this is trippling my costs it is all still free! Doing this created three github action workflow files, I deleted two and edited the third, but before I deleted them I made a note of the AZURE_STATIC_WEB_APPS_API_TOKEN. If you look in your settings -\u0026gt; secrets for your repo you will see secrets have been created, this is the secure token that github uses to update your static web app.\nWhile we are in settings we might as well look at environments. I created a Prod, Test and Dev environment that I was going to use in my github actions.\nEnvironments can have various rules setup on them.\n Required reviewers - this is like an approver, a user specified here must aprove for the workflow to be deployed Wait time - I didn\u0026rsquo;t use this, but it looks like a certain amount of time can be set to pause the deployment. (I assume to do some kind of manual check) Deployment Branch - specify what branch are allowed to be deployed to what environments. I specified develop, main and feature branches could be deployed to the Dev environment, develop and main could go on Test and main could go on Prod Environment secrets - I didn\u0026rsquo;t use this as my secrets were already created, however it looks like your secrets can be associated with a specific environment  Now that we have the static web apps setup and the environments lets look at the github action file.\nFirst of all I removed the PR stuff and just concentrated on pushes. I wanted my workflow to be.\n Push to feature branch Deploys to Dev env PR feature branch to develop Once merged code gets pushed into develop Deploys to Test env PR develop to main Once merged code gets pushed into main Deploys to Prod env (after approval)  The approval on deploying to production I think is probably overkill, but I still have it setup like that for now.\nMy gh action has three jobs defined as dev: test: and prod: they are all the same except they have the azure_static_web_apps_api_token that is correct for their environment.\nThey also each have a environment defined eg\nenvironment: name: Prod Lastly Test and Prod have an if test setup, if the test is false the job won\u0026rsquo;t run. Importantly it won\u0026rsquo;t fail it just won\u0026rsquo;t run.\nFor Prod this needs to only run on main branch so we have\nif: github.ref == \u0026lsquo;refs/heads/main\u0026rsquo;\nFor Test this needs to only run on develop so\nif: github.ref == \u0026lsquo;refs/heads/develop\u0026rsquo;\nI could have a test for develop to only run on feature/* but I have allowed it to run everytime.\nThere is loads more you can do with github actions, but hopefully this gives you a taste of some of the things you can do. I currently have a mix of Azure DevOps and github actions so I will be working on getting github actions to do more.\n View on DevTo  View on Hashnode   ","date":"Jan 10, 2022","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bj77rx0jetjdf7c24nhk.png","permalink":"https://www.funkysi1701.com/posts/2022/using-github-actions/","series":null,"tags":["GitHub","DevOps"],"title":"Using GitHub Actions"},{"categories":null,"content":"Lets have a look at what my goals were for 2021. I had eight of them, lets look at them one by one.\n  Azure certification. In May of this year I sat and passed the Azure Fundamentals exam. I am calling this goal as achieved.\n  Mentoring. I didn\u0026rsquo;t do anything about working with others or mentoring, so not sure I achieved this one. However I do now work in a development team and I have been reviewing others code and having my own reviewed. We have a junior developer and I am enjoying the opportunities I have to work with him and share my wisdom.\n  F#. I have done zero work with F# in 2021, so this one I didn\u0026rsquo;t achieve.\n  Cosmos db/Mongo db. I have worked with both of these technologies in 2021. I used Cosmos the most with my website, and storing data for it. I used Mongo/Atlas for auditing for a project I did for my previous job. I wouldn\u0026rsquo;t say I was expert in either of these, but I am starting to get a flavour of non SQL Server databases. I should also note that I am using mysql a lot in my latest job, so another non SQL Server technology that I get to use on a daily basis.\n  Give a talk. I said last year that maybe I would make baby steps towards doing this, and I have. In the interview for my new job I gave a short presentation (I thought it was bad, but others didn\u0026rsquo;t!) I also gave a short introduction to Blazor talk which went down well.\n  Mandlebrot Generator. Did nothing on this one as well. I may have googled the code but that is as far as I got.\n  Pwned Pass Mobile App. I got an increase in users in 2021 and hence it it still running and I still pay for the API key. I am still considering what to do with it.\n  Time for me. Achieved this one, had plenty of time for myself.\n  Not a bad year but what are my goals for 2022?   Video. I have just purchased a green screen, so my first goal is to learn how to use OBS, how to light myself properly without getting horrible reflections. I have the content for my first talk, the talk I gave at work about Blazor.\n  Conference. Attend an in person conference. I am booked to attend Scottish Summit, however it has been postponed due to COVID-19.\n  Blog More. I have been neglecting writing blog posts a bit recently, so I want to do more.\n  Metrics. I have been recording various metrics from twitter, github etc and I would like to expand this and make it a service.\n  Profile Pic. My profile pic is getting a bit old, so it would be nice to update the images I use online and improve my personal brand.\n   View on DevTo  View on Hashnode   ","date":"Jan 1, 2022","img":"","permalink":"https://www.funkysi1701.com/posts/2022/2022-goals/","series":null,"tags":["Goals"],"title":"2022 Goals"},{"categories":null,"content":"How exciting my Lynx Computer from my childhood has come home, all 96k of it pic.twitter.com/aeN38KBiS8\n\u0026mdash; Simon Foster (@funkysi1701) December 26, 2021  I can\u0026rsquo;t remember the syntax for BASIC, luckily I have been able to find the Manual .\nAll the commands are listed inside so lets see what we can do.\nThe Lynx presents you with a command prompt in which you can type text. Back in the 80s we had a tape player to load programs from tape, however I don\u0026rsquo;t have one today so only programs I write can be run.\nPRINT - To write Hello World, you can just type PRINT \u0026ldquo;Hello World\u0026rdquo; and Hello World appears on the screen. To Write a program that displays Hello World, you just write the line number first.\n10 PRINT \u0026#34;Hello World\u0026#34; To run this you type RUN To view the code you type LIST\nTo Edit a specific Line you can use Ctrl+E and type the line number, or you can just write the line out again.\nCLS - This command clears the screen\nINPUT N - stores text typed by the user and stores it in the variable N\nGOTO N - Execution of code continues at Line Number N\nThe first Program I wrote with a bit of help from my boys.\n10 CLS 20 PRINT \u0026#34;What is your Age?\u0026#34; 30 INPUT N 40 IF N\u0026gt;5 AND N\u0026lt;41 THEN PRINT \u0026#34;a good age\u0026#34; 50 ELSE IF N\u0026lt;6 THEN PRINT \u0026#34;a spaceman\u0026#34; 60 ELSE IF N\u0026gt;40 THEN PRINT \u0026#34;too old\u0026#34; My 4yo didn\u0026rsquo;t like being \u0026ldquo;too young\u0026rdquo; in the original version, so my 6yo helped me change him to be a \u0026ldquo;spaceman\u0026rdquo;.\nNot bad and it was fun pair programming with a 6yo, all my typos were quickly spotted, and he easily understood the logic of IF/ELSE/THEN statements.\nThe Lynx comes from 1983 and has just 96k of memory. I am very lucky it actually still works, however I have been able to find an emulator so I can write Lynx BASIC from the comfort of my laptop. jynxemulator , it is also on github but it doesn\u0026rsquo;t include the ROMs so getting from the website is a better option.\nThe developer experience today is so much nicer than it must have been in the 1980s, however back then distractions must have been much reduced.\n No internet or google to get answers to your questions No Copy/Paste of text No Load/Save (unless you have a working disk drive or tape player!) No IDE No Build or Release process just type RUN  I then have additional sites, that I use as my playground for learning new tech. I can easily link between them and I can tweak the style so they \u0026ldquo;fit\u0026rdquo; nicely together.\nI am still considering what to do with dev.to. I like that I am using it as the backend for my blog posts, and its API gives me that flexibility to display that content where I want.\n","date":"Dec 28, 2021","img":"https://www.funkysi1701.com/images/FHi_NyOXEAo9YbG.jfif","permalink":"https://www.funkysi1701.com/posts/2021/back-to-basic/","series":null,"tags":["BASIC","History","Programming"],"title":"Back to BASIC"},{"categories":null,"content":"The final episode of Star Trek: The Next Generation features a few scenes set 25 years into the future. That episode aired May 1994. The newest Star Trek TV show Star Trek Picard has just aired its first episode and this is 25 years and 8 months after that episode. This article may contain minor spoilers for the first episode of Star Trek Picard, you have been warned.\nI thought it would be interesting to compare the two time periods.\nFate of Picards crew In All Good Things\u0026hellip; all of Picards crew are still alive except one. Deanna Troi passed away but it is never revealed how.\nIn Star Trek Picard, we know Data has died as featured in the feature file Star Trek Nemesis. From the trailers we know Riker and Troi are still alive. We do not know the fate of Crusher, LaForge and Worf yet, but I am going to assume they are still alive somewhere.\nIn both timelines we have one dead crew member.\nLiving arrangements In both time lines Picard is living in France at his vineyard.\nIn All Good Things\u0026hellip; we do not know who Picard is living with, it is assumed he lives alone, which is why Geordi feels the need to check on him, once he learns about his Irumodic Syndrome.\nIn Star Trek Picard we learn that Picard is living with a Romulan couple, due to the involvement Picard made to save the Romulan people when their star went supernova. Picard appears older but for the most part in good health, he can\u0026rsquo;t for example run up a flight of stairs (he is 92 so not surprising really!) and is plagued by dreams from his past.\nRomulan Neutral Zone In All Good Things\u0026hellip; there is no Neutral Zone, the Romulan empire has been taken over by the Klingons.\nIn Star Trek Picard one would assume there is no Neutral Zone as well because the Romulan star system was destroyed. A handful of Romulans survived mostly due to the actions of Picard.\nLooks As we have seen Patrick Stewart does not appear to be ageing. Below is a comparison of how he looks in the two time frames.\nI really enjoyed the first episode of Star Trek Picard and it is going to be interesting seeing what other similarities and differences there are between it and All Good Things\u0026hellip;\n","date":"Dec 15, 2021","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2022/picard.jpeg","permalink":"https://www.funkysi1701.com/posts/2021/a-comparison-of-all-good-things-and-star-trek-picard/","series":null,"tags":["StarTrek","Picard"],"title":"A Comparison of All Good Things... and Star Trek Picard"},{"categories":null,"content":"Welcome to Day 5 of the Festive Tech Calendar! Usually at this time of year I like to take the opportunity to look back on the last 12 months, highlight some of my achievements and make a few goals for the new year.\nLike many of you 2021 has been a difficult year, but it has also been a year of change, and I am in a much better place now than at the start of the year.\nTo tell you my 2021 story I first need to give you a bit of a history lesson about my career. So, lets fire up your flux capacitor (or Tardis of similar value) and journey back to 2006.\nBack to 2006 In 2006 I got my first IT job. My housemate, who was working as an IT Manager for a small Health and Safety Company, suggested I come work with him. I was currently working an admin job with no idea what to do with my life, so I said yes.\nSo, in October 2006 I joined as a member of the IT Team. I worked mainly on first line support, but as the IT department was small, I learnt about all sorts of IT things, from fixing printers, setting up the CEOs BlackBerry mobile phone with emails, administering active directory, writing SQL queries to swapping tapes for the weekly backups.\nThe company had many faults which I won\u0026rsquo;t discuss here, but the work was varied, and I was always learning something new so I stayed with the company.\nAt the end of 2010, the IT Manager resigned, shortly followed by the rest of the IT department. In January 2011 I was IT Manager, I had no staff, plenty of IT problems and no clue what I was doing.\nThis was an amazing time for me, I learnt how to interview and hire staff, I learnt more about the different systems we used. It was approximately at this point in time that I switched from being exclusively a SysAdmin to starting to learn Development.\nI had been creating simple websites for a while, mainly using PHP and MySQL. But since I started working in IT I learned more and more about databases, SQL Server and writing T-SQL scripts. I was the companies \u0026ldquo;database guy\u0026rdquo;, so the company naturally asked me to do more and more database work. This led to learning other technologies like MS Access and C# so I could do more and solve more of the company\u0026rsquo;s problems.\nHowever, the company was small, and I would always be dragged in to fix SysAdmin problems so I never managed to spend 100% of my time doing development, so in 2016 I moved on to my first full time development job.\nThe new job was great, but I kept in touch with my old CEO, Ally and helped out with bits of work on the side, writing SQL queries, updating the odd bespoke application.\nBack where it all began In 2019 I started formulating a plan to go back to my old job as a contract developer. To my amazement Ally almost bit my arm off to get me back, agreeing to all my terms. So, in the summer of 2019 I started full time as a contract developer, mostly working from home with the occasionally meeting in the office and supporting the business with IT issues.\nMy second stint with the company was different than my first, I was able to concentrate almost exclusively on development work. As the only developer I was in full control of development, I decided to use dotnet and Blazor.\nLate in 2019 I was given advanced warning that the company was being sold. In Feb 2020 the company did get sold, the new owners kept me on to finish the project I was working on but made me a permanent member of staff. In March 2020, the whole company began to work from home due to COVID-19, with parts of the business being furloughed.\nThis was a difficult time; on top of the stress the whole world was feeling from the global pandemic I was trying to assist with various IT integrations that you would expect when any company gets sold.\nJanuary 2021 This is pretty much where you find me in January 2021. I was working remotely developing internal applications with dotnet/Blazor with the occasional Teams meeting to demonstrate what I had built so far.\nWorking on your own as a developer is great! You start work and can write code all day. There is no daily stand up, you don\u0026rsquo;t need to explain what you have been working on, or what challenges you are facing. You don\u0026rsquo;t have any process or formality to follow, only what you impose on yourself.\nI created a build/release pipelines in Azure DevOps, I created pull requests that would kick of a build, run my unit tests, and run some static code analysis. I would glance over my PRs but 99% of the time I would approve them.\nThis was the greatest weakness of working on my own, no one to suggest ideas of better ways of writing code, no one to bounce ideas off, no one to encourage or be encouraged when I figured out something clever or offer to help you overcome a problem.\nRedundancy In Mid July I was told that the development function was no longer required, and my job was at risk of redundancy. As I was the only one working in development it sounded very much like a done deal. I had a couple of redundancy meetings to attend and I would need to talk through how various applications worked as part of a handover.\nTo be honest I agreed with the decision to make me redundant. New development work was not coming through, although the users of the applications I had developed loved what I had created, upper management were keen to migrate off these systems and use more centrally managed systems. If I was in charge, I would probably have made a similar difficult decision.\nAs soon as I found out I was at risk of redundancy I called up a couple of recruitment agents I had spoken to before. They told me that it was a great time to be looking for a job and immediately arranged some interviews for the next week. I also took advantage of social media and asked on twitter if anyone was looking for developers like me. To my surprise this resulted in at least one interview.\nOne thing I stressed in all my conversations with recruiters and companies I spoke with, was that I was looking for a team. I wanted to work in a team, bounce ideas off others, mentor others, learn from others. This was the most important thing I wanted in whatever my next role was going to be.\nOther important things for me was continuing to use Azure or similar cloud technologies. However, I always think that the tech stack and technology can be learnt on the job, tech moves so quickly these days that you have to be constantly learning to stay relevant.\nAbout two weeks after I first found out about my impending redundancy I contacted (or more likely was contacted by) a third recruitment agency. It was this third one that eventually got me a job, however all three were brilliant, and kept me updated and answered all of my questions.\nThe next few days were packed with phone calls, interviews, tech tests. I had a lot of different types of companies that I spoke with, from Software Consultancy companies, FinTech companies, Energy suppliers, Legal companies and many others.\nDuring this time of interviews, I can think of only two in person interviews. The rest made use of Teams, Zoom, Google Meet and even Skype (yes at least one company still uses it for video chats!)\nThe usual way the process went was a conversation with the recruiter about a role after which my details were sent to the company, If the company liked the sound of me an initial informal interview was arranged, after that a second more technical interview was held, sometimes there would be a tech test, sometimes it was more technical questions being asked in the interview. Some companies had more stages that this, but this was what I typically encountered.\nI lost count of the actual number of interviews I had, but I used a spreadsheet to try and keep it all straight in my mind. The last thing I wanted was to ask a question about the wrong company!\nPositive No I got plenty of noes from these interviews, however all were phrased as a positive no. That being they liked me as a person but something about my skill or experience wasn\u0026rsquo;t quite right. A friend once described me as an odd mix of junior and senior developer. I agree with that statement, I have a lot of experience in some areas and a real lacking in others.\nA common one was working in a team. Being a lone developer is not good for gaining experience working in a team. However, each time I spoke with companies I stressed that the team was what I was looking for.\nAnother weakness of mine is front end. When I am building something, functionality is more important than getting it looking good. I came from a database and backend start, so it is only natural that I am more at home working on these things. Also, my most recent projects are working with Blazor, which is brand new and few companies are doing much with it yet.\nI tried really hard not to be discouraged by these noes and that the right company was just around the corner. One no hit harder than most. I had an initial chat with the team of a FinTech Company and then there was a second interview with the CTO which I felt had gone OK.\nFrom what I had heard it ticked a lot of boxes, the team sounded good, with lots of support and development opportunities and they were moving to the Cloud, so my Azure experience sounded ideal. The CTO liked me and thought I would be a good fit, but I was part way between a junior and a senior, so he had gone away to try and make a bespoke role for me, eventually it would be a no.\nLooking back, it is easy to see, that this was really encouraging. They tried to create a bespoke role for me! However, at the time all I could think about was all the noes I was hearing.\nThe job for me About the same time, I had my initial interview with the company I would eventually accept a role with. I kept being told that there was a tech test to do, but for some reason it never got sent to me, so the interview featured a lot of tech questions and they really grilled me. I came away without much of a sense of if it went well or not.\nHowever, they liked me and wanted me to do a presentation at their office. I am rubbish at public speaking and doing presentations, so I thought I will do this presentation, but it is very unlikely they will like what I do.\nThe topic of my presentation was how I upskilled a team on a new project or technology. As a lone developer what an earth could I do for a topic like that? I thought about what I had done to upskill myself on various things, however that doesn\u0026rsquo;t really address the question, as they wanted to know more about what I had done to upskill a team.\nI thought back to a time when I helped get a largely un-version controlled codebase into source control and automated the build and release pipeline using Azure and Azure DevOps. This involved talking to SQL and data developers so could be a better option for a presentation.\nI arrived at the office to do my presentation. First of all, I parked next to the factory where all the forklifts were, I was directed to the correct place to park. Then I tried to go to the wrong building, a very helpful employee helped me go to the correct place. I was introduced to the people who would be interviewing me, one of whom I had worked with in the past, who immediately said you should hire this guy, he is an excellent developer! As introductions go, that\u0026rsquo;s not bad!\nIn my opinion the presentation was bad. I didn\u0026rsquo;t have a PowerPoint or similar to go with my talk, so I just waffled on for ten minutes about what I had done a few years back about how I helped get source control being used.\nThe next day I got the job offer. I was very surprised to get the offer as I thought I had done so badly the day before they would give me another positive no. I took a few hours to think about it, however all my other applications were either not started or waiting for the next stage, so I was 99% sure I would accept.\nI had no idea what to expect when I started, as I didn\u0026rsquo;t really ask many questions at the last interview as I was convinced, I was going to get a no after my presentation.\nI worked my notice period and at the start of October I started my new job. My contract only arrived the day before I started, so in the days before I started my mind was making up reasons why the job would disappear.\nThe job started with a two-day induction. Things that were covered on the induction were, mental health, sleep cycles, company values, health and safety and of course a bit of form filling and photocopying passports that you would expect on a first day.\nConclusion Back when I had been job hunting, each time I stressed the importance of team. The company I had joined had an amazing focus on supporting its staff and getting the best from each other. I had got the best possible fit for the kind of team I was looking for.\nUsually when I start a new job, I get a large dose of imposter syndrome. Now, two months after I started, I can say, I didn\u0026rsquo;t really get that this time. Maybe it is how the team functions, maybe it is my level of experience, or maybe its just something else.\nI am happy in my new job. There is a lot to learn, both technology and how to work well in this team. But I am supported, there are people to ask question, I am contributing almost from day one. I have already shared a brief talk about my experience with Blazor, which went down better than I expected. I am excited to learn what is next for me.\nIf you have read all the way to here, Thank you! This is my story and I hope you have found it interesting. If you are looking for a new role, I would encourage you to focus on the soft skills you are looking for next, if you concentrate on team like I did, there is a good chance you will land in an amazing team.\nDon\u0026rsquo;t forget to check out some of the other great content that is being created by all the amazing Festive Calendar 2021 contributors.\n","date":"Dec 5, 2021","img":"https://raw.githubusercontent.com/gsuttie/festivetechcalendar/main/calendar.jpg","permalink":"https://www.funkysi1701.com/posts/2021/lone-developer-to-senior-developer-my-2021-story/","series":null,"tags":["FestiveTechCalendar2021","Career","LifeStory"],"title":"Lone Developer to Senior Developer, my 2021 story"},{"categories":null,"content":"We are over halfway through 2021, let\u0026rsquo;s have a quick look at some things I have done this year:\n  Deployed a new version of the Pre Qualification questionnaire website for my employers\n  Passed Azure Fundamentals Exam\n  Mongo DB Atlas - experimented with using it for auditing my application\n  SonarCloud - tried to improve the quality of my code\n  Azure DevOps API -\u0026gt; built an application to show builds and releases from my Azure DevOps organisation, code is on github\n  Experimented with replacing SQL Agent Jobs with Azure Functions, ended up abandoning this project but was a great learning experience\n  Played with MS Graph - displaying profile pic from Azure AD but I am the only one in my org with a photo so abandoned this as well\n  Added a captch to a website to reduce spam\n  Charted my Gas and Electricity usage using the Octopus Energy API\n  Charted various metrics from services I use, eg twitter followers, github commits, blog posts published\n  Started learning react, easily connected it with Azure AD\n  Integrated my employers project management system with a postcode API and google maps so can see where in the country different projects are located\n  Tested dotnet 6 preview and the latest Visual Studio 2022 preview\n  I was made redundant and got a new job\n  Wrote my first piece of dotnet code to run on a Raspberry Pi\n  Kept cost of Azure down, by removing unused services, refactoring and optimizing existing services.\n  Wow, I have done loads this year, what is next?\n","date":"Aug 31, 2021","img":"","permalink":"https://www.funkysi1701.com/posts/2021/more-than-halfway-through-2021/","series":null,"tags":["Goals"],"title":"More than halfway through 2021"},{"categories":null,"content":"So today I sat the Azure Fundamentals Certification Exam and passed! Really pleased with myself at achieving this. It was one of my goals for 2021 so I can tick that off.\nBack in January I booked my first exam, however due to technical problems I didn\u0026rsquo;t get as far as the Exam. In order to be able to sit an exam like this you need a webcam and microphone so you can be monitored remotely to check that you are not cheating. Something in my network was blocking them from seeing my video so it got cancelled before it started.\nMy theory is that either my internet connection was playing up at the time, My Pi Hole was blocking something, or something else on my network was blocking the video feed.\nThis cancellation put me off Certifications, as I wasn\u0026rsquo;t sure how to debug the issue and find out exactly what the problem was.\nA few months back I won a free Azure Certification for taking part in a skills challenge run by Gregor Suttie and Richard Hooper and this was due to expire at the end of this month so I thought why not book a second exam and see if I can get past the technical problems. I didn\u0026rsquo;t do any exam prep as I thought, I wouldn\u0026rsquo;t get that far.\nThis time I connected directly to my router, bypassing most of the network, firewall and other items on my network that could possibly cause an issue.\nI cleared my desk, took photos of my ID, took pictures of my desk from every orientation, and waited for the technical issues to start.\nI was in a queue waiting for the exam to start.\nA connection issue has occurred you have been sent to the back of the queue. Oh here we go again!\nBut no I connected with the invigilator, who asked to confirm my monitors were disconnected and to move my wallet out of reach off my desk.\nAnd we are off, I am answering questions from the exam.\nI won\u0026rsquo;t go into detail about the questions, but I whizzed through them, most made me think, some I guessed at. I wasn\u0026rsquo;t expecting to pass, I thought maybe half marks or just under due to my familiarity with Azure. (I have been using it for years which must count for something!)\nI was wrong I passed comfortably and now I have my first certification.\nMy advice for you if you have been using Azure for a while is to book this exam and see how you do, you may well pass like me. There are plenty of opportunities to get a free exam, attending conferences like Ignite often qualify you for one, look out for challenges and competitions by #AzureFamily people on twitter as they are very encouraging and helpful in your Certification journey.\nAnd yes I am thinking about what Certificate to do next!\n","date":"May 11, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eytphopn3inx2x77m9n5.png","permalink":"https://www.funkysi1701.com/posts/2021/my-road-to-certification/","series":null,"tags":["Azure","Certification"],"title":"My road to Certification"},{"categories":null,"content":"I have had a Raspberry Pi for a few years and recently I connected it up again, I plugged in the camera and everything worked.\nTo start off you can view photos from the camera with the raspistill command. With a bit of clever scripting and the crontab I got the Pi taking pictures every 60 seconds. Even managed to take a nice picture of a robin.\nHowever scripting isn\u0026rsquo;t really programming, and I would like to write a bit more code. Dotnet can run everywhere these days and it made sense to see if it would run on a Raspberry Pi.\n@pete_codes has written a nice guide to getting started with dotnet on a Raspberry Pi https://www.petecodes.co.uk/install-and-use-microsoft-dot-net-5-with-the-raspberry-pi/ This guide and the nuget package https://www.nuget.org/packages/Unosquare.Raspberry.IO/ was all I needed to get started taking pictures with my Pi.\nMy initial goal is to take some wildlife pictures, stick my camera to a window and take pictures of what flies/crawls/jumps past the window.\nThe code I have written so far is available on github https://github.com/funkysi1701/RaspberryPiDotNet So far the code takes a picture, uploads this file to Azure Blob Storage (so as not to fill up the Pi with too many image files) and deletes the image locally.\nRun a dotnet publish -c Release and then cron can run dotnet RaspberryPiDotNet.dll (with full paths to the relevant files)\nI then use crontab to execute the code every 60 seconds.\nMy code has an appsetting.json file which has a couple of settings that need completing for my code to work.\nStorage: This is the connection string for Azure Blob Storage LocalPath: This is the path to where the camera will save its photos to, something like /home/pi/ is all you need but feel free to specify what you need.\nOnce the photos are in blob storage I plan to display them somewhere, add options to delete what I don\u0026rsquo;t want, maybe do something timelapsey.\nI also don\u0026rsquo;t have a proper release pipeline and this grates on me a bit. I have been doing a mixture of writing code in VS and pushing that to github and then doing git pull on the Pi, and also writing code directly on the Pi. (VS Code can connect via SSH which is pretty cool!)\n","date":"May 10, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/e21z7vbamy6w6akhhiwd.jpg","permalink":"https://www.funkysi1701.com/posts/2021/dotnet-on-a-raspberry-pi/","series":null,"tags":["RaspberryPi","Programming","DotNet"],"title":"DotNet on a Raspberry Pi"},{"categories":null,"content":"Back when I was a kid, I used to record our weekly gas and electricity meter readings in a little notebook. We then typed these reading into a spreadsheet (this was in the pre-Excel days), which allowed plotting as a line graph.\nHow would I go about doing a similar thing today? First off, I have a smart meter that submits meter readings every 30 minutes or so. However, I do not know anyway to get access to these readings directly, short of manually recording them like I did 30 years ago!\nOctopus Energy have a public API which allows you to pull your consumption readings. The smart meter sends your usage to your energy supplier, in my case Octopus, they then process these readings and allow them to be queried with an API they have created. It is not a direct connection to your data, but it is the next best thing.\nOther energy suppliers will hopefully follow this example and allow users access to their consumption data.\nHow do I use the API? Using the API is straight forward. Octopus supply you with a secret which you use to authenticate against the API with Basic Auth, no password just a username. Then you just need to pass some details of your meters to get an object containing the last few days meter readings.\nAPI Docs  GET /v1/electricity-meter-points/{mpan}/meters/{serial_number}/consumption/ GET /v1/gas-meter-points/{mprn}/meters/{serial_number}/consumption/  {mpan}/{mprn} of your gas or electricity meter, and {serial_number} is the serial number of the meters.\nSomething to be aware of, I initially collected the last days consumption, which worked, however on one day I encountered a gap in the data for electricity. So, I changed to collect and store the last month\u0026rsquo;s data. I can then query this for what I need.\nUsually, the last 24 hours of data is available after midnight of that day. e.g. at midnight 2nd March all the data for 1st March should be available. This is not guaranteed so don\u0026rsquo;t rely on it, however I see no problem with having a few days delay between charting your usage.\nI am still testing this out but so far, I have three charts for gas (and the same for electricity), the first chart covers a 24-hour period, the next covers a day total over 2 weeks, the final chart covers a total for each month (as I write this I have less than a month\u0026rsquo;s worth of data!)\nFor the day and 2 weeks charts, I plot a comparison line of the previous period so you can easily compare the current and previous usage. From my limited testing I have already discovered my usage is very similar day to day.\nAnother point of interest is that gas consumption is in m^3 and electricity is in kW/h. If you are interested in trying the Octopus Energy API, here is a referral link .\n","date":"Mar 7, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/63ep8hp6ipyn2l4emiop.png","permalink":"https://www.funkysi1701.com/posts/2021/charting-my-energy-usage-with-the-octopus-energy-api/","series":null,"tags":["API","OctopusEnergy"],"title":"Charting my Energy usage with the Octopus Energy API"},{"categories":null,"content":"Azure DevOps release pipelines have lots of options to do things how you want. One of my favourites is the option for approval.\nThere are two ways you can do approvals Pre and Post deployment. Lets look at both.\nPre Deployment Approval Lets imagine you have a simple deployment pipeline that deploys to a test/development environment before deploying to a production environment.\nPre Deployment Approval happens immediately before the release so in this example, click in the ellipse before the Prod release step.\nYou will get a screen like the above, you can select what users need to approve it and how long approval waits before timing out, the default is 30 days, but I tend to use a shorter time out of 3 days.\nPost Deployment Approval Post Deployment Approval happens immediately after the release so in this example, click in the circle after the Test release step.\nYou will get a screen like the above, with the same settings as before.\nThat is pretty much all there is to approvals so either option will prompt you to approve before anything gets deployed to your production environment.\nDeployment Hours To complicate matters I make use of the following setting to define deployment hours. This setting will start the Prod deployment at 3am Mon-Fri.\nIf I configure Post Deployment Approval, as soon as my deploy to Test has completed a request for Approval is sent.\nIf I configure Pre Deployment Approval, at 3am Mon-Fri a request for Approval is sent (not ideal if you tend to be asleep at 3am)\nSo it looks like Post Deployment Approval is more useful for my use case. However if you deny approval either in Pre or Post approval this will mark the deployment as failed and show Red in your list of deployments.\nFrom a casual glance it looks like the deployment to Test is failing, it isn\u0026rsquo;t I am just opting to not continue my deployment to production.\nMy Pipeline This is how I have my pipeline setup. Deployment happens on Test and doesn\u0026rsquo;t have a post approval step.\nAfter Test an empty stage called Approval runs and that has a post deployment approval, this happens immediately after Test so you get asked straight away for approval.\nProd does not start as I have my deployment hours configured. Once it is time for deployment to Prod to start it executes.\nNow a casual look at my past releases, you can easily see which have been stopped by approval and which have failed due to whatever issue, and which have run all the way through to Prod.\nAnd deployments to Prod can only ever run during my defined deployment window.\nI am interested to hear how you have your deployment pipeline setup. Do you make use of Pre or Post Approvals? Do you ensure deployments always happen at specific times?\n","date":"Feb 14, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/i/9k6vo6pfv434u7yq3mt4.png","permalink":"https://www.funkysi1701.com/posts/2021/azure-devops-release-pipelines-pre-and-post-approval/","series":null,"tags":["AzureDevOps","DevOps","Azure"],"title":"Azure DevOps Release Pipelines Pre and Post Approval"},{"categories":null,"content":"I didn\u0026rsquo;t make any goals for 2020, or if I did, I didn\u0026rsquo;t officially announce them. 2020 has been a hard year for all of us, 2021 is going to be better.\nHere are a few ideas for my goals for the year ahead.\n Azure certification Mentoring F# Cosmos db/Mongo db Give a talk Mandlebrot Generator Pwned Pass Mobile App More time for me    I want to get a certificate to show how much I know. The obvious area for this is Azure. I spend a lot of time playing with Azure, building and deploying to the platform I should be able to get certified in this area. I actually had an exam booked in 2020 but it was an in-person exam so was cancelled when Covid hit. I plan to sit the foundation exam in the first quarter of 2021 and I will take it from there.\n  I have had a few times recently where I have been reminded that I don\u0026rsquo;t have much experience working with others. Mostly because I work in a one-man development team so there are limited opportunities in the workplace. I have a few ideas to change this, but this is one of my top priorities for 2021.\n  \u0026amp; 4) My experience is very Microsoft and C# so I want to spend some time exploring and learning tech that is adjacent to this. A functional language like F# sounds like a good compliment to my existing skills and my data skills are also very SQL server so some document database skills would be a good place to spend some time.\n  Doing a talk has been on my list for years. I am terrified of doing one and with everything online now I don\u0026rsquo;t know if that makes things easier or harder. I don\u0026rsquo;t think 2021 is going to be the year for this but maybe I will take baby steps towards this goal.\n  Twitter recently reminded me about watching a Mandlebrot generate one pixel at a time in the 1980s. I would love to explore the code used to generate them and how fast they currently are to produce.\n  My Google play store Xamarin forms app that uses the HIBP API has less than 100 users and I am considering closing it down, especially as I pay a monthly fee for API keys. I haven\u0026rsquo;t really decided what to do, maybe I will spend some time improving it, maybe I will close it down, maybe I will build something else.\n  For many people 2020 has been a hard year, I am one of those\n  ","date":"Dec 28, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/goals-for-2021/","series":null,"tags":["Goals"],"title":"Goals for 2021"},{"categories":null,"content":"I saw a tweet about building a twitter clone being harder than you would think. So this of course started me thinking how I would go about building something like that.\nOk so where would I start? First a few assumptions.\n Development by a lone developer ie me Tech stack will be dotnet and other tech I am familiar with Database backend, probably SQL Server but I might use table storage for cost reasons should I try and actually build this. However if I design this well this should be something that could be swapped out as the system grows User accounts on the system will be small as I can\u0026rsquo;t imagine anyone ever signing up. Why sign up to a social media platform with no users?  I guess the next question is what is Twitter? A website that allows you to share 280 characters of text with your followers, allows you to follow other users updates and allows other user to follow your updates.\nIt also has an API that allow you to do almost everything that you can with the website.\nThen there are of course mobile apps to consider but I am going to assume this is out of scope, however assuming a good enough API then this shouldn\u0026rsquo;t be a problem for future development.\nFirst Steps To start off with I would concentrate on the API, and then build a web client that makes use of the API.\nSo what would my MVP (minimum viable product) be?\n User can authenticate with my API to get a token which allows access to other API endpoints User can create a tweet User can view own tweets User can view tweets of another user User can view tweets in their timeline User can follow/unfollow other users User can search for other users User can search for keywords in tweets  I think that is probably sufficient to build my MVP for the API.\nAn interesting side note is that I could use the OAuth Twitter authentication to allow users to login to my twitter clone with the real twitter login details. However this makes no sense to me as we are essentially adding a dependency on the real twitter.\nSo what would I use for the frontend? I would start off with a Client Side Blazor frontend. Once I had a proof of concept that worked, I would think about styling and adding the UI elements that are familiar to twitter users.\nThe beauty of Client Side Blazor is that I can host cheaply in azure storage and distribute around the world via a CDN.\nDue to the high number of times that follower and following count and other stats are queried I would consider storing these in the database and include a regular job to recalculate them so they don\u0026rsquo;t get out of sync with the data.\nHaving said all of this I am very tempted to fire up Visual Studio and see how far I get, and what problems I encounter along the way.\n","date":"Dec 22, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/building-a-twitter-clone/","series":null,"tags":["Twitter","Design","Architecture"],"title":"Building a Twitter Clone"},{"categories":null,"content":"It has been a bit of a mad week this week. I joined a new team so lots of time learning what\u0026rsquo;s what and also being pulled in two directions as usual demands come through on top of that.\nMy blog runs on Blazor and I have been making use of JavaScript interop to update the html headers and update the page title to match the blog post article. This works great, I load the page and check the headers and they were saying what I wanted.\nThe problem was I wanted to add tags for twitter cards This means that when I paste a link to my blog on twitter you get a nice preview and pic of me in the tweet. This was not working at all even though I had the correct headers.\nI eventually figured out that the problem was the fact I was using JavaScript to update my headers after the page had been initially loaded. Twitter was fetching my page before these headers got added and therefore couldn\u0026rsquo;t see the twitter card headers.\nMy solution was to use invalid html. Not ideal but it works. I added the required html tags in the body of my page using Blazor/C# instead of using JavaScript to add them into the header. Twitter appears to not be fussy in finding them in the wrong place.\nTwitter provides a validator tool at Twitter Card Validator which my website now passes.\nNot much else to say this week, apart from I am missing Visual Studio and C#, I have been mostly using VS Code on Linux and looking at php which isn\u0026rsquo;t as much fun as my usual day job.\n","date":"Dec 12, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/005-twitter-cards/","series":null,"tags":["Twitter"],"title":"#005: Twitter Cards"},{"categories":null,"content":"I use sp_send_dbmail to send results of sql queries by email to business users. Recently an issue was raised that data was being cut off after 255 characters. To fix this I added @query_no_truncate = 1, however this stopped the column headings from being included. No idea why you can\u0026rsquo;t have all the data and column headings but there you have it.\nWhat I am doing now is running 2 queries, one to get the headings, and one to get the data. In theory you should be able to combine them with a Union however you then have datatype issues for non text columns so I gave up with that idea.\nMy results have 60 something columns (don\u0026rsquo;t ask its for a data import into a third party system!) so I am not typing them all out. I can shove query results into a temporary table and then execute to get a list of columns.\nSELECT name FROM tempdb.sys.columns WHERE object_id = object_id(\u0026#39;tempdb..#TempTable\u0026#39;) However I need my list to be horizontal so I can use as column headers. I can use dynamic SQL and a pivot to flip them round.\nDECLARE @cols AS NVARCHAR(MAX), @query AS NVARCHAR(MAX)  SELECT @cols = STUFF((SELECT \u0026#39;,\u0026#39; + QUOTENAME(name)  FROM tempdb.sys.columns  WHERE object_id = object_id(\u0026#39;tempdb..#TempTable\u0026#39;)  FOR XML PATH(\u0026#39;\u0026#39;), TYPE).value(\u0026#39;.\u0026#39;, \u0026#39;NVARCHAR(MAX)\u0026#39;),1,1,\u0026#39;\u0026#39;)  SET @query = N\u0026#39;SELECT \u0026#39; + @cols + N\u0026#39; FROM ( SELECT name FROM tempdb.sys.columns WHERE object_id = object_id(\u0026#39;\u0026#39;tempdb..#TempTable\u0026#39;\u0026#39;) ) x PIVOT ( MAX(name) FOR name IN (\u0026#39; + @cols + N\u0026#39;) ) y\u0026#39; ","date":"Dec 6, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/weekly-update-004/","series":null,"tags":["SQL"],"title":"Weekly Update #004"},{"categories":null,"content":"Been a quiet week, so wasn\u0026rsquo;t expecting to have much to write on here, however a few things happened worth talking about.\nMy First PR A comment was made to me to do something with the postcodes that are in the system I am developing. Find out what projects have postcodes near each other, and that way work can be grouped together and reduce potential mileage costs of staff that need to visit these projects.\nA quick google search found https://postcodes.io/ which has an API that returns nearby postcodes. It also has a C# wrapper https://github.com/markembling/MarkEmbling.PostcodesIO A comparison of what was being returned from the wrapper and what the API said should be returned revealed that the distance between postcodes wasn\u0026rsquo;t being returned.\nAs the code was on GitHub I could easily see how easy or difficult it might be to add the missing bit of information. It was easy! So, I forked the repo and made the change. I published the change to a private NuGet repo in my Azure Dev Ops account. That way I could try my revised package to check it did what I wanted.\nI left a message on the GitHub project letting the owner know I had a potential fix for an issue. The project hadn\u0026rsquo;t been updated in over a year, so the owner may not be interested, or the project may have been abandoned.\nI was in luck just 17 hours after I left a message the project owner said to create a Pull Request, which I did and shortly afterwards my code had been merged in and an updated version of the package existed in the public NuGet feed.\nI have been thinking about contributing to open source for a while. However, I had not seen a project I wanted to contribute to, or a problem that I knew how to fix until now that is.\nPHP This week I had a call asking me if I knew PHP?\nI did, over ten years ago, before I got my first IT job, I spent time learning PHP and MySQL. I created a blog, and I also created a website for my Dad\u0026rsquo;s camera club. The code I created back then was awful. No shared code, all the code was associated with the page, or sorts of bugs occurred and as time went by it became increasingly hard to update. The site was well liked but I eventually lost interest and moved on to learn other things.\nThis call led to me talking with the head of IT, and later a couple of the developers who have since granted me access to the codebase of a project.\nI haven\u0026rsquo;t had time to spend a lot of time looking at the code so far, however this is nothing like the PHP I had built before.\nThe project makes use of the laravel framework and the first file I opened had methods and classes, so apart from the syntax you could think you were looking at C#.\nAnother thing that interested me was the project used docker containers, it has automated builds as well. Lots of modern programming ideas that I had some ideas about. I am looking forward to learning more about this project and how I might contribute to it.\n","date":"Nov 28, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/weekly-update-003/","series":null,"tags":["OpenSource","PHP"],"title":"Weekly Update #003"},{"categories":null,"content":"I know Active Directory is fussy about clocks being in sync however not sure how todays issue happened.\nI run my docker compose file from Visual Studio and I get a weird error.\nSecurityTokenNotYetValidException: IDX10222: Lifetime validation failed. The token is not yet valid. ValidFrom: \u0026#39;System.DateTime\u0026#39;, Current time: \u0026#39;System.DateTime\u0026#39;. I deleted my containers, open and close Visual Studio a few times, nothing helps. Eventually I think to find out what the time is on my container. It has yesterday\u0026rsquo;s date. What has happened here? Surely recreating containers would have caused them to have todays date? I reboot and everything is fine again.\nTurns out that it is a know issue, see https://thorsten-hans.com/docker-on-windows-fix-time-synchronization-issue I am using WSL2 and I have now changed back to using Hyper-V and the issue hasn\u0026rsquo;t come back.\nEarlier in the week I spotted my build step was failing.\n - task: NuGetToolInstaller@0 Swapping to the next version of the step is all I needed to do to fix it.\n - task: NuGetToolInstaller@1 My guess is that support was dropped for this earlier version or there is some other incompatability with .Net 5.\n","date":"Nov 21, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/weekly-update-002/","series":null,"tags":["Docker","Azure"],"title":"Weekly Update #002"},{"categories":null,"content":"One of my favourite podcasts is Troy Hunts weekly update. In it he discusses stuff that he has been working on, plus some personal stuff. I am going to attempt to do something similar. It will probably take me a few of these before we get a look and feel that works.\nMonday A week off work, mainly to use it up before year end, plus want to get a few jobs around the house done.\nI did ask the following question on Twitter.\nHey #azurefamily and #dotnet developers how do I get more involved in mentoring?\n\u0026mdash; Simon Foster (@funkysi1701) November 9, 2020  As a one person dev team, my biggest weakness is working with others so any ideas of how to change that are great.\nTuesday Dotnet 5 is out! The latest version of dotnet is released by Microsoft and to celebrate there is dotnetconf to listen to. Due to time zones and family commitments, I haven\u0026rsquo;t listened to an awful lot of it but I did see the keynote and loved the 3 Scott\u0026rsquo;s chat.\nWednesday My youngest son was 3 today, due to Coronavirus we didn\u0026rsquo;t do much but we celebrated as a family, and he even had a zoom call.\nThursday Blazor has a new feature Virtualize where a list of items can only load the ones on screen. I have been trying to get this to work on my blog, works great running locally but not working in production yet.\nThink I know what might be happening. I use Cloudflare to do my SSL, as Custom SSL certs for the cheaper Azure Web Apps is not supported. Something in Cloudflare is caching or interfering.\nhttps://zimmergren.net/solved-asp-net-core-blazor-web-sites-does-not-work-with-cloudflare-html-minification/ Turning off HTML minification fixed my issue!\nOne additional thing I added to my Blog is the /config page which details some of the config settings. I think this probably came from https://www.hanselman.com/blog/adding-a-git-commit-hash-and-azure-devops-build-number-and-build-id-to-an-aspnet-website but it was a while ago when I first did this on another project.\nAt the moment we have .net Version, Commit and Build links.\nThe .Net Version is obtained from\n@System.Runtime.InteropServices.RuntimeInformation.FrameworkDescription A few other bits of info can be obtained from System.Runtime.InteropServices.RuntimeInformation which I have included on the page for fun. There are probably security concerns with exposing all this info publicly so something to bear in mind if you try this.\nBuild Info is passed to my code by a build step\n- script: \u0026#39;(echo $(Build.BuildNumber) \u0026amp;\u0026amp; echo $(Build.BuildId)) \u0026gt; .buildinfo.json\u0026#39;  displayName: \u0026#34;Emit build number\u0026#34;  workingDirectory: \u0026#39;$(Build.SourcesDirectory)/src/WebBlog\u0026#39;  failOnStderr: true This simply passed the build id and number which are stored as variabled and saves them in a text file.\nI then have a class that reads them and constructs a link.\nusing Microsoft.Extensions.Hosting; using System; using System.IO; using System.Linq; using System.Reflection;  namespace WebBlog {  public class AppVersionInfo  {  private const string _buildFileName = \u0026#34;.buildinfo.json\u0026#34;;  private readonly string _buildFilePath;  private string _buildNumber = string.Empty;  private string _buildId = string.Empty;  private string _gitHash = string.Empty;  private string _gitShortHash = string.Empty;   public AppVersionInfo(IHostEnvironment hostEnvironment)  {  _buildFilePath = Path.Combine(hostEnvironment.ContentRootPath, _buildFileName);  }   public string BuildNumber  {  get  {  if (string.IsNullOrEmpty(_buildNumber))  {  if (File.Exists(_buildFilePath))  {  var fileContents = File.ReadLines(_buildFilePath).ToList();   if (fileContents.Count \u0026gt; 0)  {  _buildNumber = fileContents[0];  }  if (fileContents.Count \u0026gt; 1)  {  _buildId = fileContents[1];  }  }   if (string.IsNullOrEmpty(_buildNumber))  {  _buildNumber = DateTime.UtcNow.ToString(\u0026#34;yyyyMMdd\u0026#34;) + \u0026#34;.0\u0026#34;;  }   if (string.IsNullOrEmpty(_buildId))  {  _buildId = \u0026#34;123456\u0026#34;;  }  }   return _buildNumber;  }  }   public string BuildId  {  get  {  if (string.IsNullOrEmpty(_buildId))  {  var _ = BuildNumber;  }   return _buildId;  }  }   public string GitHash  {  get  {  if (string.IsNullOrEmpty(_gitHash))  {  var version = \u0026#34;1.0.0+LOCALBUILD\u0026#34;;  var appAssembly = typeof(AppVersionInfo).Assembly;  var infoVerAttr = (AssemblyInformationalVersionAttribute)appAssembly  .GetCustomAttributes(typeof(AssemblyInformationalVersionAttribute)).FirstOrDefault();   if (infoVerAttr != null \u0026amp;\u0026amp; infoVerAttr.InformationalVersion.Length \u0026gt; 6)  {  version = infoVerAttr.InformationalVersion;  }  _gitHash = version[(version.IndexOf(\u0026#39;+\u0026#39;) + 1)..];  }   return _gitHash;  }  }   public string ShortGitHash  {  get  {  if (string.IsNullOrEmpty(_gitShortHash))  {  _gitShortHash = GitHash.Substring(GitHash.Length - 6, 6);  }  return _gitShortHash;  }  }  } } The BuildId and BuildNumber properties just fetch the details saved into the text file during the build. This can then be passed to build the build link.\n\u0026lt;a href=\u0026#34;https://dev.azure.com/{OrgName}/{RepoName}/_build/results?buildId=@appInfo.BuildId\u0026amp;view=results\u0026#34;\u0026gt;  @appInfo.BuildNumber \u0026lt;/a\u0026gt; Finally, the GitHash properties need to fetch the hash and shorthash of the commit which is a bit more complex. This is achieved using the following line in your build.\n- task: DotNetCoreCLI@2  displayName: \u0026#39;Publish\u0026#39;  inputs:  command: \u0026#39;publish\u0026#39;  publishWebProjects: true  arguments: \u0026#39;--output $(Build.ArtifactStagingDirectory) /p:SourceRevisionId=$(Build.SourceVersion)\u0026#39; /p:SourceRevisionId=$(Build.SourceVersion) add the revision hash to [assembly: AssemblyInformationalVersion] during the build which can then be extracted using the gitHash property above, before being passed into the commit link.\n\u0026lt;a href=\u0026#34;https://github.com/{OrgName}/{RepoName}/commit/@appInfo.GitHash\u0026#34;\u0026gt;  @appInfo.ShortGitHash \u0026lt;/a\u0026gt; ","date":"Nov 14, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/weekly-update-001/","series":null,"tags":["DotNet"],"title":"Weekly Update #001"},{"categories":null,"content":"Have you wondered what info you are leaking via your response headers?, do you want some kind of guide about what headers to set or remove altogether?\nHead on over to https://securityheaders.com/ This is a site created by security expert Scott Helme that rates a URL based on what response headers it can see.\nI am pleased to say www.funkysi1701.com is now getting an A.So how do you add/remove headers in dotnet core?\nIn my configure method in Startup.cs I have the following code block.\napp.Use(  next =\u0026gt;  {  return async context =\u0026gt;  {  context.Response.OnStarting(  () =\u0026gt;  {  context.Response.Headers.Add(\u0026#34;Permissions-Policy\u0026#34;, \u0026#34;microphone=()\u0026#34;);  context.Response.Headers.Remove(\u0026#34;Server\u0026#34;);  context.Response.Headers.Remove(\u0026#34;X-Powered-By\u0026#34;);  context.Response.Headers.Remove(\u0026#34;X-AspNet-Version\u0026#34;);  return Task.CompletedTask;  });   await next(context);  };  }); I have only included a few of the headers I am adding as the excellent https://securityheaders.com/ can tell you which headers you should add and what options you might want.\n","date":"Sep 26, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/security-headers/","series":null,"tags":["DotNet","Security"],"title":"Security Headers"},{"categories":null,"content":"My last blog post was over six months ago.\nCovid 19 has hit the world, and I will be honest I have found it a challenging time.\nMy Blog had gotten into a bit of a mess. It had become fragmented with different versions of the same thing; I will attempt to explain what has become of my blog.\nThe original WordPress site can currently be found at https://www.pwnedpass.com/ I would prefer it to be on a sub-domain of funkysi1701.com but for some reason I haven\u0026rsquo;t been able to get that to work, not sure if it is a limitation of my hosting package. I like WordPress, it is very flexible, easy to get blog posts out there. But I want to write content about development and having a site I can tinker with is important to me.\nMost of my WordPress blogs have been imported into dev.to and a few extra have been written on this platform. I like dev.to it is a wonderful place to share content and it has one or two extra features I like.\ndev.to has an integration with Stackbit/Netlify and this became https://dev.funkysi1701.com . I like having a personal site, but having the same content as dev.to. To add content to this site all I need to do is write it on dev.to and some magic will go on behind the scenes and new content will be published.\nHowever, as a developer I don\u0026rsquo;t like magic, I want to understand what is going on a fiddle with all the settings and make it do what I want.\ndev.to has an API, I can build a site in .Net Core and make API calls to fetch the content I want. I understand APIs, I understand .Net and can customise my site exactly how I want it, plus play about with a .net website. This is what https://www.funkysi1701.com is now.\nSo what have I built so far. I have a Server Side Blazor site running .Net 5. Why Server Side and not Client Side I hear you ask? Well only because I have more experience with Server Side and know how to quickly create a website with that technology, I may change it as time goes by, but we will see.\nI have two pages a list of my blog posts and a page that displays the content. Both of these use the dev.to API. I lied, there is a third page I hacked together to do some page redirection from the WordPress URLs. This is something I will change as time goes on.\nThere are lots of improvements I want to do, there are probably also lots of broken images or links as well. Hopefully, this will result in a good platform to blog about as well as on.\n","date":"Sep 25, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/back-to-blogging/","series":null,"tags":["Blogging","Blazor","DotNet"],"title":"Back to Blogging"},{"categories":null,"content":"Let\u0026rsquo;s Encrypt is amazing, you can easily add SSL certificates to any website and automate the renewal process. I have talked before about how impressive it is.\nOnce you start adding SSL certificates to your production sites however you may want to check when they expire so you don\u0026rsquo;t get caught out. You can always open your site in your favourite browser and view the certificate information and expiry date.\nHowever there is a way to automate this check.\n[Fact] public void IsSSLExpiring() {  var handler = new HttpClientHandler  {  ServerCertificateCustomValidationCallback = CustomCallback  };  var client = new HttpClient(handler);   HttpResponseMessage response = client.GetAsync(\u0026#34;https://www.example.com\u0026#34;).GetAwaiter().GetResult();  Assert.True(response.IsSuccessStatusCode); }  private bool CustomCallback(HttpRequestMessage arg1, X509Certificate2 arg2, X509Chain arg3, SslPolicyErrors arg4) {  var now = DateTime.UtcNow;  var expire = arg2.NotAfter;  var diff = (expire - now).TotalDays;   Assert.InRange(diff, 30, 1000);  return arg4 == SslPolicyErrors.None; } This code gets the SSL expiry date from https://www.example.com and will fail the xunit test if the expiry date is less than 30 days in the future. I then schedule my tests to run regularly on all my environments with a Let\u0026rsquo;s Encrypt Certificate and this gives me advanced warning if a SSL certificate is about to expire.\nThe Assert.InRange(diff, 30, 1000) line will fail the test if the expiry date is less than 30 days or greater than 1000, but as the default expiry for Let\u0026rsquo;s Encrypt certificates is three months it will never be greater than 1000 days even with a freshly installed certificate. These values can be tweaked to suit your use case, however 30 days is enough time for me to investigate what is happening.\nTo execute my tests I use a scheduled build in Azure DevOps, but anything that regularly can run your tests will do the job.\nThe code above is just a simple example to get your started for my purposes I have put all my URLs into config files and just pass these into my tests, so I don\u0026rsquo;t need a custom test for every different URL.\n","date":"Mar 3, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/testing-for-expiring-ssl-certificates/","series":null,"tags":["Security","SSL","Testing"],"title":"Testing for expiring SSL Certificates"},{"categories":null,"content":"It has just been announced at the annual Star Trek Las Vegas (#STLV ) convention that Patrick Stewart is going to return to his role as Jean-Luc Picard in a new TV Show.\n Few other details have been announced other than the TV show will take place 20 years after the film Star Trek: Nemesis and will feature the character of Jean-Luc Picard.\nLets run though a few things I would like to see from this new show.\nLow on Action Star Trek: Discovery is a great show and has brought Star Trek back to our TV shows. However it is an action show, it zooms along so fast and with so much action I would appreciate a show that can takes its time a bit more.\nI want a show that can show Picard carefully considering some moral issue and making a decision. Something we were used to seeing on TNG.\nFeature Picardâ€™s excellent speeches One of the best features I can think of Jean-Luc Picard is his ability to make speeches about one issue or other.\nLets look at a few examples:\nInsurection : Picard speaks about the forced relocation of a group of people.\n How many people does it take, Admiral, before it becomes wrong? Hmm? A thousand, fifty thousand, a million? How many people does it take, Admiral?\n First Contact : Picard is hell bent on revenge for what the Borg have done to him and refuses to destroy the Enterprise to save his crew.\n â€œI will not sacrifice the Enterprise. Weâ€™ve made too many compromises already. Too many retreats. They invade our space and we fall back. They assimilate entire worlds, and we fall back. Not again! The line must be drawn here, â€¦this far, no further! And I will make them pay for what theyâ€™ve done.â€\n Measure of a Man : Picard argues for the rights of Data.\n â€œYou see, heâ€™s met two of your three criteria for sentience, so what if he meets the third. Consciousness in even the smallest degree. What is he then? I donâ€™t know. Do you? (to Riker) Do you? (to Phillipa) Do you? Well, thatâ€™s the question you have to answer. Your Honour, the courtroom is a crucible. In it we burn away irrelevancies until we are left with a pure product, the truth for all time. Now, sooner or later, this man or others like him will succeed in replicating Commander Data. And the decision you reach here today will determine how we will regard this creation of our genius. It will reveal the kind of a people we are, what he is destined to be. It will reach far beyond this courtroom and this one android. It could significantly redefine the boundaries of personal liberty and freedom, expanding them for some, savagely curtailing them for others. Are you prepared to condemn him and all who come after him to servitude and slavery? Your Honour, Starfleet was founded to seek out new life. Well, there it sits. Waiting. You wanted a chance to make law. Well, here it is. Make a good one.â€\n Ask important questions We live in uncertain times. There are lots of pressures on todayâ€™s world and we need to acknowledge these. Picard is an excellent character to use to ask important questions of the day.\nWhat does it mean to be human?\nHow should we treat people that are different to us?\nHow should we react to extremism?\nDeal with his age In the last episode of TNG a future version of Picard is shown 25 years into the future. This is a similar time period so it will be interesting to compare the two. Have Picard look back on his life and ask questions about his life choices.\nBe consistent with his past Picard is a well known character. So writers please donâ€™t change him. I want a character we know, not a character that looks like Picard but acts in a way contrary to what we would expect.\nPicard has a history we know, he served on the stargazer and of course the Enterprise, he has an artificial heart, he was assimiliated by the Borg. All\nReference 50 years of Star Trek Discovery and Enterprise are TV shows which have a problem. There is Trek going on after them which imposes a few rules on what can and canâ€™t be done.\nThis is a TV Show that will take place after every other Trek TV and Film and can do whatever the creators want. This gives freedom, but it also gives the responsibility to reference what has gone before.\nThe 2009 Star Trek film features the destruction of Romulus and Spock travelling back in time. How about Picard dealing with surviving Romulans helping rebuild their society, continuing the work that Spock started in the TNG episode Unification.\nI look forward to finding out what the Star Trek production team has in store for us. If they have Patrick Stewart convinced I am sure it is going to be good.\n","date":"Aug 8, 2019","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2018/08/download.jpg?w=662\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2019/picard-is-back/","series":null,"tags":["StarTrek","Picard","PatrickStewart"],"title":"Picard is back!"},{"categories":null,"content":"Automated releases of software are great but how can we add an element of feedback so only good releases go live.\nI have been using Azure DevOps to release my PwnedPass android app to the Google Play Store for a while now. There are options to deploy to the alpha, Beta or Production tracks and even to set % of users to target. For the full range of options check out the Google Play extension for Azure DevOps.\nMy release starts by publishing to 10% of users on the production track, my next step makes use of the increase rollout option to increase this %, you can have as many of these additional steps as you want until you reach 100% of your users.\nNow if you run this release now it will just run through each of the steps one after the other. Now of course you can add a pre or post approval to your pipeline but this just adds a manual dependency to your release. Whoever does the approving needs to check things are working before approving or worse just approves regardless.\nAzure DevOps has the concept of gated releases which allows you to add automated checks before or after a release happens. These automated checks can be any of the following:\n An Azure Function A Rest API call Azure Monitor Alert Query Work Items Security and Compliance Assessment  We are going to make use of the Azure Monitor Alert, to create an alert from your Application Insights data and only continue the rollout if no failures are detected.\nOpen up your application insights resource in the Azure portal and look in alerts. Click add new alert rule.\nSelect your application insights resource in Resource, In Condition choose a condition to check, I chose Failed Requests, so every time a failure is registered in my API I can stop the deployment. The exact criteria you want to use is entirely up to you.\nCreate an action group, I just set my alert to send an email to myself but there are other alert actions you may want to try. Give your alert a name and description and click save.\nNow all we need to do is make Azure DevOps make use of this alert. In your release pipeline select the pre-deployment conditions of your second step and open up the Gates section.\nChoose a suitable time to evaluate, I have been using something long like 12 or 24 hours so if there are problems there is time for it to be noticed. Choose Version 1 of the task (I was not able to get it to work with Version 0)\nNow select your Azure subscription and Resource Group and leave the rest of the settings as they are. Now your Deployment will stop and analyse application insights for any Failed requests and will halt if it finds any.\nI am still testing this out but it will take a few days to figure out if this what I want due to the large time scales involved. I feel this is going to be an improvement of manually approving release steps.\n","date":"Apr 5, 2019","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/04/image.png?fit=662%2C116\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2019/gated-release/","series":null,"tags":["Azure","API","ApplicationInsights","DevOps"],"title":"Gated Release"},{"categories":null,"content":"Its been a while since I first released Pwned Pass so lets have a look at where we are now.\nWe are very close to 500 Downloads from Google Play and we have recently smashed past 100 active installs, peaking at 116 and even now we are still over 100. I have had 9 reviews (6 x 5*, 2 x 1 * and a 4 *) which averages out at 4 * Over Christmas I released a UWP version that can be found in the Microsoft Store . This has currently had 9 downloads and even had a download to windows mobile (someone out there still likes the platform!) I have a fairly smooth deployment process using Azure DevOps. After every check in of code a build runs which compiles the UWP and Android versions. The build also increments the version numbers that is required to deploy to either of the app stores.\nEvery successful build of the master branch will kick off a release to the Beta track of Google Play. If I am happy I then release to 10% of the Production track, which can then be increased to 100% (or halted). The release to Microsoft Store happens after the Beta track of Google Play. Only reason for this order is that there isnâ€™t a beta area for UWP apps so I want to quickly test change on android before rolling out for windows.\nAll these steps require confirmation by me before proceeding and often donâ€™t get further than the beta track.\nA further development is that I have open sourced the source code to github do take a look if you are curious or want to contribute. With the purchase by Microsoft there are easy ways to connect github repositories to Azure DevOps. Once I create a Pull Request in github it creates a build in Azure DevOps and all the build and release steps can happen.\nI am still not 100% sure if I want to keep my bug and issue tracking in github or Azure DevOps as both have features for doing so.\nOne future improvement I want to make is to automate the creation of screenshots. When I create a new feature and it gets checked in. I would like to automatically created screenshots of the key pages and submit them to the different app stores. Currently I am not sure if this is possible or how to go about it. I have some ideas to experiment with so we will see what I can do.\n","date":"Jan 23, 2019","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/01/image.png?w=662\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2019/pwned-pass-update/","series":null,"tags":["Xamarin","Android"],"title":"Pwned Pass Update"},{"categories":null,"content":"This is my annual lets make some goals for the new year blog post. So in no particular order.\nImprove House I canâ€™t outdo last years goal of buying a house but I can continue to make efforts to improve it. I was going to put something about decorating or doing something to our bedrooms, however today I spotted a hole in the roof so this may well absorb most of my home improvement budget.\nConnect with local area Since we moved in October we havenâ€™t really connected with our local area yet. My only interaction with my neighbours was when my car blocked his drive!\nI am going to make efforts to change this in 2019. I donâ€™t exactly know how yet, but I have some ideas. My son starts nursery and later school this year so could be one way to meet other parents and teachers. I want to connect with a local church, so far this hasnâ€™t happened so need to make more effort with this. There are other local groups that put on child friendly events which would be good to attend.\nLightning Talk I failed last year to do this so it is back on my list. I know I must have stuff of value to share so going to try again on this.\nA lightning talk is a very short talk or presentation given at a conference or user group. I am not a natural public speaker so this is a step out of my comfort zone, however I am going to start small and see what happens.\n##Family Holiday As always we Fosters are going to have a holiday in the summer and have some quality family time. I also want to take the boys to London for a weekend.\nBlogging This blog has taken a bit of a backseat in 2018 so I want to try and refocus it for 2019. My commitment is for 12 blog posts in 2019, that is one a month. It doesnâ€™t sound like much but I know how easily life can get in the way.\nHaving said that my log has had record views this year. 7159 views. However I believe this number to be incorrect and has been inflated by my app development and some of the automated processes taking place.\nRoutine Last goal is to try a get more of a routine going. With two small boys, one of which is starting school in September a routine is going to be essential and will allow all of us all a chance to get more things done.\n","date":"Jan 1, 2019","img":"","permalink":"https://www.funkysi1701.com/posts/2019/lets-see-what-2019-can-do/","series":null,"tags":["Goals",""],"title":"Lets see what 2019 can do!"},{"categories":null,"content":"Since I started creating an android app I have been writing simple UI tests.\nI have been taking advantage of the Visual Studio App Center which allows you to test against hundreds of different devices in the Test Cloud.\nIn order to write a UI test create a UI Test App, this makes use of the nuget package Xamarin UI Test. By default you will now have a test called AppLaunches which will take a screenshot of you app after it starts.\nYou can now run this test against any device from Visual Studio assuming you have it physically plugged into your machine. However, how do you run against the Test Cloud?\nTo run against the Test Cloud you need to first install node.js. Now you can install the appcenter cli with the following command.\n**npm** install -g appcenter-cli To run the tests in the Test Cloud run the following command\n**appcenter** test run uitest --app \u0026#34;\u0026lt;username\u0026gt;/\u0026lt;appname\u0026gt;\u0026#34; --devices \u0026#34;\u0026lt;username\u0026gt;/\u0026lt;deviceset\u0026gt;\u0026#34; --app-path _pathToFile.apk_ --test-series \u0026#34;master\u0026#34; --locale \u0026#34;en_US\u0026#34; --build-dir _pathToUITestBuildDir_ where  is your appcenter username, is the name of your app in appcenter and is the group of apps you have created in app center to test against.\nLook at device sets in the test section of the appcenter and click the new device set button. You can then search for any device you like and add it to a set, as I write this there are 244 devices you can test against.\nIt is currently not possible within app center to run tests against a new build, however if you build your app in VSTS as well you can create build or release step that runs it. As the Visual Studio app center is still under development I wouldnâ€™t be surprised if it is added at some point.\nIn VSTS look for Mobile Center Test in the definitions and you can specify the same variables as specified in the command line above.\nNow how do you actually write a useful UI test? I mentioned above you get a default test which contains the following code, this takes a screenshot of your app. However you donâ€™t have to include this when using the Test Cloud as screenshots are included for free.\napp.Screenshot(\u0026#34;First screen.\u0026#34;); Lets look at what else is included in app\napp.Repl(); This starts an interactive REPL (Read-Eval-Print-Loop) which lets you explore what is on screen in your app and pauses execution. I donâ€™t include this in my tests, however I do make use of it to explore what is on screen and what tests I might make use of.\napp.Tap(c =\u0026gt; c.Marked(\u0026#34;Button\u0026#34;)); This taps an element on screen called Button. There is also a method called TapCoordinates which would allow you to click anywhere you like.\napp.WaitForElement(c =\u0026gt; c.Marked(\u0026#34;View\u0026#34;)); After clicking a button you are probably going to want to wait for the app to load extra data or a new screen. The WaitForElement waits for an element to appear on screen. There are also methods that wait a period of time or wait until an element no longer exists.\nThese are the main methods I have used so far, however there is an extensive list including methods for scrolling, swiping, pinching and adjusting the volume buttons. So you should be able to test all manner of app functionality and if you make use of the Test Clouds will know which devices are causing problems.\n","date":"Jan 8, 2018","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2018/01/01-newproject-vs.png?resize=300%2C182\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2018/mobile-app-ui-testing/","series":null,"tags":["Android","Azure","App"],"title":"Mobile App UI Testing"},{"categories":null,"content":"Writing SQL queries is typically done with SQL Management Studio (SSMS). However this tool is a bit of a beast so letâ€™s look at how you could use Visual Studio Code instead.\nVisual Studio Code is a free text editor but it is so much more than just a text editor. VS Code can be downloaded from https://code.visualstudio.com/Download To work with SQL Server download the mssql extension. Press CTRL+SHIFT+P and then Select Install Extension and type mssql.\nIntellisense in Visual Studio Code is brilliant, better than SSMS. Lets look at how to get it all set up.\nCreate a new file and set the language type to SQL (Press CTRL+K,M )\nOpen the command palette, CTRL+SHIFT+P and type SQL to show the mssql commands. Select the Connect command.\nThen select Create Connection Profile , this creates a profile to connect with your SQL Server. Follow the prompts to get it all setup.\nLook in the bottom right corner of the status bar and you should see you are connected.\nNow if you type sql you will see a long list of SQL code snippets that you could use.\nChoose a snippet to create, and edit it as required. When you are happy press **CTRL+SHIFT+E ** to execute.\nThis is basically all there is to it. However this is an incredibly powerful way of working, the intellisense instantly tells you what database objects you can use in your query and there is a wealth of different snippets you can use.\nWhen returning data you get a similar view to SSMS but you can save as Excel, CSV or JSON.\nSSMS is a very graphical way of doing things, you can double click a table and see its columns or indexes. VS Code relies on TSQL commands but you have access to exactly the same information.\nFor more information about VS Code and the mssql extension check out https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-develop-use-vscode ","date":"Nov 6, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/sql-with-visual-studio-code/","series":null,"tags":["Code","Database","SQL","Programming"],"title":"SQL with Visual Studio Code"},{"categories":null,"content":"A while back I blogged about learning about interfaces as I didnâ€™t really understand the value of them. I do now.\nI created an application that used interfaces so I could learn how it worked. I created a Logger Interface and created multiple classes that implemented that interface so I could swap out the different implementations easily. I created a SQL Logger and a File Logger and my code could be written and be completely unaware of which implementation it was using.\nThis application uses SQL Azure and so I have a monthly bill to pay. Wouldnâ€™t it be cool if I could reduce this bill? How about using the cheaper table storage instead?\nEasy!\nCreate a new class that implements my interface and all I need to do is write the three methods defined in my interface and I can swap from SQL Azure to table storage.\nAnother benefit to interfaces is testing. Say I have an interface called inotification for sending notifications, I can have several implementations of this email, twitter, slack etc\nNone of these implementations should be used in unit tests, as you donâ€™t want a tweet being sent every time you run your tests. Why not create an implementation that simply returns something for each method call and doesnâ€™t actually do anything. I can then run my tests with my fake implementation which tests my code logic but not the implementation I have chosen (this can be tested later on with integration tests or user testing if required).\nThis is pretty much all I have to say about interfaces. I just like how I can swap different implementations.\nIt does take a bit of work to get the interface setup. I found that when writing the second implementation the interface would need to change slightly, mostly as it was badly designed to begin with. I think for beginners there may be some value to writing multiple implementations of an interface so you can be sure your interface is good, however I am sure with experience this will not be required.\n","date":"Oct 31, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/interfaces-are-cool/","series":null,"tags":["BadDesign","Interface","Programming"],"title":"Interfaces are cool!"},{"categories":null,"content":"I tried to resist but I am going to have to write about the new Star Trek series Discovery. Warning this post is going to include SPOILERS. If you read on you have been warned.\nI watched the first two episodes of the brand new Star Trek TV show. As I am an international viewer I used Netflix, if you are from the US you need to use CBS All Access. I have heard a lot of complaining that the show is not free to watch. CBS is making Star Trek Discovery to make money, if they donâ€™t make money they will stop making it. Its as simple as that.\nNetflix costs Â£5.99 per month and CBS All Access costs $5.99 per month. I donâ€™t believe this is a lot of money. The 5.99 above allows you to watch any of the 726 Star Trek episodes at any time of day whenever you want as many times as you want, plus you have access to all the other movies or TV shows available. I donâ€™t think this is too much to ask.\nBack to Discovery. I loved it! It felt like Star Trek. I had feared it might only share the name and would be an action filled TV show that had little in common with what he had seen before. I was wrong this is definitely a show that can proudly call itself Star Trek.\nThe Klingons\nWhen it was first announced that the Klingons would feature in the new show I was a bit â€œmehâ€. We had done lots with the Klingons before and they had never been my favourite alien race. The look of the Klingons was also going to be changed, I have to admit this didnâ€™t bother me. Klingons have had their look updated before. In 1966 Klingons had dark faces and smooth foreheads, in 1979 the forehead ridges were added as the first Star Trek movie was made, and now Discovery has removed all hair from Klingons. I can explain it away as productional changes, I donâ€™t need an on screen explanation like we had on Enterprise (or even DS9).\nWhat I have seen so far is a menacing alien race that fits with what we have seen before with lots of references to honor, Kahless and speaking Klingon. I must admit reading all the subtitles is getting a bit tiresome, but that is a minor issue.\nConflict\nHistorically Star Trek has not featured conflict between the main starfleet characters due to the idea that humanity has evolved beyond this. DS9 got round this by having Odo and Kira who are not starfleet characters so can have a little bit of conflict. Discovery has completely abandoned this idea.\nIn the pilot episode, the Michael Burnham commits mutiny on her captain, even attacking her with a vulcan nerve pinch. In the third episode we finally meet the discovery crew. Gone are the TNG days where the ships crew are like a family, I am not sure I can think of a single character that would call another character â€œfriendâ€. We are in a time of war so this would be expected, however I do hope we see deepening friendships form between characters.\nI am OK with the change to feature more conflict. I must admit the darkness of the third episode, did stop to make me think a bit, however by the fourth episode I was won over.\nCast\nThe story is concentrated around Michael Burnham, so I do worry that other characters wonâ€™t get a look in. However from what I have seen Michael Burnham is a great character. She is a strong female character, with a intriguing back story relating to Sarek and possibly also Spock.\nSaru the alien character on the show, played by the very tall Doug Jones is great. From the trailers he was all about sensing death, but there is far more to him than that. I am looking forward to learn more about him and his threat ganglia.\nAs the show has gone on we have started to learn about the other characters like Captain Lorca, Tilly and Paul Stamets.\nTitle Sequence\nI donâ€™t like the opening title sequence, it feels very cheap like a draft version which hasnâ€™t been finished yet. I was looking forward to a title sequence that would show off the USS Discovery, maybe like TNG where it warps around our own solar system. What we see is some of the tech that features in Star Trek which is nice but I want more. The theme tune while great that it features parts from the classic theme doesnâ€™t stand on its own. I can hum all the other TV shows themes, and now after 4 episodes would struggle to do that.\nHowever I expect that after a while I will grow to like this more.\nOverall\nI like Discovery and I will keep watching it. I want to find out what happens to the characters. There are no annoying characters like other shows have had. There is character development, none of the characters are going to finish where they started. I want more friendship between characters, but I expect that will come, I want more exploring and doing Star Trek stuff, but most of all I want more episodes, roll on next weeks show.\nOther things I noticed\n Sound effects on the bridge are awesome, it makes me feel at home The USS Discovery doesnâ€™t feature in the first two episodes despite being the title of the show. Others have noted that Captain Georgiou has books on her shelf which feature classic episode titles  ","date":"Oct 10, 2017","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/05/DAD0hTKUAAAUkTP.jpg?resize=662%2C366\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2017/discovery-my-thoughts-so-far/","series":null,"tags":["StarTrek","Discovery"],"title":"Discovery â€“ My thoughts so far"},{"categories":null,"content":"In November 2015 it was announced that a new Star Trek series was going to be launched. It has been a long wait with multiple delays but Star Trek Discovery is finally here.\nStar Trek Discovery launches in the US on 24th September and in the UK on 25th September.\n Lets have a look back how we got here.\nStar Trek was created by Gene Roddenberry in the 1960s. He imagined a show where we would explore the galaxy while also exploring the human condition. He described it as a wagon train to the stars. However it was cancelled after 79 episodes and three seasons due to budget cuts and low viewers.\nAfter cancellation its popularity boomed in reruns. In the mid 1970s an animated series was created. But demand for Star Trek continued until a new Star Trek show was commissioned called Phase Two.\nIn 1977 Star Wars came out, and the phase two idea was cancelled and turned into a movie. In 1979 we got Star Trek The Motion Picture. It went massively over budget and failed to really capture the Star Trek spirit.\nA much cheaper sequel was created in 1982 Star Trek II: The Wrath of Khan with Roddenberry taking more of a back seat. A film widely considered to be one of the best trek films. Four more feature films starring the original cast were made.\nHowever in 1987 Gene Roddenberry teamed up with Rick Berman and The Next Generation was created with a brand new cast. The series ran for seven seasons set roughly a century after the adventures of Kirk and Spock.\nPopularity of Star Trek soured and this led Rick Berman to help create the spin off series Deep Space 9, Voyager and lastly Enterprise along with four feature films continuing the adventures of the TNG cast. When Enterprise was cancelled in 2005, Star Trek appeared to be dead after 18 years on TV.\nHowever in 2009 movie creator JJ Abrams recast the classic crew for the big screen. The film was big on action, but took place in an alternative timeline allowing for familiar characters to have new adventures without interfering with events depicted in the TV shows.\nTwo sequels to this film have been produced and are bringing Star Trek back into popularity. This has given rise to Discovery. Star Trek is returning to the TV screen and it is going to be great.\n","date":"Sep 22, 2017","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/09/gqojtfz1dhmxoiri7g8p.png?resize=662%2C372\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2017/star-trek-is-back-with-discovery/","series":null,"tags":["StarTrek","Discovery"],"title":"Star Trek is back with Discovery"},{"categories":null,"content":"I think Azure is great, but there is loads to it so I can never know about all of its features. There is a video series hosted by Scott Hanselman called Azure Fridays which I have started to watch in an effort to keep more up to date about some of its cool features.\n I watched this video recently and it is all about application insights and new ways you can debug your web applications by creating snapshots. I am a big fan of application insights so adding extra ways to debug my apps is a big win for me. Once I get this feature working in my code I will no doubt blog about it.\n","date":"Sep 18, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/azure-friday/","series":null,"tags":["Azure","Programming","ApplicationInsights"],"title":"Azure Friday"},{"categories":null,"content":"I recently blogged about using Azure Web Jobs , Azure Function is another way of doing the same thing, lets look at how they work.\n(Sorry its been a while since I blogged but I suspect an erratic schedule will continue for the next few months.)\nTo create an Azure Function go to the Azure portal and click add new and search for \u0026ldquo;Function App\u0026rdquo;\nGive your app a name and select the usual resource group and location settings.\nNow when you open Function Apps you should see your new app.\nI want my Function App to run on a schedule so I clicked the + next to functions and selected TimerTrigger. I am a c# programmer so I selected this option as well.\nGive your function a name and specify using the usual cron notation how often it should run. I want mine to run at 9.30pm each night so use 0 30 21 * * *\nNow comes the code bit. By default you get a window with the following code in it\nusing System;  public static void Run(TimerInfo myTimer, TraceWriter log) {  log.Info($\u0026#34;C# Timer trigger function executed at: {DateTime.Now}\u0026#34;); } It is entirely up to you what you get your function to do. In my case I just wanted to call a URL on a schedule so I created some code that used httpclient.\n using System; using System.Net.Http;  public static async Task Run(TimerInfo myTimer, TraceWriter log) {  log.Info($\u0026#34;Buffer 0 function executed at: {DateTime.Now}\u0026#34;);  HttpClient client = new HttpClient();  var result = await client.GetAsync(\u0026#34;URL\u0026#34;);  string resultContent = await result.Content.ReadAsStringAsync();  log.Info(resultContent); } Once you have created your app and it has run you can use the monitor section to view success and failures.\nThere is loads more you can do with Azure Function but this is a good place to start.\n","date":"Sep 12, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/azure-functions/","series":null,"tags":["Azure","Programming","DevOps"],"title":"Azure Functions"},{"categories":null,"content":"Pwned Pass is now available from the Google Play Store .\nPwned Pass is a simple android app that allows you to type in a password and tells you if it has been used in a data breach.\nTroy Hunt of Have I Been Pwned? recently added a new API to his website which allows you to search his extensive database of pwned passwords, 306 million of them. I have simply created a Android frontend to this API.\nThe API itself takes a SHA1 hash of the password and either returns a HTTP 200 if the password is found or a HTTP 404 if the password does not exist in the HIBP database. For more details of how Troy Hunt created this check out his blog post .\nMy app simply generates a SHA1 hash of anything that is typed in and then passes this to Troy Huntâ€™s API. I then get the HTTP return code so I know if the password exists or not.\nIt should be noted that: Do not send any password you actively use to a third-party service â€“ even this one! I donâ€™t log anything that you type into my app and all I am then doing is passing a SHA1 hash over SSL to HIBP. However you shouldnâ€™t trust my word alone.\nThe app itself is written in Visual Studio with Xamarin Forms in a similar fashion to the app I talked about last week .\nAs I am using Xamarin Forms there is the potential that I may develop iPhone or UWP versions of this code in the future. With that in mind I have made use of interfaces for the android specific parts of the code.\nI also make use of the modernhttpclient nuget package due to problems I encountered with httpclient and SSL. This is due to limitations of what libraries are available in mono and what has been implemented, I suspect there are better ways to solve this but that is all part of the fun.\nPlease do have a look at Pwned Pass and let me know what you think. Especially if it doesnâ€™t work or throws errors. I would like to spend time making this app as good as I can make it.\n","date":"Aug 14, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/pwned-pass-available-from-the-play-store/","series":null,"tags":["Android","Mobile","Programming","App"],"title":"Pwned Pass â€“ Available from the Play Store"},{"categories":null,"content":"For the past week or so I have been playing around with Xamarin and creating an android app.\nWell I now have an app in the Google Play Store. Check out https://play.google.com/store/apps/dev?id=6148298088834956775 . Before you rush and download the app I must warn you that it doesnâ€™t do much yet. It displays some content that is on my website and there are a few links to allow sharing of content. I have some ideas to display content from my blog and allow sharing. I also have some other ideas for apps that might actually be useful to people that are not me. If you have ideas or feature requests do let me know.\nOk how did I go about creating this app and getting it in the app store?\nXamarin is now part of Visual Studio so step one is install all the Xamarin features to Visual Studio and build an app.\nNext I wanted to monitor my app. Now I know Application Insights doesnâ€™t support apps so what tools are out there? HockeyApp is something I had heard of but they are in the process of being replaced with Visual Studio Mobile Centre .\nIt was relatively easy to hook up my app to Visual Studio Mobile Centre. First install the required nuget packages. Then add using statements and the following line to your MainActivity.cs file (these instructions are available on the Mobile Centre)\nusing Microsoft.Azure.Mobile; using Microsoft.Azure.Mobile.Analytics; using Microsoft.Azure.Mobile.Crashes; using Microsoft.Azure.Mobile.Distribute; using Microsoft.Azure.Mobile.Push; MobileCenter.Start(\u0026#34;[Unique ID]\u0026#34;,typeof(Analytics), typeof(Crashes), typeof(Distribute), typeof(Push)); Now you can connect the Mobile Centre to your source code (VSTS in my case) and get it to run a build for every commit.\nOne complexity of the build is that you need to supply a keystore file (basically a certificate to digitally sign your app). I found the best way to do this was to use Visual Studio to create the file.\nIn VS2017 there is a option called Archive Manager under the tools menu. In here click the distribute button and select Ad-hoc. In the signing identity section you can create a keystore file. Enter a few details and a keystore file will be created in AppData\\Local\\Xamarin\\Mono for Android\\Keystore[keystore name][keystore name.keystore]\nOnce you have added the keystore file to your build you can enable the distribute option. Now you will get an email after every build with a link to install your app.\nEvery time your app crashes the details will be logged in the crashes section for you to explore and fix the issues.\nThe Analytics section allows you to explore how your app is being used. You can also add Analytics.TrackEvent(\u0026ldquo;Feature X\u0026rdquo;) to measure the usage of different features.\nThere are more things you can do which I will explore more at another time along with how to get your app into the Google Play Store.\n","date":"Aug 7, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/android-app-development-and-the-visual-studio-mobile-centre/","series":null,"tags":["App","Azure","Programming","Android"],"title":"Android App Development and the Visual Studio Mobile Centre"},{"categories":null,"content":"For a while the Async and Await commands in c# have confused me.\nLike most things the best way to learn about something is to use it in a real world example. I am currently adding an email alert feature to a website. This is an ideal example of something that would benefit from Asynchronous programming. There is no need for the webpage to wait to send 1000s of emails, lets just send a call to get started and allow the browser to carry on as normal.\nThis is my first try at using async and await so feel free to suggest best practises in the comments.\nLets start with a Send method in my EmailController.\npublic ActionResult Send(int id, int pageId, int userID) {  if (!Authorize.checkPageIsAuthorised(userID, (Authorize.PageIds)pageId))  {  return Redirect(\u0026#34;/login\u0026#34;);  }  else  {  Task\u0026lt;string\u0026gt; t = SendNotifications(id,userID);  return Redirect(Request.UrlReferrer.ToString());  } } This simply checks to see if you have permission to the page. If not redirects to the login page otherwise it makes a method call and redirects back to the page it came from.\nLets have a look at that method call in more detail.\nTask t = SendNotification(id, userid);\nSendNotification doesnâ€™t return a normal string it returns a Task, so lets look at how we are creating this.\npublic async Task\u0026lt;string\u0026gt; SendNotifications(int id,string type,int userid) {  //logic ommitted  await ef.SendEmail(model, emailHtmlBody);  return \u0026#34;OK\u0026#34;; } The return type is set to Task but it has the aysnc keyword appended to it. It also makes a call with the await keyword.\npublic async Task SendEmail(EmailModel model,string emailHtmlBody) {  //logic removed  await smtp.SendMailAsync(message); } So that is it. My first bit of code that uses Async and Await. My controller calls a method asynchronously which then calls another method asynchronously which sends emails asynchronously.\nAsync â€“ This enables the Await keyword to be used in the method\nAwait â€“ This is where things get asynchronous. The await keyword allows the code to wait asynchronously for the long running code to complete.\n","date":"Jul 24, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/async-and-await/","series":null,"tags":["Async","Await","Asynchronous","Programming","C-Sharp"],"title":"Async and Await"},{"categories":null,"content":"I recently watched Troy Huntâ€™s What Every Developer Must Know about HTTPS course on Pluralsight. Its very good and really make you think about SSL certificates and how to correctly implement them.\nOne thing in particular Troy mentions is the website SSL Labs . This website allows you to test a websites implementation of SSL. A grade of A to F is assigned with A being the best and F being the worst.\nTroy Hunt has a blog post where he discusses how Australian Banks score. Lets look at a few UK banks.\n   Bank SSL Certificate Grade Home Page Under SSL     HSBC www.hsbc.co.uk  B Y   Nationwide onlinebanking.nationwide.co.uk C N   NatWest www.nwolb.com  C N   Barclaycard www.barclaycard.co.uk  A- Y   Barclays bank.barclays.co.uk A- N   Lloyds Bank www.lloydsbank.com  A N   Royal Bank of Scotland www.rbsdigital.com  C N   Standard Chartered www.sc.com  C Y   Virgin Money uk.virginmoney.com A+ Y   Santander retail.santander.co.uk A- N    On the whole the ratings are all quite good with all being in the range A-C. However I have also indicated if they have SSL on the home page. Only 4 out 10 website listed above have the home page load under SSL.\nWhy does this matter as long as the login is under SSL? Any page that loads over http is potentially at risk from a man in the middle attack. A fake malicious home page could contain links to any page and trick users into entering personal information.\nIf you want to test a bank or other website not listed here. Go to https://www.ssllabs.com/ssltest/index.html and type the address that is on the SSL certificate in to the search. The good news is that this site scores a A.\nTroy mentions that there is rapid growth in the adoption of SSL, there is also rapid growth in improving ratings. One of these banks went from a C to an A during the course of writing this blog.\n","date":"Jul 17, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/how-good-is-your-ssl/","series":null,"tags":["SSL","Security"],"title":"How good is your SSL?"},{"categories":null,"content":"Whenever I write a new test I have to think how best to do it. Hopefully I can summarise a few tips here to help get started.\nArrange Act Assert The first thing I think about when writing a test is Arrange, Act, Assert. Arrange, Act, Assert is a pattern for writing the tests.\nArrange â€“ This gets things in order ready to execute the test.\nAct â€“ This executes the method you want to test.\nAssert â€“ This compares the value produced in the Act step with a known value typically with a method similar to the following\nAssert.AreEqual(expected value, actual value)\nSay for example you wanted to test a method called ReturnsTrue() which does nothing but returns a value of true. This method is in a class called ReturnsTrueClass\nThe Arrange step in this example would be.\nReturnsTrueClass t = new ReturnsTrueClass(); The Act step in this example would be.\nvar result = t.ReturnTrue(); The Assert step in this example would be.\nAssert.AreEqual(true, result); This is a stupidly simple example but hopefully you get the idea of how you can build all your tests with these three steps.\nRecently I saw a tweet complaining that someone has mixed up expected and actual in the Assert statement.\nThere is a minor but special hell reserved for those who mix up the expected and actual parameters in Assert.Equals\n\u0026mdash; Keith Williams (@zogface) July 5, 2017  At first glance this probably isnâ€™t the worst mistake to make as if your tests are all passing actual and expected are the same.\nHowever tests will fail, that is the whole point of them, you can then fix bits of code. If you have mixed up actual and expected it adds extra time to debugging and figuring out what values are produced from your code and what you are expecting it to produce. It may be your test uses a mocking framework and somewhere in there, there is an issue, with mixed up expected/actual you may assume a problem in your code rather than the test.\nAlso, how do you make such an error? When I type Assert.AreEquals() in Visual Studio, Visual Studio tells me what each parameter does, it takes a matter of seconds to do this, just by hovering over the code.\nOne last tip to say about tests. Write your tests to test the behaviour of your application.\n","date":"Jul 10, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/writing-your-first-test/","series":null,"tags":["Act","Arrange","Assert","Programming","Testing"],"title":"Writing your first test"},{"categories":null,"content":"I am a fan of Azure but today I have been looking at AWS. Specifically how to upload and download files.\nAWS S3 stores files in Buckets. I already had an AWS S3 account setup with a Bucket. I am going to assume you have got a bucket setup and concentrate on the code to get files in and out.\nFirst step is to use nuget to install the AWS packages. In nuget the packages you want are called AWSSDK.Core and AWSSDK.S3.\nThe using statements you want to use are called Amazon.S3 and Amazon.S3.Transfer, not sure why this doesnâ€™t match nuget, this difference caught me out a couple of times.\nNow to the code that uploads files\nAmazonS3Client AWSclient = new AmazonS3Client(accessKeyID, secretAccessKeyID, Amazon.RegionEndpoint.EUWest1); TransferUtility fileTransferUtility = new TransferUtility(AWSclient); using (FileStream streamWriter = new FileStream(path, FileMode.Open)) {  TransferUtilityUploadRequest fileTransferUtilityRequest = new TransferUtilityUploadRequest  {  BucketName = \u0026#34;flawlessimages\u0026#34;,  InputStream = streamWriter,  Key = fileName  };  fileTransferUtility.Upload(fileTransferUtilityRequest); } Lets break it down and look at what it does.\nAmazonS3Client AWSclient = new AmazonS3Client(accessKeyID, secretAccessKeyID, Amazon.RegionEndpoint.EUWest1); This creates an instance of AmazonS3Client, we are passing the Access Key and Secret Access Key both of which can be found from your Amazon S3 account My Security Credentials section. Amazon.RegionEndpoint.EUWest1 specifies the amazon data centres that your bucket is located in.\nTransferUtility fileTransferUtility = new TransferUtility(AWSclient); This creates an instance of TransfterUtility using the AmazonS3Client instance we created in the previous step.\nusing (FileStream streamWriter = new FileStream(path, FileMode.Open)) { This opens up a filestream from a files path and specifies that the file should be opened.\nTransferUtilityUploadRequest fileTransferUtilityRequest = new TransferUtilityUploadRequest {  BucketName = \u0026#34;flawlessimages\u0026#34;,  InputStream = streamWriter,  Key = fileName }; fileTransferUtility.Upload(fileTransferUtilityRequest); This last step specifies which bucket to upload to, what input stream to upload and the Key to use. Key is just AWS way of referring to files, more commonly referred to as the filename.\nThis is all you need to do to upload a file to your Bucket. The file will be located at https://s3-eu-west-1.amazonaws.com/[bucketname]/[filename] , however by default it will not be downloadable until you set Read permission to everyone, once you do that anyone who has the link will be able to download your file.\nThis is the same permission level as any file you have on your webserver, however AWS has a better way.\nusing (s3Client = new AmazonS3Client(accessKeyID, secretAccessKeyID, Amazon.RegionEndpoint.USEast1)) {  GetPreSignedUrlRequest request1 = new GetPreSignedUrlRequest  {  BucketName = bucketName,  Key = filename,  Expires = DateTime.Now.AddMinutes(5)  };  urlString = s3Client.GetPreSignedURL(request1); } Here we are generating a url to download the file, but we are specifying that it is only valid for 5 minutes. This means that if you share the url it will only work for 5 minutes, after that AWS will give an access denied message.\nThis is much better security than you have on a typical web server, and easy to implement, every time a user clicks on a download link you generate a new presigned url and send the download to the browser, as long as this process doesnâ€™t take longer than 5 minutes the user will never know.\n","date":"Jul 3, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/uploading-files-to-aws/","series":null,"tags":["AWS","C-Sharp","Programming"],"title":"Uploading Files to AWS"},{"categories":null,"content":"I keep hearing about Azure WebJobs but I have never used them. Time to change this.\nWebJobs are a feature of Azure App Service that can run a script at a specific time. In my case I would like to hit a specific url of my website at the same time every day.\nTo the right you can see an example of the WebJobs form on the Azure portal that you need to fill in.\nYou need to supply a name for your webjob.\nYou need to upload the script that will run in my case I used a powershell script. My script consisted of which basically just loads the url specified.\n$progressPreference = \u0026#34;silentlyContinue\u0026#34;; $result = Invoke-WebRequest -Uri (\u0026#34;https://www.google.com\u0026#34;) -Method Get -UseBasicParsing; Type refers to if your job will be triggered or run continuously, I want it to be triggered.\nTriggers refers to if you want it to be scheduled or manual, something that you can run on an ad hoc basis. I of course want scheduled.\nIf you are familiar with the linux CRON then the next box will make sense to you for everyone else I will try and make sense of it. The box consists of 6 numbers which can either have a value or a *. The numbers correspond to the following {second} {minute} {hour} {day} {month} {day of the week}.\nA hourly job would be expressed as 0 0 * * * *, ie every day of week, every month, every day, every hour and only when minute and second equals zero. For more help with this check out the MSDN docs about it. I want to use 0 30 21 * * * to run daily at 9.30pm.\nThatâ€™s it everything setup, now time to wait and see if it works.\nOh no!\nIt failed to run at the specified time.\nThe reason for this is the scheduler requires the feature Always On to be turned on which is not available in the free App Service. Before you reach for your wallets, I found a solution on this blog post that allows them to run on the free tier.\nThe thinking behind this solution is you need to keep the website alive throughout the day so Tom has created a script that does this. His script can be found on his blog or on his github page .\nSet this script up to run every 5 minutes (0 */5 * * * *) like the example above.\nThe nextthing you need to do is create a Custom connection string in the Application Settings blade called SecretThing. Tomâ€™s script references this to access the website and keep it alive. The password you need to put in SecretThing can be found in you publish profile (downloaded from the Overview blade in the Azure portal). For more details and a better explanation check out Tomâ€™s blog .\nOne last thing to mention about WebJobs is that you can see details about when they have run at https://[YourWebAppName].scm.azurewebsites.net/azurejobs/#/jobs and this can be a great place to help debug your scripts.\n","date":"Jun 26, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/using-azure-webjobs-to-automate-stuff/","series":null,"tags":["Automate","Azure","Cron","Programming"],"title":"Using Azure WebJobs to Automate Stuff"},{"categories":null,"content":"A while ago I blogged about promoting my blog with Buffer. At the time I made use of the nuget package BufferAPI but lets look at some improvements I can make.\nThe BufferAPI package worked great from my console app, but when I tried to use it from a Controller in an MVC app I never got it to work. Lets look at the API docs and see if I can rewrite it.\nThere are two main types of API calls GET which gets data from the server and POST which posts data to the server. These come from the types of HTTP requests.\nI quickly figured out how to use the GET API call to authenticate using https://api.bufferapp.com/1/profiles.json?access_token=XXXX However POST was defeating me. That was until I remembered Fiddler .\nI had heard Troy Hunt (and others) talk of using Fiddler to examine what data is being passed among websites. Troy uses it to do a man in the middle test to see what information can be stolen.\nIt is really easy to setup, install Fiddler, click yes to a few security warnings and you can see what information is being passed from your code to remote APIs.\nOnce I had Fiddler installed I could compare what information is being passed between a successful API call using the BufferAPI nuget package and an unsuccessful API call using my code.\nFiddler also showed that passing my authentication token in a POST request is much better. Despite both GET and POST being encrypted when using HTTPS, anything at either end that logs URLs will have a log of your username and password.\nIf you have not tried Fiddler, give it a try especially if you are doing things with API calls.\n","date":"Jun 19, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/fiddler-and-apis/","series":null,"tags":["",""],"title":"Fiddler and APIs"},{"categories":null,"content":"Last week I talked about Power BI , what it is and some of the different services you can use with it. This week lets add some of that data to a simple web page.\nFor this example I am going to add the google analytics data from this website to this page.\nFirst login to your Power BI https://app.powerbi.com/ Click the get data link at the bottom left.\nClick My Organisation to bring up the app search box.\nClick the Apps tab and search for â€œgoogleâ€ in the search box, you should then see Google Analytics, click into this and then click the get it now button.\nLog into your google account. If you have multiple google accounts I found it worked best to sign out of all of them or run this in an incognito window.\nOnce you are signed in you should see a list of the different google analytics data you have, select the one you want to use and click import.\nPower BI will then go away and start loading the data.\nOnce loaded go to Reports and select the Google Analytics that has been loaded. If you have more than one, it is a good idea to rename each one eg Corporate Site Google Analytics, Blog Google Analytics so you wonâ€™t get mixed up.\nIn the file menu select Publish to web and agree that you are OK for this to be made public.\nYou will then be given a piece of HTML code that starts with \u0026lt;iframe copy this onto your web page. Reload your webpage and you should see something similar to mine below.\nIt should be noted that while you can but the webpage containing the iframe behind a login page, the data could still be accessed if you knew the url contained within the iframe, this is why the link can be emailed and continue to work.\n","date":"Jun 12, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/how-do-i-add-power-bi-data-to-a-webpage/","series":null,"tags":["PowerBI",""],"title":"How do I add Power BI data to a webpage?"},{"categories":null,"content":"For the past few weeks my software developing has been taking a back seat as I planned and coordinated the IT requirements of an office move.\nThe company I work for has been working out of a converted barn, but as the company has grown we have outgrown the building and for quite a while it has been a real squeeze to get everyone in. We now have a shiny new offices with plenty of room for growth.\nIt was at the start of the year when I first started getting involved with the new office. At that time building work was about to begin. The office had concrete floors so in order to get network and power points in the floor, channels would have to be cut into the concrete for the floor boxes and cable runs.\nAt the old office our servers were in a cabinet in the corner of the room which meant they could be quite loud especially when the fans were going full pelt. Our American owners were supplying brand new IT equipment and we would have a dedicated server room.\nSoon a weekly conference call was setup so we could coordinate with the IT people in America and the various different contractors that would help deliver our new office.\nOne thing I was particularly proud of was YDS. York Data Services (YDS) is an ISP I have worked with in the past at a previous job and I was able to continue my relationship with them and they were contacted and became our primary internet supplier.\nUnfortunately most ISPs have to deal with BTOpenreach who have a bit of a monopoly on getting phone lines or leased lines installed. The initial estimate we were given was two weeks after we were due to move in. It was looking like we would move in and two weeks later we would get an internet connection. We were in the process of getting quotes for a temporary internet connection when BT contacted us and could get us connected up. Soon afterwards a huge cabinet and two pallets full of equipment was delivered. Two days later a team was dispatched to rack everything up. On the rack we had the following equipment: 4 x UPS, 4 x Network Switches, 2 x Routers (for 2 x Leased Lines from different providers, the secondary connection has not been installed yet), 2 x Palo Alto Firewall devices, 2 x New Servers.\nJust prior to our move we had a rack full of equipment but no patch cables, nothing connected up and I was beginning to get concerned that we would not be ready in time.\nThankfully a team was dispatched to help out over the weekend of the move, with the patch cables being delivered only hours beforehand. I worked closely with them and we worked late into the night. By the end of the Friday night our domain controller had been moved and its IP address updated. DHCP scopes had been setup for the new network. Once that was done we could get the WiFi points connected up (These had already been fitted to the ceiling).\nThe following day we moved our existing servers and updated their IP addresses and got everything patched up. Furniture started arriving so desks could start being setup and phones connected up. Everything was slotting into place, and for the first time our new office was starting to look like an office.\nBy the time it got to Monday the only problem was the phone number had not transferred as requested (something else to blame on BT) All staff could either connect via an Ethernet cable or connect to our brand new WiFi network and had access to all the IT services they had at the old office. Another minor issue was the NAS we used for backups had to be reset as it couldnâ€™t communicate with the domain so no one could login, luckily this didnâ€™t affect the data on it.\nFrom my perspective the move was a massive success considering how complex and how many different people had been involved.\n","date":"May 1, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/office-move/","series":null,"tags":["ITAdmin","Routers","Servers","OfficeMove"],"title":"Office Move"},{"categories":null,"content":"I am trying to understand interfaces and when to use them in my code.\nAn interface defines a contract and any class that implements that interface agrees to fulfil that contract.\nLets look at an example as this tends to be how I learn best.\nMost applications require some sort of data to work from so lets start by defining IData which can load data.\npublic interface IData {  Blog LoadData(string Connectionstring); } My interface defines one method LoadData and outputs an object called Blog (I will explain why in a minute)\nA common data source is a SQL database so we could define a SQL class that implements IData.\npublic class SQL : IData {  public Blog LoadData(string Connectionstring)  {  Blog blog = new Blog();  using (SqlConnection con = new SqlConnection(Connectionstring))  {  con.Open();  //etc We could also get data from an RSS feed of a blog (hence why I called the object Blog earlier)\npublic class XML : IData {  public Blog LoadData(string Connectionstring)  {  XmlDocument myXmlDocument = new XmlDocument();  myXmlDocument.Load(Connectionstring);  Blog blog = new Blog();  foreach (XmlNode RootNode in myXmlDocument.ChildNodes)  {  //etc Both classes implement IData and have a method called LoadData which has a string parameter and outputs a blog object. The string parameter is either a connection string to a SQL database or the URL of the rss feed. Not sure if there is a better way of doing this bit, maybe the name of the string needs making more generic.\nNow we have some classes that implement an interface what can we do with them. Lets write a class called GetData which gets data but doesnâ€™t care if it comes form the rss feed or a SQL database.\npublic class GetData {  private IData _repo;  public GetData(IData repo)  {  _repo = repo;  }   public Blog LoadData(string Connectionstring)  {  var original = _repo.LoadData(Connectionstring);  return original;  } } When we call GetData we can either pass in XML or SQL as the class is not tied to either implementation. We could even write other classes that implement IData for testing purposes.\nMy full code can be found on github .\nThe advantages of writing code in this way include code that is easier to extend, easier to test and easier to maintain. This is only the start of my understanding and I am sure this is going to be a topic I come back to in the next few weeks.\n","date":"Mar 27, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/interfaces/","series":null,"tags":["C-Sharp","Interface","Programming"],"title":"Interfaces"},{"categories":null,"content":"Today I spent some time learning the R language.\nThe problem I was trying to solve was to convert local prices of some items into Euros. I had been using a fixed exchange rate for all data, but as exchange rates fluctuate so much this is incorrect.\nMy first though was to find a free API that I could query to get the values I wanted. The first API I found didnâ€™t cover all the currencies, the next one I found I burnt through the free allowance in one pass.\nA colleague of mine mentioned using R to solve this, he sent me some links and I set out to write my first piece of R code.\nMy finished code can be found on github and I will attempt to explain some of it.\nR defines functions fairly simply\nnameoffunction \u0026lt;- function(arg1, arg2) { arg1 * arg2 } I have created a function that takes 2 parameters date and currency. I know I have about 10 different currencies that I want to get currencies for and I want to loop through each day so I will need to pass in a date.\nThe source of my exchange rate information is the www.xe.com website, its historical exchange rate page passes currency and date into the query string so I should be able to build up a string containing all the different elements.\nAll programming language can concatenate strings and R is no different. R uses paste()\nvar \u0026lt;- paste(\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;) However R has an annoying feature in this function. I would expect that var in the above example would contain \u0026ldquo;HelloWorld\u0026rdquo;, it doesnâ€™t it contains \u0026ldquo;Hello World\u0026rdquo;. Why it automatically adds a space I donâ€™t know?\nvar \u0026lt;- paste(\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;, sep=\u0026#34;\u0026#34;) I am not entirely sure what all of the code does but I can take a good guess.\nread_html() I would guess loads a html page, html_nodes() finds all the html tags of a certain type on the page, in my case \u0026lt;table\u0026gt;, html_table() reads the first table it finds.\ntable1[2] selects the second column, and head() selects a specific number of rows. I want the first row and second column so I combine these two as head(table1[2],1)\nNow that I have found my exchange rate what do I do with it? R can read and write to SQL Server so why not store this info in a SQL lookup table. I can then use this data in a stored procedure when I process my data.\nTo query sql you can use sqlQuery(), it has two parameters, a sql connection and a TSQL command (eg a SELECT, INSERT, UPDATE statement)\nI use a while loop to loop through every day between 1st October 2016 and today and look up the exchange rate for each currency.\nFor now I am manually running this R script, however there are ways to run R directly from SQL Server which I may well investigate. I could then have a SQL job to run this on a schedule, maybe once a day to get the latest exchange rates. I also would like to do something a bit cleverer like only getting exchange rates for the days that I need them by querying existing database tables.\n","date":"Feb 27, 2017","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/02/Exchange-Rate-Calculator.jpg?resize=300%2C202\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2017/learning-r/","series":null,"tags":["ExchangeRates","R","Programming"],"title":"Learning R"},{"categories":null,"content":"I am a big fan of Azure but I know zero about its biggest rival â€“ Amazon Web Services or AWS.\nSo lets sign up for a free trial and see what it can do. The AWS free trial is available from https://aws.amazon.com/free/ and lasts for 12 months. From memory I think the Azure free trial lasted only one month.\nTo start you need to login with your amazon account and create an AWS account. This requires your name and address and your payment info (you will only get billed if use services not covered by your free trial).\nInterestingly AWS requires you to verify your identity via an automated phone call. (I donâ€™t recall doing anything like this for Azure but please correct me if I am wrong.)\nOnce you are logged in you get a series of links displaying all the different services that are available. First impression is this is a simpler view to Azureâ€™s portal with a similar amount of services. At the top right is an option to select which region you want to use, in Azure I use North Europe and West Europe, AWS has Ireland and Frankfurt.\nCreate a Web App First thing to try is setting up a website. I selected create a web app and I get a page asking me for its basic details (very similar to Azure, however AWS asked what language your code is written in, Azure handles all of these) AWS websites appear to support a host of different options similar to Azure.\nThe actual creation of your website takes a few moments (like on Azure). However the default URL for websites is similar to http://test.vjbbimyv7w.eu-central-1.elasticbeanstalk.com/ which is not quite as nice as the Azure equivalent http://test.azurewebsites.net Azure has a host of command lines available via powershell. AWS has a similar command line interface called AWS CLI, including the option to deploy from git to your website.\nAWS Toolkit for Visual Studio is an extension that allows for the publishing of websites to AWS. (Just like you can publish to Azure)\nAs I learn more about AWS I will continue to blog about it. Amazon Web Services Pt 2 ","date":"Jul 21, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/amazon-web-services/","series":null,"tags":["AWS","Azure","Amazon","Programming"],"title":"Amazon Web Services"},{"categories":null,"content":"Thatâ€™s right this is the one hundredth post that I have written on this blog.\nSo what have I learned in the past 100 posts?\n It is easier to write a blog before you become a parent. More recently I am increasingly finding it difficult to find the time to blog. It used to be that I could write on an evening, but now James is around I often prefer to play with him, or more often stop him crawling where he shouldnâ€™t. I like my job. The inspiration for most of this site is my day job and as you can read, I do a wide variety of different things, but I have a lot more to learn as well. Finding your niche is hard. 100 posts in and I am still not sure what my niche is. I started out with the broad niche of IT and what I do, I then considered something about IT and fatherhood but I donâ€™t think that topic is really me. My current thinking is maybe DevOps especially as my role these days fits squarely between Development and Operations. Its time for a refresh. I have been meaning to change the theme of this site for some time and I feel after 100 posts now is as good as time as any. I want to emphasise my skills and what I am learning and increase the emphasis on DevOps, which I think will be my niche. Watching visitor numbers is addictive. Every day I look at how many people have looked at my blog, but I have yet to see a pattern between what I write and how many reads I get. Is my writing getting better? I donâ€™t know. Are more people reading? Probably not. Will I keep going? Yes  So what is next?\nHopefully a refreshed look in the next few months. Hopefully regular posts. If there is something you want to see on here drop me a message via any of the social media links or put a comment below.\nWhat is my favourite post?\nMaybe User Groups and F# which proved very popular and got me to start going to user groups something I still enjoy today. I also like Coding Myself Into A Corner which got me to start thinking more if I was giving myself future problems. But there are many others I like such as James and Becoming a father ","date":"Jun 2, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/i-m-100-blog-posts-old/","series":null,"tags":["100","Blogging"],"title":"Iâ€™m 100 blog posts old"},{"categories":null,"content":"I use SQL Server Management Studio all the time for writing queries, getting data and general SQL development.\nI have enjoyed seeing the improvements that each new version of SQL Server Management Studio (SSMS) introduced. One great improvement was intellisense.\nThis feature saves typing and reduces errors by automatically suggesting tables, column names or other database objects.\nA common query that I get asked to write is provide a spreadsheet that gives the information that satisfies certain criteria. This is easy to do in SSMS, you can write the query, click execute and the rows that satisfy the criteria are displayed. These rows can then be easily copy/pasted into Excel or other spreadsheets.\nA common data item that gets stored in databases is addresses and addresses often contain line breaks to make the data display better. In the earlier versions of SSMS, when you copied and pasted these line breaks were ignored and the data displayed the same in SSMS as it did in Excel. However in the more recent version, theses line breaks got copied across breaking your spreadsheet and making it hard to see what data corresponded with what.\nNow I donâ€™t know if this should be described as introducing a bug or fixing one. I can easily argue both sides. If your data contains a line break and you copy this data it should include the line break in the destination, but if it does that it displays badly in Excel.\nThe fix I have been using until recently is to use the following TSQL command in my queries.\nSELECT REPLACE(REPLACE(@str, CHAR(13), \u0026#34;), CHAR(10), \u0026#34;) This command replaces any line breaks with an empty string. Both Char(10) and Char(13) are needed because you can have different types of line breaks. This is great if you are writing the script from scratch but isnâ€™t great if your are running a stored procedure or your query has a lot of columns.\nThe answer to this is to use Visual Studio to run your SQL query. In Visual Studio you can write and run queries via Server Explorer and the results produced donâ€™t contain line breaks. I have only just discovered this solution, but so far it has worked and is very easy to do, plus as I do most of my development in Visual Studio anyway it saves me having to open SSMS to test my queries.\n","date":"Dec 3, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/2015/sql-server-management-studio/","series":null,"tags":["Database","Programming","SQL"],"title":"SQL Server Management Studio"},{"categories":null,"content":"As you have probably heard Star Trek is coming back to TV. I recently joined with the Sci Fi Waffle podcast to discuss.\nSci Fi Waffle: Episode 12 â€“ STAR TREK IS BACK!!\nIn this episode we discuss the great news that Star Trek IS BACK!!.\nShawn and I are joined again by James Roberts and TrekMates News feed Editor Simon Foster to speculate about the announcement of a new Star Trek TV series due to debut in January 2017.\nJames can be found on our sister podcast The Battle Bridge and Simon can be found on Twitter @funkysi1701 To listen go http://www.trekmate.org.uk/sci-fi-waffle-episode-11-star-trek-is-back/ , to leave feedback go http://forum.trekmatefamily.com/ ","date":"Nov 12, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/11/3f448330857e4ca7ba40ab5465a3c2cb62cdb98c.png?resize=620%2C260","permalink":"https://www.funkysi1701.com/posts/2015/star-trek-is-back-in-2017/","series":null,"tags":["StarTrek","podcast"],"title":"Star Trek is back (in 2017)"},{"categories":null,"content":"I have been asked to investigate adding a fail over internet connection to one of our offices.\nCurrently this office is connected via a wireless link with our head office. Unfortunately we have recently experienced some poor performance with this connection, this has now been corrected but like all technology there is the chance of it failing in some way in the future and causing loss of business because of it.\nInternet Connection This is one of my first considerations. I need a reliable internet connection and there are a lot of options to consider, lets look at a few.\nADSL Connection This is your standard internet connection that most homes have, you get around 20Mb/s download but only 2Mb/s upload for a fairly low price per month. Speeds are dependent on area and the quality and distance from your exchange, some rural areas have speeds much lower than this. As this connection is for an office I want the best connection I can afford so would rather not go down this option if I can avoid it.\nFTTC (Fibre to the Cabinet) This is what ISPs typically mean when they say superfast broadband. The cables that have been laid to your exchange cabinet have been upgraded to fibre optic cables which are capable of much faster transfer speeds (eg 40Mb/s download and 10Mb/s upload). This is my ideal choice, however BT Openreach are responsible for upgrading the country to these cables and they are not very quick about it. A few years back when investigating this FTTC had almost reached our office, but when I investigated today nothing seems to have progressed. Both FTTC and ADSL require a phone line into the office to run on.\nLeased Line This is where your ISP connects a dedicated line to your office which provides you a much faster connection (upload and download speeds are the same) but this is very expensive. We have this for our head office as this is where our servers are located and the faster speed benefits the entire company. We do not want to do this for every branch office as it will be too expensive.\nSpur from a Leased Line It could be possible to branch or spur off the leased line to our second office. This would be one leased line that terminates in two locations instead of one. However due to the distances involved I do not think this will be possible in my case and may end up costing as much as a second leased line.\nISP As well as the type of connection I need to decide who will provide it. There are hundreds of ISPs out there but I like to use ISPs that have a good reputation or I have used in the past and have been happy with. I do not stick with one ISP for all my connections as I like to have some connections that continue to work should that ISP be experiencing problems. ISPs that I would recommend are YDS, Eclipse and PlusNet\nMoney Off While I am talking about internet connections the government has been offering a scheme offering businesses money off upgrading their connections to a superfast connection (either FTTC or Leased Line) More information can be found on your councils website, If you have an office in York have a look here for more details. We got Â£3000 off installation so worth investigating if you qualify.\nVPN Connection Offices require access to far more resources than just the internet. This particular office requires access to our email and database servers as a bare minimum. One way to access a remote offices network resources is through a VPN (Virtual Private Network) connection. So my next consideration is how to establish a VPN link between our offices.\nSoftware Windows Server includes RRAS (Routing and Remote Access Service) which you can use to configure a VPN connection. One important thing to note is that the server needs at least two network connections, one on the internet with a public IP and one on the internal network.\nHardware The more expensive routers often have settings that allow the configuration of a VPN connection.\nI have used both of these methods in the past. RRAS is a pain to work with but has evolved with the newer versions of windows server, so may not be as bad as I remember. Using a router to do VPN introduces its own set of problems and there is no guarantee of avoiding RRAS, you may still need it to authenticate the VPN provided by your router.\nMultiple Connection Handling Internet connectivity is being provided in two ways, the primary way is via a Wireless link and the second way is via one of the options above.\nThere are lots of options to deal with multiple connections, do I want to load balance both connections and use them all the time, do I want to fail over onto the second connection only if the first one fails, or do I want it to be a manual process or switching over to the backup network if problems occur.\nAs you can see there are lots of different technologies to consider before I can add resiliency to this office and I havenâ€™t even started to think about if an additional server will be required.\n","date":"Sep 24, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/08/14091d1357164106-internet-connection-drops-every-couple-minutes-cable-sxchu-internet1.jpg?resize=660%2C252\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2015/adding-internet-connection-resiliency/","series":null,"tags":["Fibre","VPN","Internet"],"title":"Adding Internet Connection Resiliency"},{"categories":null,"content":"For the past few months I have been deploying changes to my companies database every couple of weeks or so. Over a weekend when the database was not being used I would make a backup of the database, load up visual studio, deploy the database changes and copy a couple of front-end files.\nMy weekends and evenings have recently become a bit more precious to me since the arrival of my son James. At suitable times to run the database upgrade I am either, asleep, about to fall asleep or be spending time with James.\nThis entire deployment can be set to run at a time of my choosing using SQL Server Jobs, (not sure why I never thought of doing this before). I still need to check everything is working after it has run. The job can send me an email if it encounters any problems so I can still ensure the company has the minimum of problems but has more frequent changes.\nUnmanaged Database Deployment OK so what do I need to do to set this up.\n Test all your code changes are working correctly on a backup of the database and everything is committed to source control. Backup everything that is going to be changed so it can be rolled back in case of a problem. I donâ€™t rely on the daily backup jobs for this, I do my own. Maybe I am a bit paranoid, or maybe I am just being cautious.  BACKUP DATABASE DBName TO DISK=\u0026#39;E:\\SQL Backups\\Filename.bak\u0026#39; I would include in the backup file name the date and time of the deployment for easy reference later on. For the front-end files these are backed up daily and only ever change during a deployment so I am going to rely on the day to day backups, plus they are in source control so in a worse case scenario I can look there.\nFrom Visual Studio create a deployment script using the publish option and save as a file on your database server. Create a SQL Server Job and give it a descriptive name. In steps create a step called backup and enter the T-SQL code above. Set this to stop on failure and continue on success. Create a second step called upgrade of type operating system with the following code.  sqlcmd -S (local) -i \u0026#34;E:\\SQL Backups\\deploy_test.sql\u0026#34; The -S parameter is the name of your SQL Server instance and the -i parameter is the path to the sql script you generated from Visual Studio above. Set this job to quit reporting success or failure.\nIn schedule create a date time you want the job to run, make sure this is a one time job as you donâ€™t want it to try and run again possibly causing problems. In notifications I have set to email on completion, you can set it to email on failure only. I would prefer to know the outcome of the job regardless. Using SQL Server jobs to copy files over the network is possible but is not trivial to setup so I am going to use windows task scheduler for this. Create a job in task scheduler to copy all the ADPs or ADEs that have been changed. If the SQL job fails you will need to copy the old versions of these back but that is simple enough to do.  This process can be expanded to check code out of source control and do even more automated deployments but for now this is good enough for my purposes.\n","date":"Sep 10, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/09/deploy.jpg?resize=263%2C300\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2015/unmanaged-database-deployment/","series":null,"tags":["SQL","Programming","Backups","Deployment"],"title":"Unmanaged Database Deployment"},{"categories":null,"content":"I am pleased to announce the birth of James David Martin Foster weighing 9 lb 12 on 19th August 2015.\nMother and Baby (and Father) are doing well.\nI am not a baby person, I hate children and I run a mile from other peopleâ€™s babies.\nBut James performed some magic on me and now I will do anything for him. If you have had children you probably know what I am talking about and if you havenâ€™t you are probably like me 24 hours before James arrived.\nDuring the pregnancy I had been living in denial that I would become a father, despite the wife speaking about nothing else and our home filing with baby equipment. It was only minutes before I first saw my son that I changed.\nDue to Jamesâ€™s large size, the birth wasnâ€™t easy. After about an hour of pushing the midwife called in a team of doctors and nurses to help and I think it was at this point that it started to hit me. I am not an emotional man, if anything I tend to repress my feelings but I completely and utterly failed to do that on this occasion. By the time the baby had been placed on my wifeâ€™s chest tears of joy were streaming down my face and this continued for some time. Moments after birth James had a tight grip on my finger. His tiny hands were like miniature versions of my own hands.\nI have been asked what it felt like holding James in my arms for the first time, this happened on the following day. I am not sure if I can put it into words, but a few adjectives come to mind, amazing, great, fantastic, humbling. James depends on me (and his mother) for everything and his existence has put a new spin on everything I do. I want to be there for him, help him learn stuff, provide for him and love him.\nI have been working very hard recently, spending my free time writing this blog, learning stuff and doing other work. The reasons for this hard work are now clear, I work so I can provide for James and so I can spend my free time with him.\nBefore James arrived I had it in my head that his mother would do the majority of the caring for him. This is not going to happen now that I have met James. I want to cuddle him, change his nappies, dress him, hold his hand, take him to school, watch TV with him, stroke his head when he is ill, tell him about my life experiences, introduce him to wine, tell him about girls, help him move home, etc.\nI am not saying being a parent is going to be easy. I am pretty clueless about what is in store for us, but already I am worrying if he is OK, trying to calm him down when he is upset and being woken in the early hours.\nAnyway only time will tell if I continue being this positive about fatherhood, but so far I am loving it. Normal blog posts will hopefully resume over the next few weeks (or when I have time).\n","date":"Sep 3, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/07/james5.jpg?resize=300%2C225\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2015/baby-magic-and-becoming-a-father/","series":null,"tags":["Fatherhood","Dad","Baby"],"title":"Baby Magic and Becoming a Father"},{"categories":null,"content":"I recently saw this blog post by Brent Ozar that I thought I might discuss.\nBrent listed 13 questions to ask about a database before you start working with it. I am going to go through these 13 questions and expand on them based on my experiences.\n Is this database in production now?  I think it should go without saying that the first thing you should find out is if your database is in production. If its not in production you can do what you like and no one will notice. I know what databases are in production and which arenâ€™t where I work so I can answer this one.\nIf this goes down, what apps go down with it?  What apps are running on what databases is a good second question. I have at least one database which has multiple front end apps. At first glance you would think that that these two apps are not connected but they are, I need to be careful with both of these apps to make sure they donâ€™t break each other. I know what apps run off what databases.\nWhen those apps go down, is there potential for loss of life or money?  This is a difficult question so I will split it into two. Loss of life, my databases donâ€™t control life support machines or nuclear weapons so my first instinct would be to say no. However it is not that straightforward, what if your app allowed contractors to know the location of dangers inside a property they were working in. Once your app goes down, they could have an accident due to lack of information. Loss of money, this one is more straightforward. Time is money in the business world so any time that your app is down and your employees are not able to work is a loss of money. If your database is linked to an eCommerce site, the loss of money could be extremely high. I know what affect downtime will have on my users and business.\nHow sensitive are these apps to temporary slowdowns?  Similar to the previous question, a slowdown can be as serious as downtime for some applications. Luckily most of my apps are internal only so are not seriously affected by slowness.\nWhen was the last successful backup?  I manage the backup schedule for all my databases so I know exactly when each one was last backed up. When ever I do anything to a production database I will run a backup so I can roll back in case of problems. As part of developing changes I run my changes on a backup of the data. I can script all my changes and repeatedly run them against a backup until I am sure no problems will occur.\nWhen was the last successful restore test?  More important than a backup is testing restoring your databases. If you canâ€™t restore data then your backup is useless. I try to test restoring my backups at least weekly so I know that I can rely on my backups.\nIs everyone okay losing data back to the last successful backup?  If disaster strikes you could loose all data between now and the time of your last backup. But all is not lost transactional backups can be scheduled throughout the day, in my case the most data we could loose is 15 minutes. This could be configured to be more or less frequent depending on your data. But remember the previous question and make sure you test a restore of your transactional backups, if you canâ€™t restore from them you will be forced to restore from the last successful backup.\nWhen was the last successful clean corruption test?  Corruption can be a killer if it is not found quickly. If you need to restore to the last backup before corruption occurred this could result in a significant amount of data loss. To check for corruption you need to run DBCC regularly.\nDo we have a development or staging environment where I can test my changes first?  If the answer to this is no, then your next job is to setup a development or staging area. Having a development environment makes development a lot easier and I donâ€™t think I could manage to do all the changes I have done recently without one.\nIs there any documentation for why the server was configured this way?  I really wish we had more documentation about configurations as it would make finding out why thing were setup the way they are. So unfortunately the answer to this question is No.\nWhat changes am I not allowed to make?  Depending on what your app does, where it is hosted, how quick the changes are needed and many other factors will all restrict what changes you can make. Historical decisions on the database can also affect what changes can be made, if the database has been structured in a certain way, it may be very difficult to restructure it in a more efficient way.\nWho can test that my changes fixed the problem?  This is an interesting question, from experience the best people to speak to about problems with the database are the users. If they can show you how to reproduce a problem, you should be able to fix this problem, after that you can probably get them to verify that it has been fixed.\nWho can test that the apps still work as designed, and that my changes didnâ€™t have unintended side effects?  This is an extension of the previous question. The main users of your app should be your first port of call to find out if the app works as expected. However exploring side affects and undesired features is something that I would test as part of the development process. It has taken a while but I have constructed a detailed check list that can be used for testing so I know that most bugs can be found before release.\n","date":"Jul 20, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/07/uf010206.jpg","permalink":"https://www.funkysi1701.com/posts/2015/things-to-know-before-working-on-your-databasethings-to-know-before-working-on-your-database/","series":null,"tags":["Backups","SQL","Programming","Disaster Recovery"],"title":"Things to know before working on your database"},{"categories":null,"content":"Last night I went to a Code Dojo at Leeds Sharp (the coding user group I have started going to). A Code Dojo is a programming challenge that people work on usually in pairs.\nThe challenge that we worked on was to code a solution to the puzzle game Sudoku. The code we worked with can be found on github.\nI donâ€™t know much about Sudoku but the game goes like this. You start with a 9Ã—9 grid, and the idea is to fill in all the missing numbers.\nEach Row can only use each digit 1 to 9 once, each column can only use each digit 1 to 9 once and each subgrid (3Ã—3) can only use each digit once.\nI worked with Richard one of the Leeds Sharp organisers, we started by trying to loop through each empty cell and insert values in a brute force attack, but this approach didnâ€™t get us very far (literally as the application crashed!)\nThe approach that we needed to use was to loop through calculating possible values.\nIt was only when I sat down at this dojo did I realise how much I still have to learn, but while talking with Richard about different approaches and different coding structures I started to learn.\n","date":"Jun 26, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/06/2000px-Sudoku-by-L2G-20050714.svg_.png?resize=660%2C660\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2015/sudoku-challenge/","series":null,"tags":["Dojo","C-Sharp","LeedsSharp"],"title":"Sudoku Challenge"},{"categories":null,"content":"Today is my day off, but I wake up and have a quick look at nagios to see if there is anything I need to worry about. Yes there is, SQL Server has run out of disk space on its data disk.\nI race downstairs and VPN onto the server to find out what has happened. One of my monitoring databases has had runaway log growth and is over 80Gb is size.\nBACKUP LOG [DBName] WITH TRUNCATE_ONLY DBCC SHRINKFILE(\u0026#39;DBName_Log\u0026#39;) Free disk space is back to normal, all users will be unaware of the problem and everything is fine again. I create a daily job that runs the above code, that way it should stay a manageable size.\nNext I need to find out why it happened and to prevent it happening again in the future (Next time I have a day off I want to lie in!)\nI check the SQL logs and notice\nBACKUP LOG WITH TRUNCATE_ONLY or WITH NO_LOG is deprecated. The simple recovery model should be used to automatically truncate the transaction log.\nThen I remember what I have done to cause this issue. I have a separate disk for my backup files and earlier in the week I noticed this disk was filling up, a large amount of space was taken up by transactional backup files. I thought I donâ€™t need to backup the transactions for this non critical database, I will just do a full backup at the start of everyday.\nHowever what I forgot is that a transactional backup keeps the log file under control, once this backup was stopped the log file grew uncontrollably. The answer, change the database from FULL mode to SIMPLE. This is my understanding of how backups work in FULL mode. A full backup is done at the start of the day which resets the log file, then changes in the database are stored in the log file, this is backed up into a transactional backup and the log file gets reset. If you have regular transactional backups throughout the day the log file doesnâ€™t grow too much, however with no transactional backups your log file contains an entire days worth of changes and so for a monitoring database this could be quite large.\nIn SIMPLE mode you canâ€™t do transactional backups and the log doesnâ€™t grow uncontrollably. This shouldnâ€™t be used for production databases as if there is a problem you could loose data.\n","date":"Jun 12, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/2015/runaway-sql-log-growth/","series":null,"tags":["ITAdmin","Backups","SQL"],"title":"Runaway SQL Log growth"},{"categories":null,"content":"Every developer uses source control, it is a great tool for keeping track of changes to your code, seeing who has done what.\nHowever I keep messing up, my use of it. I am fairly disciplined when creating new features, all my changes will get committed to source control and when I am happy I will deploy these changes to the live system. But as soon as there is a bug, especially one where the client is chasing for a fix, I will deploy a fix as soon as humanly possible on to the live system.\nAt first glance there is nothing wrong with what I have described but what happens the next time I deploy a new feature. Yes the bug the client was complaining about gets deployed with the new feature as the fix was never committed into source control. The client gets angry as the bug he was screaming about is back again.\nEvery change you make to the system MUST be committed to source control. If it isnâ€™t that change will look like it never existed. I have worked with source control for over 5 years why do I keep making this rookie mistake over and over.\nTroy Hunt has a blog post about the 10 commandments of using source control. His number two commandment is â€œIf itâ€™s not in source control, it doesnâ€™t existâ€ He talks more about code you have written being not saved into source control, but the principal is the same for my example as his.\nThere are ways to automatically deploy from source control, however most of the time you donâ€™t want your live database being rebuilt because you fixed a typo. Additional steps will need to be implemented and there is still the chance that you might want to bypass these steps to fix the urgent problem. The only way past this problem is for you and everyone on your team to be disciplined and only ever commit to source control first, and only after that deploy live (either automatically or manually)\n","date":"Mar 16, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/2015/source-control-fail/","series":null,"tags":["Git","Programming"],"title":"Source Control Fail"},{"categories":null,"content":"I have spent most of the day tweaking my Azure websites. Lots of fun!\nLast week unfortunately Azure had some problems and many websites that were running in the North Europe data centre were unavailable for several hours. And you guessed it my websites were hosted here.\nAll hosting providers are going to have downtime from time to time and this is just something you have to take on the chin. The important thing to do in times like these is communicate with your customers about what is going on and that you are doing everything you can to restore service.\nHowever Azure has some amazing features that you can configure to help manage when downtime occurs.\nAzure is Microsoftâ€™s global cloud platform. And it really is global, there are data centres in North Europe, West Europe, Brazil, Japan, two more in Asia and five in the US. In the event of problems it is highly unlikely that more than one of these would go down at once. If all of these are unavailable, I expect the planet earth is facing some kind of cataclysmic event and the fact that my website is down is not a priority.\n To take advantage of these multiple data centres, Azure has something called a Traffic Manager.\nTraffic Manager has various settings but I am using it in failover mode. This means that if one website goes down, the next one is used.\nAll you need to do is create a traffic manager, add two or more websites to it (called endpoints) and choose a page that needs to be monitored so Azure knows which websites are up and which are down.\nIf you are using SSL or custom domain names, there are a few extra steps you need to do. Your custom domain name needs pointing at the traffic manager, not the individual websites. The websites themselves have three domain names, the traffic manager address, the azure address and the custom domain name. The SSL certificate can then be assigned to each website that you have added to the traffic manager.\nThat was easy wasnâ€™t it, and now if a website goes down traffic manager will use the next one. While testing this, the transition to the next website was almost immediate. I did notice that if you had a browser showing the website open during a problem you sometimes got an error page, I think this was probably due to browser caching, reopening a tab or browser fixed this issue.\n","date":"Mar 12, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/2015/azure-traffic-manager/","series":null,"tags":["Azure","DevOps","ITAdmin"],"title":"Azure Traffic Manager"},{"categories":null,"content":"My desktop is always a mess. I constantly download files there and forget all about them.\nEvery now and then I copy files into sub directories, so my desktop looks sane for a day or two before it gets out of control again.\nWhy donâ€™t I write a script that I can schedule to do this for me. Then my desktop will always be tidy.\nI have written a few simple batch scripts, but of course the best scripting language out there at the moment is PowerShell. Lets use that.\nWindows provides a nice little utility for writing scripts called the Windows PowerShell ISE, so let\u0026rsquo;s start by loading that up.\nPS has lots of help included to help you, just run Get-Help [name of ps command]\nTo move files you can use Move-Item which works very similar to copy, specify source and destination. In my case I moved files based on their file extension.\nMove-Item *.pdf folder\nNow all I need to do is schedule this script to run either every day or so, or maybe every time I login or switch my computer on.\nPowerShell can do lots more interesting things which hopefully I will blog about soon.\n","date":"Mar 11, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/2015/tidying-my-desktop/","series":null,"tags":["PowerShell","Desktop","Programming","ITAdmin"],"title":"Tidying my desktop"},{"categories":null,"content":"Last week I blogged about the Game of Life.\nWell it took some searching through dead hard drives and old USB storage but I found the program I wrote, and better than that I have figured out how to turn it back into source code.\nThe file date is November 2004 and the source code has no comments so I donâ€™t know what was going through my head when I wrote it, or even what some of it does.\nTo view the source code download Life.src , to download the finished program download Life .\nI am actually surprised I was expected the code to be much worse than it was, I am not saying it is good, but I can see more than one method and I canâ€™t see any code that obviously is repeated. I can see some terrible variable names. Note to future self donâ€™t use tmp676_674!\nAssuming that I wrote this in 2004 what would I have been doing then? It was before I started working in IT. (Hard to imagine I know!) It may even have been before I started living with Keith, before he recruited me for my first IT role. I think I was probably working as an administrator for Defra at this point.\nIf we put 2004 in technical terms, it was before I joined facebook, Office 2003 and XP would have been installed on my PC, and probably felt brand new.\n","date":"Feb 21, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/02/Life.jpg","permalink":"https://www.funkysi1701.com/posts/2015/source-code-for-game-of-life/","series":null,"tags":["Java","Programming","SourceCode"],"title":"Source Code for Game of Life"},{"categories":null,"content":"I am lazy, I wonâ€™t try and deny that. When my alarm goes off in the morning, I will snooze it for twenty minutes or so before getting out of bed.\nIn my work my laziness continues. Remote Desktop (or RDP) is probably my number one laziness tool. For those that donâ€™t know RDP allows you to connect to another computer and access it like you were sat in front of it. So I can be sat at my desk and RDP into any other computer in the office including any server. However this laziness tool does sometimes require a bit of effort sometimes, powering on the target computer, logging on locally, making sure the user account you are using is allowed to use RDP.\nWriting a script is another example of a laziness tool. I often get asked to do tedious and long-winded task, because I am lazy I will go out of my way to learn how to write a script to do this, so that I can run this script and do this long-winded task in a matter of seconds. There are loads of different types of scripts from database SQL scripts, to PowerShell scripts that can do almost anything on your server.\nPowerShell is something Microsoft are really pushing at the moment, you can even write scripts to create new user accounts, so no longer will you have to remember to tick that tickbox for every new user account. And because PowerShell is part of almost all MS technologies you can link Exchange to Active Directory and you donâ€™t even have to remember the right syntax as PowerShell has a built-in help command to tell you how to run that useful command.\nAs I am now moving into a more developer role can I continue to be lazy? I sure can. Code should be written once and reused as often as possible and this is one of the features of OOP (Object Oriented Programming).\nIt can be as simple as creating a master page so you donâ€™t need to recreate the same code on every one of your webpages. Or every time you find yourself rewriting the same code again, you plug it into a method so it can be called again and again.\nBut of course the ultimate way to be lazy is of course get yourself some staff and spend all day getting them to do everything. If you are lucky they may even try and adopt some of these lazy ideas.\n","date":"Feb 4, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/02/bill-gates-quote.jpg","permalink":"https://www.funkysi1701.com/posts/2015/laziness/","series":null,"tags":["PowerShell","RDP","Lazy","ITAdmin"],"title":"Laziness"},{"categories":null,"content":"The first program anyone writes in a new language is often really simple and just displays the text â€œHello World!â€, below are a few examples from languages I have knowledge of.\nConsole.WriteLine(\u0026#34;Hello, world!\u0026#34;); \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello, world!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; php echo \u0026#39;Hello, world!\u0026#39;; ?\u0026gt;MsgBox(\u0026#34;Hello, World!\u0026#34;) SELECT \u0026#39;Hello, world!\u0026#39; Or maybe\nSELECT * FROM table WHERE name = \u0026#39;Hello World!\u0026#39; \u0026#39;Hello, world!\u0026#39; console.log(\u0026#39;Hello, world!\u0026#39;); PRINT \u0026#34;Hello, world!\u0026#34; Turns out I know quite a few programming languages. And yes there have been times where I need to stop and think about which language I am writing in.\n","date":"Jan 10, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/01/512px-HelloWorld.svg_1.png","permalink":"https://www.funkysi1701.com/posts/2015/hello-world-2/","series":null,"tags":["Languages","Programming","SourceCode"],"title":"Hello World!"},{"categories":null,"content":"I have done a few interviews from both sides of the table, Iâ€™m not very good at either but thought I would have a go at answering my favourite questions.\nDescribe an IT disaster and what you did to turn it around?\nThe question is all about the turn it around bit. Iâ€™ve had lots of answers that emphasise the problem rather than what the candidate did to turn things around.\nMy answer would be: on a Saturday I was rearranging the server room and when I came to turn on our main file server and pdc it wouldnâ€™t boot. My plans for the weekend went out the window it was all about getting this server back up. I tried the usual unplug everything and reconnect still nothing. I rang a friend to get a second opinion and between the two of us we formulated a plan of action. I then rang my director to tell her that I was having problems and what I was going to try. (keeping people informed is an essential skill)\nSo the server in question was getting power but nothing was happening during boot. The motherboard had died. What I needed to do was connect the raid card to another server so I could copy the data from it to our Nas drive (luckily I had an upgrade plan and this failure had just accelerated it)\nA few hours later the data was copying and I could breathe again. I told my director that I had fixed things but there would be minor issues on Monday.\nDescribe your strengths and weaknesses?\nIt wouldnâ€™t be an interview without a strengths and weaknesses question. As an interviewer I want at least one weakness and I want it to actually be a weakness.\nMy strengths are my problem solving skills, I can look at a problem and investigate what is going on and find a solution. If its a technology I havenâ€™t used before I can read up about it and find out how it works and then use it to solve the issue.\nMy weakness is my interpersonal skills, I am much better at analysing a computer issue than figuring out why one member of my staff is not performing. This is an area that is improving over the last few years I have taken on more responsibility over people and I am learning more about getting the best out of them.\nTechnical Questions\nI have never included technical questions in one of my interviews until recently and I think it is a good way to gauge ability. I often come out of an interview unsure how well that person would perform under the stresses of my job. If I have something down on paper it is a good start.\nBut the personality that comes through during the interview must also be considered as most knowledge can be taught.\nA good technical question I have used in the past is write a paragraph explaining DHCP. This question illustrates what their knowledge is like for an essential technology, but it also indicates what their writing style is like. Could the passage they have written be given to a Director or Client?\nGood IT people need both technical knowledge and the ability to communicate at all levels, I still struggle to know if someone is good or not and even sometimes if I am any good.\n","date":"Oct 23, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/10/how-not-to-ace-the-technical-interview.jpg?w=550\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2014/interview-questions/","series":null,"tags":["ITAdmin","Interview","Tecnical"],"title":"Interview questions"},{"categories":null,"content":"Letâ€™s continue looking at a database schema for storing details of every Star Trek Episode. If you are new to databases, a schema is just the design of the structure of the database.\nWe have three tables, Episode, EpisodeWriter and Writer. See my last post for more details of these. It has been suggested that a slight change to this structure would enable storage of more of the creative staff.\nLets rename Writer and call it Credit, and rename EpisodeWriter and call it EpisodeCredit. Now any creative staff member involved in an episode can be stored in the Credit table. Lets alter EpisodeCredit and add an extra column called CreditType. CreditType is just a text field that stores the role that creative person had on that episode, it can be anything from Director, Writer, Actor, Science Consultant etc.\nIn case anyone wants to recreate the databases I have described here, I have saved the SQL on a separate page which can be found here.\nWe now have the ability to store information relating to the episode in the episode table and any creative people in the credit table. What else can we add to the database? How about a table that can be used to record when an episode was last watched. I am probably weird but sometimes I want to watch a Star Trek episode that I like but I havenâ€™t watched in ages.\nThe last watched table is really simple and just have a datetime field and episodeId. This can be further expanded to have a userId field if you wanted to keep track of what episodes your friends had been watching.\nAnother idea could be to tag episodes with certain themes or topics like Klingon episodes or meaning of life episodes or Kirk talks a computer to death episodes. Again this is fairly simple table containing TagName and EpisodeId.\n","date":"Oct 21, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/10/c285ab50-86a9-44c3-a0ce-d790acae7db1.jpg","permalink":"https://www.funkysi1701.com/posts/2014/to-boldly-go-where-no-sql-has-gone-before-part-2/","series":null,"tags":["StarTrek","SQL","Database","Programming"],"title":"To boldly go where no SQL has gone before Part 2"},{"categories":null,"content":"My last post proved quite popular so I am wondering if I can combine a post about IT and Star Trek.\nYears ago I used to have lists of Star Trek episodes, which included such information like original air date, production number, episode title and brief description.\nOne thing that was hard to keep track of was how many episodes were written by a specific person. This is because episodes are written by multiple people. A column called writer would then need to contain multiple people, another option would be to have columns called writer1, writer2 etc. This wouldnâ€™t help either as you wouldnâ€™t know which column a specific writer had been saved in.\nThe relationship between writer and episode is known as a many to many relationship. An episode can have many writers and a writer can have written many episodes. To achieve this structure in a SQL database you will need three database tables as it is not possible to create a many to many join between two tables. The first table will contain all the episodes, the second table will contain all the writers, the third table known as a junction table, will contain the relationship between the two.\nLetâ€™s do an example so we can see how this would work. Gene Roddenberry creator of Star Trek wrote the pilot episode â€˜The Cageâ€™. So Gene would be added to the writers table with an id of 1 and The Cage would be added to the episode table with an id of 1. In the junction table, it has two columns episode and writer, so we would enter 1 and 1 into these columns.\nSelect * from Episode e Join EpisodeWriter ew on e.id = ew.EpisodeId Join Writer w on w.id = ew.WriterId But if Gene Coon and Gene Roddenberry had writing credits on The Cage we would need to add Gene Coon to the writing table and add an extra entry to the episodewriter table.\nI am going to do more posts based on this as I expand the database structure to include other information, I may go on to create stored procedures for bringing back certain information or I may use this as an example to talk about coding a user interface.\n","date":"Oct 12, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/10/enterprise.jpg","permalink":"https://www.funkysi1701.com/posts/2014/to-boldly-go-where-no-sql-has-gone-before/","series":null,"tags":["StarTrek","SQL","Database","Programming"],"title":"To boldly go where no SQL has gone before"},{"categories":null,"content":"As a break from my usual topics I am going to talk about what I did over the weekend.\nI am a huge star trek fan, so me and my wife Laura went to Destination Star Trek, a convention held in London. This is a picture of us in costume, my wife created a tribble costume (A tribble is an alien race that just consists of a ball of fur) and I am wearing an original series captains uniform. We had a great time.\nThere are several things that made this trip special.\n Friends: After the last convention I remember thinking how much better the convention would be with a group of friends. Shortly afterwards I started listening to the trekmate podcast, eventually this lead to me helping out with their website and making friends via twitter with many of the hosts. This weekend was the first time I met up with them and it was great to put faces to names (or twitter names) Writers and Directors: Star Trek has a great philosophy. In the future human beings will put their differences aside and work together to explore the galaxy, this is one reason why Star Trek is so popular even after almost 50 years. This philosophy has been created by the writers and other creative staff over the years. My favourite Star Trek film is Wrath of Khan which was directed by Nicholas Meyer, this weekend I got my DVD signed by him and was able to tell him how I still enjoyed that film over 30 years after it was made. He said that he really appreciated me saying that. Listening to his talk later on, it was fascinating to hear his insights into making my favourite film. It is the writers and directors that make our favourite characters who they are, the actors themselves arenâ€™t allowed to ad lib, all they do is flesh them out with a bit of emotion or a mannerism, it is the writer that create our favourite lines or situations that we remember. Seven of Nine: During my teenage years I had lots of posters of Seven on my walls. Seven of Nine was the ex-borg that walked around the ship in a skin tight catsuit, but she was also highly intelligent and continued to explore what it was to be human. It was great to have my photo taken with her, even if it was rushed due to the large number of people that wanted their picture taken. Trivia: On the Friday of the convention, Laura took part in the beginners trivia challenge, she won. On Saturday I took part in the intermediate trivia challenge, I did really badly only got 3 correct, On Sunday I took part in the captain level challenge and I won. I really wasnâ€™t expecting that, after I did so badly the day before. Thanks Marc Stamper and Destination Star Trek for organising that it was great fun, and also fun to meet the other trekkies who I had been competing against via twitter. Party!: On Saturday night we went to a party, we had Romulan Ale to drink, we had some of the actors performing songs (James Darren was particularly good), and some of the actors mingled with fans, it was great to see the actors relaxing and if you were lucky you could chat to them, I asked Vaughn Armstrong how he was enjoying the music, he loved it.  This is just a taste of what I got up to, there was lots more, like seeing alien costumes, listening to talks, sitting on a mockup of the bridge. Only one way to end this post, Live Long and Prosper!\n","date":"Oct 5, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/10/DST3_Logo.jpg","permalink":"https://www.funkysi1701.com/posts/2014/destination-star-trek/","series":null,"tags":["StarTrek"],"title":"Destination Star Trek"},{"categories":null,"content":"Like many systems administrators I watched as the news came out yesterday about the latest version of windows. What would it be like? What would it be called? Would it be better or worse than previous versions?\nI was shocked when the name of the new operating system was revealed to be Windows 10. What really!? At first I though someone had tweeted a photoshopped image of the press conference, but no it was really going to be called Windows 10.\nWindows 10 is the next operating system to be released after windows 8, many had been calling it windows 9 or windows Threshold.\nMicrosoft has never had a fixed naming convention for its OSes, so should we really be that surprised at its new name.\n Windows 1.0 (1985) Windows 2.0 (1987) Windows 3.0 (1990) Windows 95 (1995) Windows 98 (1998) Windows ME (2000) Windows XP (2001) Windows Vista (2007) Windows 7 (2009) Windows 8 (2012) Windows 10 (2015)  One reason Microsoft may have gone with with Windows 10 is because they wanted to signify that the coming Windows release would be the last â€œmajorâ€ Windows update. What does it mean to be the last major update?\nIf you look at the list of OSes above you will see that between XP and Vista was a huge 6 years, this gap is/has caused a huge headache for IT professionals (me included). Software was written for XP because that is all that was available, when XP moved out of support earlier this year it caused loads as problems as people tried to get this software to work on more modern OSes.\nIf windows 10 is around for a long time we may end up with a similar situation to XP, in that loads of software will run only on it and cause problems if you try and port it to whatever comes next. I am only a trainee developer so I donâ€™t know what is involved with building an OS, but I would imagine there must be things that are very difficult or almost impossible to fix without ripping them out and starting again which is what happens when a new version of an OS is written.\nOnly time will tell if Windows 10 really is the last version of windows and what will happen to the OS in the future. What ever the case I am downloading a preview so I can have a look (I want my start menu back!)\n","date":"Oct 1, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/10/windows_10_fullwidth.jpg?w=800\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2014/windows-9-or-do-i-mean-10/","series":null,"tags":["Windows","Microsoft","Windows10","ITAdmin"],"title":"Windows 9 or do I mean 10"},{"categories":null,"content":"I want to learn Development and eventually move into a development role. But what is the difference between development and what I already do?\nPut simply Development or programming is the creation of new programs, websites or databases.\nIT Operations or System Administration is the administration of existing servers, websites or databases.\nI have been doing System Administration since I started my job back in 2006. What you do as part of this role can be very simple like setting up new users or computers to something very complex like upgrading the version of Exchange that the company uses for its emails.\nAs part of my role I have done a lot of tasks that could be described as development. I have created databases and built new database structures like tables, views and stored procedures. I have also assisted the development department with testing, setting up visual studio, building websites and databases from code.\nCan I describe myself as a developer? I would say yes, I have done plenty of work that is development work.\nCan I get a job as a developer? Possibly but it would depend on the role. I have many of the skills that developers use, but I have limited knowledge in many areas. What I need to do is concentrate my efforts onto the development work I do and delegate as much administration work as I can so my knowledge can increase.\nA Buzz word in IT at the moment is DevOps. I have lost count of the number of times .NetRocks or RunAsRadio have mentioned it.\nDevOps is the integration of IT Development and Operations, it emphasises the need for the two departments to work closely together to achieve common goals. In big companies the two departments can pull against each other if you are not careful, but in my case as I do both roles, it would be very difficult for the development side of myself to blame the operations side of myself for a problem.\nThis I think puts me in a good position, I just need to learn more development and I will make a valuable addition to someoneâ€™s development team.\n","date":"Sep 27, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/09/DevOps-Cloud.jpg","permalink":"https://www.funkysi1701.com/posts/2014/what-is-the-difference-between-dev-and-ops/","series":null,"tags":["DevOps","Operations","Programming","ITAdmin"],"title":"What is the difference between Development and Operations?"},{"categories":null,"content":" You may not have heard of Nagios but it has saved my bacon quite a few times.\nNagios is an open source server monitoring application that runs on many linux flavours.\nI canâ€™t remember exactly when I first installed nagios but I am guessing it was sometime in 2007/8. My boss gave me a book about it (which I never read) and told me to create a system to monitor the companies servers.\nNagios is not simple to set up. It relies on setting up various Hosts and services. Hosts are usually physical servers that you want to monitor and services are all the services you want to monitor. As this is a linux program all these can be configured by editing the right config file\nNagios is very flexible and can be expanded easily with the use of plugins, if you want to monitor something there is usually a plugin available. If you have a dell server running openmanage software there is even a plugin that allows the temperature of your server to be monitored.\nIf you want to monitor windows servers the use of nsclient++ is a real advantage. This is a simple client that runs as a service on your windows server. This allows nagios to track memory, cpu, disk space, performance and services, in fact almost everything that you would want to monitor.\nOver the years I have kept a close eye on Nagios and added extra checks as new services were added or problems encountered. A few years ago I dabbled with sending alerts out via SMS message and once I got a smart phone found an app to keep track of Nagios 24/7.\nBut recently I have started wondering if Nagios is the best way to monitor modern servers like 2012 or remote services like Azure. I want something that is easy to expand as your IT infrastructure expands. Something that relies on running on a linux OS requires your IT staff have a knowledge of linux and you keep that server maintained and updated.\nMy question is: Is Nagios still the best way to monitor my servers?\n","date":"Sep 24, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/09/nagios.png","permalink":"https://www.funkysi1701.com/posts/2014/i-love-nagios/","series":null,"tags":["ITAdmin","Monitoring","Servers","Nagios"],"title":"I love Nagios"},{"categories":null,"content":"My real name is Simon Foster but my on-line persona is Funky Si a nickname I was given during my university days which has somehow stuck. I am a Developer working in the North of England. I have been working in IT departments since 2006 and have a wide range of experiences. More recently in the Software Development space but also SysAdmin, Server Infrastructure, Databases and DevOps.\nIn my spare time I have created Pwned Pass a Xamarin Forms mobile app that makes use of the Have I Been Pwned? API to allow searching for breached emails and passwords. This is available from the Google Play store.\nCertifications \nTech I have used  .Net C# SQL Server Blazor Javascript jquery Azure Azure DevOps AWS Terraform Powershell git plus many others\u0026hellip;  Recommendations Simon is an excellent developer, always looking for new methods and technology to improve existing solutions, along with his own skills. He is open to new ideas, though not afraid to share his thoughts and feedback and guide others to the best solution possible. Itâ€™s worth noting that Simon has vast knowledge on hardware and infrastructure which really does compliment his skill as a developer. - Alasdair Thomson (Head of Business Data Solutions at NPD Travel Retail)\nSimon worked for my company for a number of years and was a very valued member of staff, and later on management. He is an intelligent and conscientious worker who gives over and above on a regular basis. He leads by example and was respected by his peers and the directors of the company. I would not hesitate to recommend Simon both in terms of the quality of his work and his ethics and values in life in general. - Alison Davies (CEO at Eurosafe UK)\nIf you want a developer who really understands DevOps, then Si is your man. With a mixture of operations, support, development and management experience, he can bring a balanced skillset to a team, as well as excellent knowledge of Azure, .NET, SQL and the web. - Keith Williams (Development Manager at IRIS for Cascade HR and KashFlow Payroll)\n","date":"Jan 1, 0001","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/09/1922276.jpg","permalink":"https://www.funkysi1701.com/posts/about/","series":null,"tags":null,"title":"About Me"},{"categories":null,"content":"On 19th August at 9.39 pm James David Martin Foster arrived weighing 9 lb 12.\nMy initial thoughts on fatherhood can be found here . More baby photos can also be found on my instragram .\nOn 11th November 2017 at 22:58pm Edward Leonard Alan Foster arrived weighing 11 lb\n","date":"Jan 1, 0001","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/07/james5.jpg?resize=300%2C225\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/funky-si-the-next-generation/","series":null,"tags":null,"title":"Funky Si: The Next Generation"},{"categories":null,"content":"Using the data supplied by Troy Hunt and his Have I been pwned? website Pwned Pass allows you to check to see if any password has appeared in a data breach.\nFor more details about Have I been pwned? check out haveibeenpwned.com and www.troyhunt.com.\nPwned Pass is a simple Xamarin app that allows you to type in a password and tells you if it has been used in a data breach.\nTroy Hunt of Have I Been Pwned? recently added a new API to his website which allows you to search his extensive database of pwned passwords, over 306 million of them. I have simply created a Android frontend to this API.\nIt should be noted:\u0026nbsp;Do not send any password you actively use to a third-party service â€“ even mine! I donâ€™t log anything that you type into my app and this app makes use of the\u0026nbsp;k-anonymity feature to avoid transmitting passwords.\nAs well as using Troyâ€™s new API I also take advantage of his existing APIâ€™s. You can search his extensive database of email addresses to see if you have been affected by a data breach all from my android app.\nPwned Pass is now available from the Google Play Store.\n\n","date":"Jan 1, 0001","img":"","permalink":"https://www.funkysi1701.com/posts/pwned-pass/","series":null,"tags":null,"title":"Pwned Pass"}]