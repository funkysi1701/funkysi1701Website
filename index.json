[{"categories":null,"content":"This morning I was listening to a podcast where the new features coming out for SQL Server 2022 were being discussed. This starting me thinking about what would be involved in upgrading.\nUpgrading production environments is complex and there are licensing considerations to take into account. However for non production workloads like development this isn\u0026rsquo;t a problem so lets look at that first.\nIn the past I have installed SQL Server Devloper Edition onto my laptop, this is fine but I have found that unless you are very careful you may end up with multiple different versions of SQL Server sitting around, and it is difficult to cleanly remove them without a fresh install of the OS.\nHowever in this day and age, Docker and Containers are king. My current development environment makes use of Docker and has a docker compose file which sets up SQL Server for this particular application, lets take a look.\n sqlserver:  image: mcr.microsoft.com/mssql/server:2019-latest  container_name: Sql  ports:  - \u0026#34;5432:1433\u0026#34;  networks:  - my-network  volumes:  - sqlvolume:/var/opt/mssql This defines which docker image to use, in this case 2019-latest, sets up ports and the name and saves the data on a docker volume.\nIf we then run SELECT @@VERSION on this instance of SQL Server we get told:\nMicrosoft SQL Server 2019 (RTM-CU13) (KB5005679) - 15.0.4178.1 (X64) Sep 23 2021 16:47:49 Copyright (C) 2019 Microsoft Corporation Developer Edition (64-bit) on Linux (Ubuntu 20.04.3 LTS) \u0026lt;X64\u0026gt; What if we change the docker-compose file to use 2022-latest?\nmanifest for mcr.microsoft.com/mssql/server:2022-latest not found: manifest unknown: manifest tagged by \u0026#34;2022-latest\u0026#34; is not found SQL Server 2022 hasn\u0026rsquo;t been released yet so there is no docker image for it yet. Try this command again in a few months when it is available.\nOK so what else can we try? What about a downgrade to 2017-latest? Will that work?\nSQL Server 2017 starts but the following error gets logged.\n2022-02-23 21:41:15.30 Server Software Usage Metrics is disabled. 2022-02-23 21:41:15.30 spid6s Starting up database \u0026#39;master\u0026#39;. 2022-02-23 21:41:15.34 spid6s Error: 948, Severity: 20, State: 1. 2022-02-23 21:41:15.34 spid6s The database \u0026#39;master\u0026#39; cannot be opened because it is version 904. This server supports version 869 and earlier. A downgrade path is not supported. Doh we can\u0026rsquo;t downgrade the existing database we have. Probably a good thing really.\nMicrosoft release regular updates for SQL Server called CU\u0026rsquo;s (Cumulative Updates), you can see above we are on CU13. Is there a CU14 or CU15 we could try?\nUpdate the docker compose to: mcr.microsoft.com/mssql/server:2019-CU14-ubuntu-20.04\nAt this point I actually got an error\n2022-02-23 21:50:20.71 Server Error: 17058, Severity: 16, State: 1. 2022-02-23 21:50:20.71 Server initerrlog: Could not open error log file \u0026#39;/var/opt/mssql/log/errorlog\u0026#39;. Operating system error = 5(Access is denied.). This is caused by trying to use SQL Server 2017 but it is easy to fix.\nIn docker desktop there is a volumes section, find the volume you are using with SQL Server and delete the errorlog mentioned above.\nNow if you retry SQL will start OK.\nRepeating the SELECT @@VERSION gives us a new CU\nMicrosoft SQL Server 2019 (RTM-CU14) (KB5007182) - 15.0.4188.2 (X64) Nov 3 2021 19:19:51 Copyright (C) 2019 Microsoft Corporation Developer Edition (64-bit) on Linux (Ubuntu 20.04.3 LTS) \u0026lt;X64\u0026gt; Microsoft SQL Server 2019 (RTM-CU15) (KB5008996) - 15.0.4198.2 (X64) Jan 12 2022 22:30:08 Copyright (C) 2019 Microsoft Corporation Developer Edition (64-bit) on Linux (Ubuntu 20.04.3 LTS) \u0026lt;X64\u0026gt; How much easier is this than manually installing updates and rebooting or attempting to uninstall and reinstall SQL Server. As SQL Server 2022 isn\u0026rsquo;t out yet I can\u0026rsquo;t say for certain what issues I may encounter but hopefully it will be as easier as this. And I don\u0026rsquo;t need to backup or restore and databases they are all available as before!\n","date":"Feb 23, 2022","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2022/header-01.png","permalink":"https://www.funkysi1701.com/posts/updating-sqlserver-with-docker/","series":null,"tags":["SQLServer","Docker","SQL"],"title":"Updating SQL Server with Docker"},{"categories":null,"content":"Today Microsoft celebrated 20 years since the first version of dotnet was released with a special live stream event.\n A lot has happened in the 20 years of dotnet (or .Net). It is my understanding that .Net was created to rival Java. When the .Net Framework was first created it was a windows only thing, but today modern .Net is a modern run anywhere platform, with .net applications running everywhere from PCs and Laptops, Raspberry Pi\u0026rsquo;s, Mobile phones and tablets (via Xamarin Forms), to Websites and Microservice APIs running on every Cloud platform out there.\nIf you want to celebrate this achievment tweet with the hashtag #DotNetLovesMe or download some of the digital swag available from github On Thursday the 17th Feb the first preview of dotnet 7.0 is going to be released, with the latest version 6.0 only being released last November.\nThe above graphic comes from the following tweet.\nThis is 20 years of #dotnet releases. #dotNETLovesMe pic.twitter.com/Zxfe1SdWTq\n\u0026mdash; Khalid ðŸŽŸ (@buhakmeh) February 14, 2022  What do you like about dotnet? When did you first start using it? What are you going to build with it next?\n","date":"Feb 14, 2022","img":"https://www.funkysi1701.com/images/FLjBrnPXwAQE8BN.jfif","permalink":"https://www.funkysi1701.com/posts/dotnet-is-20-years-old/","series":null,"tags":["DotNet"],"title":"dotnet is 20 years old"},{"categories":null,"content":"Is it to share my ideas? Is it to learn new technologies and techniques? Is it to create a following? Is it to educate others? Is it to build some kind of service? Is it some combination of all of these.\nHistory Back when the web was young and I was first learning HTML. I hand crafted web pages, adding photos I had taken with captions. If I needed a new page I just added a new html page and linked to it from another page.\nAs time went on I started to learn mysql and php and my website became a hand crafted php nightmare. I also applied what I learned to help my father run the website for his camera club.\nAt some point I started playing with WordPress. I have had various WordPress websites or blogs over the years. WordPress is very powerful you can do so many things, install so many plugins. WordPress runs on php and mysql and as my career started to centre around the .net space, I started to want something that was similar, so I could apply things I had learnt to my own website.\nThis has led me to the current state of my website. I have a WordPress blog, with most of my oldest content, my newer content lives on dev.to and I have a Blazor webassembly site that uses the dev.to api to run my new website.\nBlazor webassembly is great, however it has some limitations which I am starting to push against. To host this as cheaply as possible I am using Azure static web apps, so no .net backend all the website is front end. I have some Azure Functions that does the backend bits that I need.\nGoogle and other bots are not able to find any of my pages except index, due to the way Blazor works. I have got round this by pre rendering the content using https://prerender.io/ My next difficulty is how to generate a sitemap.xml or a rss feed for my blog. This has started to make me question my architecture decisions.\nI could use a hosted solution like ghost which is popular with a some of my peers. This would solve many of the problems I am currently facing but I wouldn\u0026rsquo;t be able to play with everything as it is hosted and therefore someone else\u0026rsquo;s problem. How important that is I will look at later.\nAnother option would be to use github pages, there are quite a few ways to publish a github page, Jekyll and Hugo appear to be the most popular. Both produce static content and both are a new for me to learn. Interestingly I could also publish either to Azure Static Web apps if github pages ends up not being suitable.\nSplit in two I think my website needs to be split in two. I need a stable blog platform probably using Hugo and github pages. This is what I want to get indexed by the search engines and be the primary way people find out about what I am doing.\nI then have additional sites, that I use as my playground for learning new tech. I can easily link between them and I can tweak the style so they \u0026ldquo;fit\u0026rdquo; nicely together.\nI am still considering what to do with dev.to. I like that I am using it as the backend for my blog posts, and its API gives me that flexibility to display that content where I want.\n","date":"Jan 25, 2022","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2022/WHAT-IS-THE-CORPORATE-WEBSITE.jpg","permalink":"https://www.funkysi1701.com/posts/why-do-i-have-a-website/","series":null,"tags":["website",""],"title":"Why do I have a website?"},{"categories":null,"content":"I\u0026rsquo;ve been running my website on Azure Static Web Apps for a while and it is pretty cool.\nWhen you create a Static Web App on Azure you get asked for the github repo of your source code and even the branch to use. Once you have selected this, you get asked for the type of code to deploy, mine is Blazor Web Assembly but you can use Angular, React or Vue.\nYou now have three variables to fill in the location in your code of the Website, the location of your Azure Functions and the output location usually wwwroot. Once you have set these three you can preview the GitHub Actions file that will be created and added to your repository.\nI get something like this\nname: Azure Static Web Apps CI/CD on: push: branches: - feature/tempbranch pull_request: types: [opened, synchronize, reopened, closed] branches: - feature/tempbranch jobs: build_and_deploy_job: if: github.event_name == \u0026#39;push\u0026#39; || (github.event_name == \u0026#39;pull_request\u0026#39; \u0026amp;\u0026amp; github.event.action != \u0026#39;closed\u0026#39;) runs-on: ubuntu-latest name: Build and Deploy Job steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_\u0026lt;GENERATED_HOSTNAME\u0026gt; }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Client\u0026#34; # App source code path api_location: \u0026#34;Api\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### close_pull_request_job: if: github.event_name == \u0026#39;pull_request\u0026#39; \u0026amp;\u0026amp; github.event.action == \u0026#39;closed\u0026#39; runs-on: ubuntu-latest name: Close Pull Request Job steps: - name: Close Pull Request id: closepullrequest uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_\u0026lt;GENERATED_HOSTNAME\u0026gt; }} action: \u0026#34;close\u0026#34; This github action will run when you create a Pull Request to the branch mentioned in the file, or if you push code into the branch. This code get added into the .github/workflows/ folder and is the location that all github action workflows live.\nI haven\u0026rsquo;t done much with github actions, however I have used Azure DevOps quite a bit. Over on the Azure DevOps side I have created a pipeline that deploys to a Dev environment, then a Test environment and finally a production environment.\nLets have a look at the workflow that I ended up with and with can break down how it all works. Note I am new to Github actions so if there is a better way of doing this do let me know.\nname: Azure Static Web Apps on: push: branches: - main - develop - feature/* jobs: dev: runs-on: ubuntu-latest environment: name: Dev name: Dev steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_ORANGE_POND_09B18B903 }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Blog\u0026#34; # App source code path api_location: \u0026#34;Blog.Func\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### test: if: github.ref == \u0026#39;refs/heads/develop\u0026#39; runs-on: ubuntu-latest environment: name: Test name: Test steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_WITTY_DUNE_0A1A77903 }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Blog\u0026#34; # App source code path api_location: \u0026#34;Blog.Func\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### prod: if: github.ref == \u0026#39;refs/heads/main\u0026#39; runs-on: ubuntu-latest environment: name: Prod name: Prod steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_BRAVE_ROCK_0AAC63D03 }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Blog\u0026#34; # App source code path api_location: \u0026#34;Blog.Func\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### The first thing I did was create three Azure Static Web Apps, I am using the free tier so while this is trippling my costs it is all still free! Doing this created three github action workflow files, I deleted two and edited the third, but before I deleted them I made a note of the AZURE_STATIC_WEB_APPS_API_TOKEN. If you look in your settings -\u0026gt; secrets for your repo you will see secrets have been created, this is the secure token that github uses to update your static web app.\nWhile we are in settings we might as well look at environments. I created a Prod, Test and Dev environment that I was going to use in my github actions.\nEnvironments can have various rules setup on them.\n Required reviewers - this is like an approver, a user specified here must aprove for the workflow to be deployed Wait time - I didn\u0026rsquo;t use this, but it looks like a certain amount of time can be set to pause the deployment. (I assume to do some kind of manual check) Deployment Branch - specify what branch are allowed to be deployed to what environments. I specified develop, main and feature branches could be deployed to the Dev environment, develop and main could go on Test and main could go on Prod Environment secrets - I didn\u0026rsquo;t use this as my secrets were already created, however it looks like your secrets can be associated with a specific environment  Now that we have the static web apps setup and the environments lets look at the github action file.\nFirst of all I removed the PR stuff and just concentrated on pushes. I wanted my workflow to be.\n Push to feature branch Deploys to Dev env PR feature branch to develop Once merged code gets pushed into develop Deploys to Test env PR develop to main Once merged code gets pushed into main Deploys to Prod env (after approval)  The approval on deploying to production I think is probably overkill, but I still have it setup like that for now.\nMy gh action has three jobs defined as dev: test: and prod: they are all the same except they have the azure_static_web_apps_api_token that is correct for their environment.\nThey also each have a environment defined eg\nenvironment: name: Prod Lastly Test and Prod have an if test setup, if the test is false the job won\u0026rsquo;t run. Importantly it won\u0026rsquo;t fail it just won\u0026rsquo;t run.\nFor Prod this needs to only run on main branch so we have\nif: github.ref == \u0026lsquo;refs/heads/main\u0026rsquo;\nFor Test this needs to only run on develop so\nif: github.ref == \u0026lsquo;refs/heads/develop\u0026rsquo;\nI could have a test for develop to only run on feature/* but I have allowed it to run everytime.\nThere is loads more you can do with github actions, but hopefully this gives you a taste of some of the things you can do. I currently have a mix of Azure DevOps and github actions so I will be working on getting github actions to do more.\n","date":"Jan 10, 2022","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bj77rx0jetjdf7c24nhk.png","permalink":"https://www.funkysi1701.com/posts/using-github-actions/","series":null,"tags":["GitHub","DevOps"],"title":"Using GitHub Actions"},{"categories":null,"content":"Lets have a look at what my goals were for 2021. I had eight of them, lets look at them one by one.\n  Azure certification. In May of this year I sat and passed the Azure Fundamentals exam. I am calling this goal as achieved.\n  Mentoring. I didn\u0026rsquo;t do anything about working with others or mentoring, so not sure I achieved this one. However I do now work in a development team and I have been reviewing others code and having my own reviewed. We have a junior developer and I am enjoying the opportunities I have to work with him and share my wisdom.\n  F#. I have done zero work with F# in 2021, so this one I didn\u0026rsquo;t achieve.\n  Cosmos db/Mongo db. I have worked with both of these technologies in 2021. I used Cosmos the most with my website, and storing data for it. I used Mongo/Atlas for auditing for a project I did for my previous job. I wouldn\u0026rsquo;t say I was expert in either of these, but I am starting to get a flavour of non SQL Server databases. I should also note that I am using mysql a lot in my latest job, so another non SQL Server technology that I get to use on a daily basis.\n  Give a talk. I said last year that maybe I would make baby steps towards doing this, and I have. In the interview for my new job I gave a short presentation (I thought it was bad, but others didn\u0026rsquo;t!) I also gave a short introduction to Blazor talk which went down well.\n  Mandlebrot Generator. Did nothing on this one as well. I may have googled the code but that is as far as I got.\n  Pwned Pass Mobile App. I got an increase in users in 2021 and hence it it still running and I still pay for the API key. I am still considering what to do with it.\n  Time for me. Achieved this one, had plenty of time for myself.\n  Not a bad year but what are my goals for 2022?   Video. I have just purchased a green screen, so my first goal is to learn how to use OBS, how to light myself properly without getting horrible reflections. I have the content for my first talk, the talk I gave at work about Blazor.\n  Conference. Attend an in person conference. I am booked to attend Scottish Summit, however it has been postponed due to COVID-19.\n  Blog More. I have been neglecting writing blog posts a bit recently, so I want to do more.\n  Metrics. I have been recording various metrics from twitter, github etc and I would like to expand this and make it a service.\n  Profile Pic. My profile pic is getting a bit old, so it would be nice to update the images I use online and improve my personal brand.\n  ","date":"Jan 1, 2022","img":"","permalink":"https://www.funkysi1701.com/posts/2022-goals/","series":null,"tags":["Goals"],"title":"2022 Goals"},{"categories":null,"content":"How exciting my Lynx Computer from my childhood has come home, all 96k of it pic.twitter.com/aeN38KBiS8\n\u0026mdash; Simon Foster (@funkysi1701) December 26, 2021  I can\u0026rsquo;t remember the syntax for BASIC, luckily I have been able to find the Manual .\nAll the commands are listed inside so lets see what we can do.\nThe Lynx presents you with a command prompt in which you can type text. Back in the 80s we had a tape player to load programs from tape, however I don\u0026rsquo;t have one today so only programs I write can be run.\nPRINT - To write Hello World, you can just type PRINT \u0026ldquo;Hello World\u0026rdquo; and Hello World appears on the screen. To Write a program that displays Hello World, you just write the line number first.\n10 PRINT \u0026#34;Hello World\u0026#34; To run this you type RUN To view the code you type LIST\nTo Edit a specific Line you can use Ctrl+E and type the line number, or you can just write the line out again.\nCLS - This command clears the screen\nINPUT N - stores text typed by the user and stores it in the variable N\nGOTO N - Execution of code continues at Line Number N\nThe first Program I wrote with a bit of help from my boys.\n10 CLS 20 PRINT \u0026#34;What is your Age?\u0026#34; 30 INPUT N 40 IF N\u0026gt;5 AND N\u0026lt;41 THEN PRINT \u0026#34;a good age\u0026#34; 50 ELSE IF N\u0026lt;6 THEN PRINT \u0026#34;a spaceman\u0026#34; 60 ELSE IF N\u0026gt;40 THEN PRINT \u0026#34;too old\u0026#34; My 4yo didn\u0026rsquo;t like being \u0026ldquo;too young\u0026rdquo; in the original version, so my 6yo helped me change him to be a \u0026ldquo;spaceman\u0026rdquo;.\nNot bad and it was fun pair programming with a 6yo, all my typos were quickly spotted, and he easily understood the logic of IF/ELSE/THEN statements.\nThe Lynx comes from 1983 and has just 96k of memory. I am very lucky it actually still works, however I have been able to find an emulator so I can write Lynx BASIC from the comfort of my laptop. jynxemulator , it is also on github but it doesn\u0026rsquo;t include the ROMs so getting from the website is a better option.\nThe developer experience today is so much nicer than it must have been in the 1980s, however back then distractions must have been much reduced.\n No internet or google to get answers to your questions No Copy/Paste of text No Load/Save (unless you have a working disk drive or tape player!) No IDE No Build or Release process just type RUN  I then have additional sites, that I use as my playground for learning new tech. I can easily link between them and I can tweak the style so they \u0026ldquo;fit\u0026rdquo; nicely together.\nI am still considering what to do with dev.to. I like that I am using it as the backend for my blog posts, and its API gives me that flexibility to display that content where I want.\n","date":"Dec 28, 2021","img":"https://www.funkysi1701.com/images/FHi_NyOXEAo9YbG.jfif","permalink":"https://www.funkysi1701.com/posts/back-to-basic/","series":null,"tags":["BASIC","History","Programming"],"title":"Back to BASIC"},{"categories":null,"content":"Welcome to Day 5 of the Festive Tech Calendar! Usually at this time of year I like to take the opportunity to look back on the last 12 months, highlight some of my achievements and make a few goals for the new year.\nLike many of you 2021 has been a difficult year, but it has also been a year of change, and I am in a much better place now than at the start of the year.\nTo tell you my 2021 story I first need to give you a bit of a history lesson about my career. So, lets fire up your flux capacitor (or Tardis of similar value) and journey back to 2006.\nBack to 2006 In 2006 I got my first IT job. My housemate, who was working as an IT Manager for a small Health and Safety Company, suggested I come work with him. I was currently working an admin job with no idea what to do with my life, so I said yes.\nSo, in October 2006 I joined as a member of the IT Team. I worked mainly on first line support, but as the IT department was small, I learnt about all sorts of IT things, from fixing printers, setting up the CEOs BlackBerry mobile phone with emails, administering active directory, writing SQL queries to swapping tapes for the weekly backups.\nThe company had many faults which I won\u0026rsquo;t discuss here, but the work was varied, and I was always learning something new so I stayed with the company.\nAt the end of 2010, the IT Manager resigned, shortly followed by the rest of the IT department. In January 2011 I was IT Manager, I had no staff, plenty of IT problems and no clue what I was doing.\nThis was an amazing time for me, I learnt how to interview and hire staff, I learnt more about the different systems we used. It was approximately at this point in time that I switched from being exclusively a SysAdmin to starting to learn Development.\nI had been creating simple websites for a while, mainly using PHP and MySQL. But since I started working in IT I learned more and more about databases, SQL Server and writing T-SQL scripts. I was the companies \u0026ldquo;database guy\u0026rdquo;, so the company naturally asked me to do more and more database work. This led to learning other technologies like MS Access and C# so I could do more and solve more of the company\u0026rsquo;s problems.\nHowever, the company was small, and I would always be dragged in to fix SysAdmin problems so I never managed to spend 100% of my time doing development, so in 2016 I moved on to my first full time development job.\nThe new job was great, but I kept in touch with my old CEO, Ally and helped out with bits of work on the side, writing SQL queries, updating the odd bespoke application.\nBack where it all began In 2019 I started formulating a plan to go back to my old job as a contract developer. To my amazement Ally almost bit my arm off to get me back, agreeing to all my terms. So, in the summer of 2019 I started full time as a contract developer, mostly working from home with the occasionally meeting in the office and supporting the business with IT issues.\nMy second stint with the company was different than my first, I was able to concentrate almost exclusively on development work. As the only developer I was in full control of development, I decided to use dotnet and Blazor.\nLate in 2019 I was given advanced warning that the company was being sold. In Feb 2020 the company did get sold, the new owners kept me on to finish the project I was working on but made me a permanent member of staff. In March 2020, the whole company began to work from home due to COVID-19, with parts of the business being furloughed.\nThis was a difficult time; on top of the stress the whole world was feeling from the global pandemic I was trying to assist with various IT integrations that you would expect when any company gets sold.\nJanuary 2021 This is pretty much where you find me in January 2021. I was working remotely developing internal applications with dotnet/Blazor with the occasional Teams meeting to demonstrate what I had built so far.\nWorking on your own as a developer is great! You start work and can write code all day. There is no daily stand up, you don\u0026rsquo;t need to explain what you have been working on, or what challenges you are facing. You don\u0026rsquo;t have any process or formality to follow, only what you impose on yourself.\nI created a build/release pipelines in Azure DevOps, I created pull requests that would kick of a build, run my unit tests, and run some static code analysis. I would glance over my PRs but 99% of the time I would approve them.\nThis was the greatest weakness of working on my own, no one to suggest ideas of better ways of writing code, no one to bounce ideas off, no one to encourage or be encouraged when I figured out something clever or offer to help you overcome a problem.\nRedundancy In Mid July I was told that the development function was no longer required, and my job was at risk of redundancy. As I was the only one working in development it sounded very much like a done deal. I had a couple of redundancy meetings to attend and I would need to talk through how various applications worked as part of a handover.\nTo be honest I agreed with the decision to make me redundant. New development work was not coming through, although the users of the applications I had developed loved what I had created, upper management were keen to migrate off these systems and use more centrally managed systems. If I was in charge, I would probably have made a similar difficult decision.\nAs soon as I found out I was at risk of redundancy I called up a couple of recruitment agents I had spoken to before. They told me that it was a great time to be looking for a job and immediately arranged some interviews for the next week. I also took advantage of social media and asked on twitter if anyone was looking for developers like me. To my surprise this resulted in at least one interview.\nOne thing I stressed in all my conversations with recruiters and companies I spoke with, was that I was looking for a team. I wanted to work in a team, bounce ideas off others, mentor others, learn from others. This was the most important thing I wanted in whatever my next role was going to be.\nOther important things for me was continuing to use Azure or similar cloud technologies. However, I always think that the tech stack and technology can be learnt on the job, tech moves so quickly these days that you have to be constantly learning to stay relevant.\nAbout two weeks after I first found out about my impending redundancy I contacted (or more likely was contacted by) a third recruitment agency. It was this third one that eventually got me a job, however all three were brilliant, and kept me updated and answered all of my questions.\nThe next few days were packed with phone calls, interviews, tech tests. I had a lot of different types of companies that I spoke with, from Software Consultancy companies, FinTech companies, Energy suppliers, Legal companies and many others.\nDuring this time of interviews, I can think of only two in person interviews. The rest made use of Teams, Zoom, Google Meet and even Skype (yes at least one company still uses it for video chats!)\nThe usual way the process went was a conversation with the recruiter about a role after which my details were sent to the company, If the company liked the sound of me an initial informal interview was arranged, after that a second more technical interview was held, sometimes there would be a tech test, sometimes it was more technical questions being asked in the interview. Some companies had more stages that this, but this was what I typically encountered.\nI lost count of the actual number of interviews I had, but I used a spreadsheet to try and keep it all straight in my mind. The last thing I wanted was to ask a question about the wrong company!\nPositive No I got plenty of noes from these interviews, however all were phrased as a positive no. That being they liked me as a person but something about my skill or experience wasn\u0026rsquo;t quite right. A friend once described me as an odd mix of junior and senior developer. I agree with that statement, I have a lot of experience in some areas and a real lacking in others.\nA common one was working in a team. Being a lone developer is not good for gaining experience working in a team. However, each time I spoke with companies I stressed that the team was what I was looking for.\nAnother weakness of mine is front end. When I am building something, functionality is more important than getting it looking good. I came from a database and backend start, so it is only natural that I am more at home working on these things. Also, my most recent projects are working with Blazor, which is brand new and few companies are doing much with it yet.\nI tried really hard not to be discouraged by these noes and that the right company was just around the corner. One no hit harder than most. I had an initial chat with the team of a FinTech Company and then there was a second interview with the CTO which I felt had gone OK.\nFrom what I had heard it ticked a lot of boxes, the team sounded good, with lots of support and development opportunities and they were moving to the Cloud, so my Azure experience sounded ideal. The CTO liked me and thought I would be a good fit, but I was part way between a junior and a senior, so he had gone away to try and make a bespoke role for me, eventually it would be a no.\nLooking back, it is easy to see, that this was really encouraging. They tried to create a bespoke role for me! However, at the time all I could think about was all the noes I was hearing.\nThe job for me About the same time, I had my initial interview with the company I would eventually accept a role with. I kept being told that there was a tech test to do, but for some reason it never got sent to me, so the interview featured a lot of tech questions and they really grilled me. I came away without much of a sense of if it went well or not.\nHowever, they liked me and wanted me to do a presentation at their office. I am rubbish at public speaking and doing presentations, so I thought I will do this presentation, but it is very unlikely they will like what I do.\nThe topic of my presentation was how I upskilled a team on a new project or technology. As a lone developer what an earth could I do for a topic like that? I thought about what I had done to upskill myself on various things, however that doesn\u0026rsquo;t really address the question, as they wanted to know more about what I had done to upskill a team.\nI thought back to a time when I helped get a largely un-version controlled codebase into source control and automated the build and release pipeline using Azure and Azure DevOps. This involved talking to SQL and data developers so could be a better option for a presentation.\nI arrived at the office to do my presentation. First of all, I parked next to the factory where all the forklifts were, I was directed to the correct place to park. Then I tried to go to the wrong building, a very helpful employee helped me go to the correct place. I was introduced to the people who would be interviewing me, one of whom I had worked with in the past, who immediately said you should hire this guy, he is an excellent developer! As introductions go, that\u0026rsquo;s not bad!\nIn my opinion the presentation was bad. I didn\u0026rsquo;t have a PowerPoint or similar to go with my talk, so I just waffled on for ten minutes about what I had done a few years back about how I helped get source control being used.\nThe next day I got the job offer. I was very surprised to get the offer as I thought I had done so badly the day before they would give me another positive no. I took a few hours to think about it, however all my other applications were either not started or waiting for the next stage, so I was 99% sure I would accept.\nI had no idea what to expect when I started, as I didn\u0026rsquo;t really ask many questions at the last interview as I was convinced, I was going to get a no after my presentation.\nI worked my notice period and at the start of October I started my new job. My contract only arrived the day before I started, so in the days before I started my mind was making up reasons why the job would disappear.\nThe job started with a two-day induction. Things that were covered on the induction were, mental health, sleep cycles, company values, health and safety and of course a bit of form filling and photocopying passports that you would expect on a first day.\nConclusion Back when I had been job hunting, each time I stressed the importance of team. The company I had joined had an amazing focus on supporting its staff and getting the best from each other. I had got the best possible fit for the kind of team I was looking for.\nUsually when I start a new job, I get a large dose of imposter syndrome. Now, two months after I started, I can say, I didn\u0026rsquo;t really get that this time. Maybe it is how the team functions, maybe it is my level of experience, or maybe its just something else.\nI am happy in my new job. There is a lot to learn, both technology and how to work well in this team. But I am supported, there are people to ask question, I am contributing almost from day one. I have already shared a brief talk about my experience with Blazor, which went down better than I expected. I am excited to learn what is next for me.\nIf you have read all the way to here, Thank you! This is my story and I hope you have found it interesting. If you are looking for a new role, I would encourage you to focus on the soft skills you are looking for next, if you concentrate on team like I did, there is a good chance you will land in an amazing team.\nDon\u0026rsquo;t forget to check out some of the other great content that is being created by all the amazing Festive Calendar 2021 contributors.\n","date":"Dec 5, 2021","img":"https://raw.githubusercontent.com/gsuttie/festivetechcalendar/main/calendar.jpg","permalink":"https://www.funkysi1701.com/posts/lone-developer-to-senior-developer-my-2021-story/","series":null,"tags":["FestiveTechCalendar2021","Career","LifeStory"],"title":"Lone Developer to Senior Developer, my 2021 story"},{"categories":null,"content":"We are over halfway through 2021, let\u0026rsquo;s have a quick look at some things I have done this year:\n  Deployed a new version of the Pre Qualification questionnaire website for my employers\n  Passed Azure Fundamentals Exam\n  Mongo DB Atlas - experimented with using it for auditing my application\n  SonarCloud - tried to improve the quality of my code\n  Azure DevOps API -\u0026gt; built an application to show builds and releases from my Azure DevOps organisation, code is on github\n  Experimented with replacing SQL Agent Jobs with Azure Functions, ended up abandoning this project but was a great learning experience\n  Played with MS Graph - displaying profile pic from Azure AD but I am the only one in my org with a photo so abandoned this as well\n  Added a captch to a website to reduce spam\n  Charted my Gas and Electricity usage using the Octopus Energy API\n  Charted various metrics from services I use, eg twitter followers, github commits, blog posts published\n  Started learning react, easily connected it with Azure AD\n  Integrated my employers project management system with a postcode API and google maps so can see where in the country different projects are located\n  Tested dotnet 6 preview and the latest Visual Studio 2022 preview\n  I was made redundant and got a new job\n  Wrote my first piece of dotnet code to run on a Raspberry Pi\n  Kept cost of Azure down, by removing unused services, refactoring and optimizing existing services.\n  Wow, I have done loads this year, what is next?\n","date":"Aug 31, 2021","img":"","permalink":"https://www.funkysi1701.com/posts/more-than-halfway-through-2021/","series":null,"tags":["Goals"],"title":"More than halfway through 2021"},{"categories":null,"content":"So today I sat the Azure Fundamentals Certification Exam and passed! Really pleased with myself at achieving this. It was one of my goals for 2021 so I can tick that off.\nBack in January I booked my first exam, however due to technical problems I didn\u0026rsquo;t get as far as the Exam. In order to be able to sit an exam like this you need a webcam and microphone so you can be monitored remotely to check that you are not cheating. Something in my network was blocking them from seeing my video so it got cancelled before it started.\nMy theory is that either my internet connection was playing up at the time, My Pi Hole was blocking something, or something else on my network was blocking the video feed.\nThis cancellation put me off Certifications, as I wasn\u0026rsquo;t sure how to debug the issue and find out exactly what the problem was.\nA few months back I won a free Azure Certification for taking part in a skills challenge run by Gregor Suttie and Richard Hooper and this was due to expire at the end of this month so I thought why not book a second exam and see if I can get past the technical problems. I didn\u0026rsquo;t do any exam prep as I thought, I wouldn\u0026rsquo;t get that far.\nThis time I connected directly to my router, bypassing most of the network, firewall and other items on my network that could possibly cause an issue.\nI cleared my desk, took photos of my ID, took pictures of my desk from every orientation, and waited for the technical issues to start.\nI was in a queue waiting for the exam to start.\nA connection issue has occurred you have been sent to the back of the queue. Oh here we go again!\nBut no I connected with the invigilator, who asked to confirm my monitors were disconnected and to move my wallet out of reach off my desk.\nAnd we are off, I am answering questions from the exam.\nI won\u0026rsquo;t go into detail about the questions, but I whizzed through them, most made me think, some I guessed at. I wasn\u0026rsquo;t expecting to pass, I thought maybe half marks or just under due to my familiarity with Azure. (I have been using it for years which must count for something!)\nI was wrong I passed comfortably and now I have my first certification.\nMy advice for you if you have been using Azure for a while is to book this exam and see how you do, you may well pass like me. There are plenty of opportunities to get a free exam, attending conferences like Ignite often qualify you for one, look out for challenges and competitions by #AzureFamily people on twitter as they are very encouraging and helpful in your Certification journey.\nAnd yes I am thinking about what Certificate to do next!\n","date":"May 11, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eytphopn3inx2x77m9n5.png","permalink":"https://www.funkysi1701.com/posts/my-road-to-certification/","series":null,"tags":["Azure","Certification"],"title":"My road to Certification"},{"categories":null,"content":"I have had a Raspberry Pi for a few years and recently I connected it up again, I plugged in the camera and everything worked.\nTo start off you can view photos from the camera with the raspistill command. With a bit of clever scripting and the crontab I got the Pi taking pictures every 60 seconds. Even managed to take a nice picture of a robin.\nHowever scripting isn\u0026rsquo;t really programming, and I would like to write a bit more code. Dotnet can run everywhere these days and it made sense to see if it would run on a Raspberry Pi.\n@pete_codes has written a nice guide to getting started with dotnet on a Raspberry Pi https://www.petecodes.co.uk/install-and-use-microsoft-dot-net-5-with-the-raspberry-pi/ This guide and the nuget package https://www.nuget.org/packages/Unosquare.Raspberry.IO/ was all I needed to get started taking pictures with my Pi.\nMy initial goal is to take some wildlife pictures, stick my camera to a window and take pictures of what flies/crawls/jumps past the window.\nThe code I have written so far is available on github https://github.com/funkysi1701/RaspberryPiDotNet So far the code takes a picture, uploads this file to Azure Blob Storage (so as not to fill up the Pi with too many image files) and deletes the image locally.\nRun a dotnet publish -c Release and then cron can run dotnet RaspberryPiDotNet.dll (with full paths to the relevant files)\nI then use crontab to execute the code every 60 seconds.\nMy code has an appsetting.json file which has a couple of settings that need completing for my code to work.\nStorage: This is the connection string for Azure Blob Storage LocalPath: This is the path to where the camera will save its photos to, something like /home/pi/ is all you need but feel free to specify what you need.\nOnce the photos are in blob storage I plan to display them somewhere, add options to delete what I don\u0026rsquo;t want, maybe do something timelapsey.\nI also don\u0026rsquo;t have a proper release pipeline and this grates on me a bit. I have been doing a mixture of writing code in VS and pushing that to github and then doing git pull on the Pi, and also writing code directly on the Pi. (VS Code can connect via SSH which is pretty cool!)\n","date":"May 10, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/e21z7vbamy6w6akhhiwd.jpg","permalink":"https://www.funkysi1701.com/posts/dotnet-on-a-raspberry-pi/","series":null,"tags":["RaspberryPi","Programming","DotNet"],"title":"DotNet on a Raspberry Pi"},{"categories":null,"content":"Back when I was a kid, I used to record our weekly gas and electricity meter readings in a little notebook. We then typed these reading into a spreadsheet (this was in the pre-Excel days), which allowed plotting as a line graph.\nHow would I go about doing a similar thing today? First off, I have a smart meter that submits meter readings every 30 minutes or so. However, I do not know anyway to get access to these readings directly, short of manually recording them like I did 30 years ago!\nOctopus Energy have a public API which allows you to pull your consumption readings. The smart meter sends your usage to your energy supplier, in my case Octopus, they then process these readings and allow them to be queried with an API they have created. It is not a direct connection to your data, but it is the next best thing.\nOther energy suppliers will hopefully follow this example and allow users access to their consumption data.\nHow do I use the API? Using the API is straight forward. Octopus supply you with a secret which you use to authenticate against the API with Basic Auth, no password just a username. Then you just need to pass some details of your meters to get an object containing the last few days meter readings.\nAPI Docs  GET /v1/electricity-meter-points/{mpan}/meters/{serial_number}/consumption/ GET /v1/gas-meter-points/{mprn}/meters/{serial_number}/consumption/  {mpan}/{mprn} of your gas or electricity meter, and {serial_number} is the serial number of the meters.\nSomething to be aware of, I initially collected the last days consumption, which worked, however on one day I encountered a gap in the data for electricity. So, I changed to collect and store the last month\u0026rsquo;s data. I can then query this for what I need.\nUsually, the last 24 hours of data is available after midnight of that day. e.g. at midnight 2nd March all the data for 1st March should be available. This is not guaranteed so don\u0026rsquo;t rely on it, however I see no problem with having a few days delay between charting your usage.\nI am still testing this out but so far, I have three charts for gas (and the same for electricity), the first chart covers a 24-hour period, the next covers a day total over 2 weeks, the final chart covers a total for each month (as I write this I have less than a month\u0026rsquo;s worth of data!)\nFor the day and 2 weeks charts, I plot a comparison line of the previous period so you can easily compare the current and previous usage. From my limited testing I have already discovered my usage is very similar day to day.\nAnother point of interest is that gas consumption is in m^3 and electricity is in kW/h. If you are interested in trying the Octopus Energy API, here is a referral link .\n","date":"Mar 7, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/63ep8hp6ipyn2l4emiop.png","permalink":"https://www.funkysi1701.com/posts/charting-my-energy-usage-with-the-octopus-energy-api/","series":null,"tags":["API","OctopusEnergy"],"title":"Charting my Energy usage with the Octopus Energy API"},{"categories":null,"content":"Azure DevOps release pipelines have lots of options to do things how you want. One of my favourites is the option for approval.\nThere are two ways you can do approvals Pre and Post deployment. Lets look at both.\nPre Deployment Approval Lets imagine you have a simple deployment pipeline that deploys to a test/development environment before deploying to a production environment.\nPre Deployment Approval happens immediately before the release so in this example, click in the ellipse before the Prod release step.\nYou will get a screen like the above, you can select what users need to approve it and how long approval waits before timing out, the default is 30 days, but I tend to use a shorter time out of 3 days.\nPost Deployment Approval Post Deployment Approval happens immediately after the release so in this example, click in the circle after the Test release step.\nYou will get a screen like the above, with the same settings as before.\nThat is pretty much all there is to approvals so either option will prompt you to approve before anything gets deployed to your production environment.\nDeployment Hours To complicate matters I make use of the following setting to define deployment hours. This setting will start the Prod deployment at 3am Mon-Fri.\nIf I configure Post Deployment Approval, as soon as my deploy to Test has completed a request for Approval is sent.\nIf I configure Pre Deployment Approval, at 3am Mon-Fri a request for Approval is sent (not ideal if you tend to be asleep at 3am)\nSo it looks like Post Deployment Approval is more useful for my use case. However if you deny approval either in Pre or Post approval this will mark the deployment as failed and show Red in your list of deployments.\nFrom a casual glance it looks like the deployment to Test is failing, it isn\u0026rsquo;t I am just opting to not continue my deployment to production.\nMy Pipeline This is how I have my pipeline setup. Deployment happens on Test and doesn\u0026rsquo;t have a post approval step.\nAfter Test an empty stage called Approval runs and that has a post deployment approval, this happens immediately after Test so you get asked straight away for approval.\nProd does not start as I have my deployment hours configured. Once it is time for deployment to Prod to start it executes.\nNow a casual look at my past releases, you can easily see which have been stopped by approval and which have failed due to whatever issue, and which have run all the way through to Prod.\nAnd deployments to Prod can only ever run during my defined deployment window.\nI am interested to hear how you have your deployment pipeline setup. Do you make use of Pre or Post Approvals? Do you ensure deployments always happen at specific times?\n","date":"Feb 14, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/i/9k6vo6pfv434u7yq3mt4.png","permalink":"https://www.funkysi1701.com/posts/azure-devops-release-pipelines-pre-and-post-approval/","series":null,"tags":["AzureDevOps","DevOps","Azure"],"title":"Azure DevOps Release Pipelines Pre and Post Approval"},{"categories":null,"content":"I didn\u0026rsquo;t make any goals for 2020, or if I did, I didn\u0026rsquo;t officially announce them. 2020 has been a hard year for all of us, 2021 is going to be better.\nHere are a few ideas for my goals for the year ahead.\n Azure certification Mentoring F# Cosmos db/Mongo db Give a talk Mandlebrot Generator Pwned Pass Mobile App More time for me    I want to get a certificate to show how much I know. The obvious area for this is Azure. I spend a lot of time playing with Azure, building and deploying to the platform I should be able to get certified in this area. I actually had an exam booked in 2020 but it was an in-person exam so was cancelled when Covid hit. I plan to sit the foundation exam in the first quarter of 2021 and I will take it from there.\n  I have had a few times recently where I have been reminded that I don\u0026rsquo;t have much experience working with others. Mostly because I work in a one-man development team so there are limited opportunities in the workplace. I have a few ideas to change this, but this is one of my top priorities for 2021.\n  \u0026amp; 4) My experience is very Microsoft and C# so I want to spend some time exploring and learning tech that is adjacent to this. A functional language like F# sounds like a good compliment to my existing skills and my data skills are also very SQL server so some document database skills would be a good place to spend some time.\n  Doing a talk has been on my list for years. I am terrified of doing one and with everything online now I don\u0026rsquo;t know if that makes things easier or harder. I don\u0026rsquo;t think 2021 is going to be the year for this but maybe I will take baby steps towards this goal.\n  Twitter recently reminded me about watching a Mandlebrot generate one pixel at a time in the 1980s. I would love to explore the code used to generate them and how fast they currently are to produce.\n  My Google play store Xamarin forms app that uses the HIBP API has less than 100 users and I am considering closing it down, especially as I pay a monthly fee for API keys. I haven\u0026rsquo;t really decided what to do, maybe I will spend some time improving it, maybe I will close it down, maybe I will build something else.\n  For many people 2020 has been a hard year, I am one of those\n  ","date":"Dec 28, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/goals-for-2021/","series":null,"tags":["Goals"],"title":"Goals for 2021"},{"categories":null,"content":"I saw a tweet about building a twitter clone being harder than you would think. So this of course started me thinking how I would go about building something like that.\nOk so where would I start? First a few assumptions.\n Development by a lone developer ie me Tech stack will be dotnet and other tech I am familiar with Database backend, probably SQL Server but I might use table storage for cost reasons should I try and actually build this. However if I design this well this should be something that could be swapped out as the system grows User accounts on the system will be small as I can\u0026rsquo;t imagine anyone ever signing up. Why sign up to a social media platform with no users?  I guess the next question is what is Twitter? A website that allows you to share 280 characters of text with your followers, allows you to follow other users updates and allows other user to follow your updates.\nIt also has an API that allow you to do almost everything that you can with the website.\nThen there are of course mobile apps to consider but I am going to assume this is out of scope, however assuming a good enough API then this shouldn\u0026rsquo;t be a problem for future development.\nFirst Steps To start off with I would concentrate on the API, and then build a web client that makes use of the API.\nSo what would my MVP (minimum viable product) be?\n User can authenticate with my API to get a token which allows access to other API endpoints User can create a tweet User can view own tweets User can view tweets of another user User can view tweets in their timeline User can follow/unfollow other users User can search for other users User can search for keywords in tweets  I think that is probably sufficient to build my MVP for the API.\nAn interesting side note is that I could use the OAuth Twitter authentication to allow users to login to my twitter clone with the real twitter login details. However this makes no sense to me as we are essentially adding a dependency on the real twitter.\nSo what would I use for the frontend? I would start off with a Client Side Blazor frontend. Once I had a proof of concept that worked, I would think about styling and adding the UI elements that are familiar to twitter users.\nThe beauty of Client Side Blazor is that I can host cheaply in azure storage and distribute around the world via a CDN.\nDue to the high number of times that follower and following count and other stats are queried I would consider storing these in the database and include a regular job to recalculate them so they don\u0026rsquo;t get out of sync with the data.\nHaving said all of this I am very tempted to fire up Visual Studio and see how far I get, and what problems I encounter along the way.\n","date":"Dec 22, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/building-a-twitter-clone/","series":null,"tags":["Twitter","Design","Architecture"],"title":"Building a Twitter Clone"},{"categories":null,"content":"It has been a bit of a mad week this week. I joined a new team so lots of time learning what\u0026rsquo;s what and also being pulled in two directions as usual demands come through on top of that.\nMy blog runs on Blazor and I have been making use of JavaScript interop to update the html headers and update the page title to match the blog post article. This works great, I load the page and check the headers and they were saying what I wanted.\nThe problem was I wanted to add tags for twitter cards This means that when I paste a link to my blog on twitter you get a nice preview and pic of me in the tweet. This was not working at all even though I had the correct headers.\nI eventually figured out that the problem was the fact I was using JavaScript to update my headers after the page had been initially loaded. Twitter was fetching my page before these headers got added and therefore couldn\u0026rsquo;t see the twitter card headers.\nMy solution was to use invalid html. Not ideal but it works. I added the required html tags in the body of my page using Blazor/C# instead of using JavaScript to add them into the header. Twitter appears to not be fussy in finding them in the wrong place.\nTwitter provides a validator tool at Twitter Card Validator which my website now passes.\nNot much else to say this week, apart from I am missing Visual Studio and C#, I have been mostly using VS Code on Linux and looking at php which isn\u0026rsquo;t as much fun as my usual day job.\n","date":"Dec 12, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/005-twitter-cards/","series":null,"tags":["Twitter"],"title":"#005: Twitter Cards"},{"categories":null,"content":"I use sp_send_dbmail to send results of sql queries by email to business users. Recently an issue was raised that data was being cut off after 255 characters. To fix this I added @query_no_truncate = 1, however this stopped the column headings from being included. No idea why you can\u0026rsquo;t have all the data and column headings but there you have it.\nWhat I am doing now is running 2 queries, one to get the headings, and one to get the data. In theory you should be able to combine them with a Union however you then have datatype issues for non text columns so I gave up with that idea.\nMy results have 60 something columns (don\u0026rsquo;t ask its for a data import into a third party system!) so I am not typing them all out. I can shove query results into a temporary table and then execute to get a list of columns.\nSELECT name FROM tempdb.sys.columns WHERE object_id = object_id(\u0026#39;tempdb..#TempTable\u0026#39;) However I need my list to be horizontal so I can use as column headers. I can use dynamic SQL and a pivot to flip them round.\nDECLARE @cols AS NVARCHAR(MAX), @query AS NVARCHAR(MAX)  SELECT @cols = STUFF((SELECT \u0026#39;,\u0026#39; + QUOTENAME(name)  FROM tempdb.sys.columns  WHERE object_id = object_id(\u0026#39;tempdb..#TempTable\u0026#39;)  FOR XML PATH(\u0026#39;\u0026#39;), TYPE).value(\u0026#39;.\u0026#39;, \u0026#39;NVARCHAR(MAX)\u0026#39;),1,1,\u0026#39;\u0026#39;)  SET @query = N\u0026#39;SELECT \u0026#39; + @cols + N\u0026#39; FROM ( SELECT name FROM tempdb.sys.columns WHERE object_id = object_id(\u0026#39;\u0026#39;tempdb..#TempTable\u0026#39;\u0026#39;) ) x PIVOT ( MAX(name) FOR name IN (\u0026#39; + @cols + N\u0026#39;) ) y\u0026#39; ","date":"Dec 6, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/weekly-update-004/","series":null,"tags":["SQL"],"title":"Weekly Update #004"},{"categories":null,"content":"Been a quiet week, so wasn\u0026rsquo;t expecting to have much to write on here, however a few things happened worth talking about.\nMy First PR A comment was made to me to do something with the postcodes that are in the system I am developing. Find out what projects have postcodes near each other, and that way work can be grouped together and reduce potential mileage costs of staff that need to visit these projects.\nA quick google search found https://postcodes.io/ which has an API that returns nearby postcodes. It also has a C# wrapper https://github.com/markembling/MarkEmbling.PostcodesIO A comparison of what was being returned from the wrapper and what the API said should be returned revealed that the distance between postcodes wasn\u0026rsquo;t being returned.\nAs the code was on GitHub I could easily see how easy or difficult it might be to add the missing bit of information. It was easy! So, I forked the repo and made the change. I published the change to a private NuGet repo in my Azure Dev Ops account. That way I could try my revised package to check it did what I wanted.\nI left a message on the GitHub project letting the owner know I had a potential fix for an issue. The project hadn\u0026rsquo;t been updated in over a year, so the owner may not be interested, or the project may have been abandoned.\nI was in luck just 17 hours after I left a message the project owner said to create a Pull Request, which I did and shortly afterwards my code had been merged in and an updated version of the package existed in the public NuGet feed.\nI have been thinking about contributing to open source for a while. However, I had not seen a project I wanted to contribute to, or a problem that I knew how to fix until now that is.\nPHP This week I had a call asking me if I knew PHP?\nI did, over ten years ago, before I got my first IT job, I spent time learning PHP and MySQL. I created a blog, and I also created a website for my Dad\u0026rsquo;s camera club. The code I created back then was awful. No shared code, all the code was associated with the page, or sorts of bugs occurred and as time went by it became increasingly hard to update. The site was well liked but I eventually lost interest and moved on to learn other things.\nThis call led to me talking with the head of IT, and later a couple of the developers who have since granted me access to the codebase of a project.\nI haven\u0026rsquo;t had time to spend a lot of time looking at the code so far, however this is nothing like the PHP I had built before.\nThe project makes use of the laravel framework and the first file I opened had methods and classes, so apart from the syntax you could think you were looking at C#.\nAnother thing that interested me was the project used docker containers, it has automated builds as well. Lots of modern programming ideas that I had some ideas about. I am looking forward to learning more about this project and how I might contribute to it.\n","date":"Nov 28, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/weekly-update-003/","series":null,"tags":["OpenSource","PHP"],"title":"Weekly Update #003"},{"categories":null,"content":"I know Active Directory is fussy about clocks being in sync however not sure how todays issue happened.\nI run my docker compose file from Visual Studio and I get a weird error.\nSecurityTokenNotYetValidException: IDX10222: Lifetime validation failed. The token is not yet valid. ValidFrom: \u0026#39;System.DateTime\u0026#39;, Current time: \u0026#39;System.DateTime\u0026#39;. I deleted my containers, open and close Visual Studio a few times, nothing helps. Eventually I think to find out what the time is on my container. It has yesterday\u0026rsquo;s date. What has happened here? Surely recreating containers would have caused them to have todays date? I reboot and everything is fine again.\nTurns out that it is a know issue, see https://thorsten-hans.com/docker-on-windows-fix-time-synchronization-issue I am using WSL2 and I have now changed back to using Hyper-V and the issue hasn\u0026rsquo;t come back.\nEarlier in the week I spotted my build step was failing.\n - task: NuGetToolInstaller@0 Swapping to the next version of the step is all I needed to do to fix it.\n - task: NuGetToolInstaller@1 My guess is that support was dropped for this earlier version or there is some other incompatability with .Net 5.\n","date":"Nov 21, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/weekly-update-002/","series":null,"tags":["Docker","Azure"],"title":"Weekly Update #002"},{"categories":null,"content":"One of my favourite podcasts is Troy Hunts weekly update. In it he discusses stuff that he has been working on, plus some personal stuff. I am going to attempt to do something similar. It will probably take me a few of these before we get a look and feel that works.\nMonday A week off work, mainly to use it up before year end, plus want to get a few jobs around the house done.\nI did ask the following question on Twitter.\nHey #azurefamily and #dotnet developers how do I get more involved in mentoring?\n\u0026mdash; Simon Foster (@funkysi1701) November 9, 2020  As a one person dev team, my biggest weakness is working with others so any ideas of how to change that are great.\nTuesday Dotnet 5 is out! The latest version of dotnet is released by Microsoft and to celebrate there is dotnetconf to listen to. Due to time zones and family commitments, I haven\u0026rsquo;t listened to an awful lot of it but I did see the keynote and loved the 3 Scott\u0026rsquo;s chat.\nWednesday My youngest son was 3 today, due to Coronavirus we didn\u0026rsquo;t do much but we celebrated as a family, and he even had a zoom call.\nThursday Blazor has a new feature Virtualize where a list of items can only load the ones on screen. I have been trying to get this to work on my blog, works great running locally but not working in production yet.\nThink I know what might be happening. I use Cloudflare to do my SSL, as Custom SSL certs for the cheaper Azure Web Apps is not supported. Something in Cloudflare is caching or interfering.\nhttps://zimmergren.net/solved-asp-net-core-blazor-web-sites-does-not-work-with-cloudflare-html-minification/ Turning off HTML minification fixed my issue!\nOne additional thing I added to my Blog is the /config page which details some of the config settings. I think this probably came from https://www.hanselman.com/blog/adding-a-git-commit-hash-and-azure-devops-build-number-and-build-id-to-an-aspnet-website but it was a while ago when I first did this on another project.\nAt the moment we have .net Version, Commit and Build links.\nThe .Net Version is obtained from\n@System.Runtime.InteropServices.RuntimeInformation.FrameworkDescription A few other bits of info can be obtained from System.Runtime.InteropServices.RuntimeInformation which I have included on the page for fun. There are probably security concerns with exposing all this info publicly so something to bear in mind if you try this.\nBuild Info is passed to my code by a build step\n- script: \u0026#39;(echo $(Build.BuildNumber) \u0026amp;\u0026amp; echo $(Build.BuildId)) \u0026gt; .buildinfo.json\u0026#39;  displayName: \u0026#34;Emit build number\u0026#34;  workingDirectory: \u0026#39;$(Build.SourcesDirectory)/src/WebBlog\u0026#39;  failOnStderr: true This simply passed the build id and number which are stored as variabled and saves them in a text file.\nI then have a class that reads them and constructs a link.\nusing Microsoft.Extensions.Hosting; using System; using System.IO; using System.Linq; using System.Reflection;  namespace WebBlog {  public class AppVersionInfo  {  private const string _buildFileName = \u0026#34;.buildinfo.json\u0026#34;;  private readonly string _buildFilePath;  private string _buildNumber = string.Empty;  private string _buildId = string.Empty;  private string _gitHash = string.Empty;  private string _gitShortHash = string.Empty;   public AppVersionInfo(IHostEnvironment hostEnvironment)  {  _buildFilePath = Path.Combine(hostEnvironment.ContentRootPath, _buildFileName);  }   public string BuildNumber  {  get  {  if (string.IsNullOrEmpty(_buildNumber))  {  if (File.Exists(_buildFilePath))  {  var fileContents = File.ReadLines(_buildFilePath).ToList();   if (fileContents.Count \u0026gt; 0)  {  _buildNumber = fileContents[0];  }  if (fileContents.Count \u0026gt; 1)  {  _buildId = fileContents[1];  }  }   if (string.IsNullOrEmpty(_buildNumber))  {  _buildNumber = DateTime.UtcNow.ToString(\u0026#34;yyyyMMdd\u0026#34;) + \u0026#34;.0\u0026#34;;  }   if (string.IsNullOrEmpty(_buildId))  {  _buildId = \u0026#34;123456\u0026#34;;  }  }   return _buildNumber;  }  }   public string BuildId  {  get  {  if (string.IsNullOrEmpty(_buildId))  {  var _ = BuildNumber;  }   return _buildId;  }  }   public string GitHash  {  get  {  if (string.IsNullOrEmpty(_gitHash))  {  var version = \u0026#34;1.0.0+LOCALBUILD\u0026#34;;  var appAssembly = typeof(AppVersionInfo).Assembly;  var infoVerAttr = (AssemblyInformationalVersionAttribute)appAssembly  .GetCustomAttributes(typeof(AssemblyInformationalVersionAttribute)).FirstOrDefault();   if (infoVerAttr != null \u0026amp;\u0026amp; infoVerAttr.InformationalVersion.Length \u0026gt; 6)  {  version = infoVerAttr.InformationalVersion;  }  _gitHash = version[(version.IndexOf(\u0026#39;+\u0026#39;) + 1)..];  }   return _gitHash;  }  }   public string ShortGitHash  {  get  {  if (string.IsNullOrEmpty(_gitShortHash))  {  _gitShortHash = GitHash.Substring(GitHash.Length - 6, 6);  }  return _gitShortHash;  }  }  } } The BuildId and BuildNumber properties just fetch the details saved into the text file during the build. This can then be passed to build the build link.\n\u0026lt;a href=\u0026#34;https://dev.azure.com/{OrgName}/{RepoName}/_build/results?buildId=@appInfo.BuildId\u0026amp;view=results\u0026#34;\u0026gt;  @appInfo.BuildNumber \u0026lt;/a\u0026gt; Finally, the GitHash properties need to fetch the hash and shorthash of the commit which is a bit more complex. This is achieved using the following line in your build.\n- task: DotNetCoreCLI@2  displayName: \u0026#39;Publish\u0026#39;  inputs:  command: \u0026#39;publish\u0026#39;  publishWebProjects: true  arguments: \u0026#39;--output $(Build.ArtifactStagingDirectory) /p:SourceRevisionId=$(Build.SourceVersion)\u0026#39; /p:SourceRevisionId=$(Build.SourceVersion) add the revision hash to [assembly: AssemblyInformationalVersion] during the build which can then be extracted using the gitHash property above, before being passed into the commit link.\n\u0026lt;a href=\u0026#34;https://github.com/{OrgName}/{RepoName}/commit/@appInfo.GitHash\u0026#34;\u0026gt;  @appInfo.ShortGitHash \u0026lt;/a\u0026gt; ","date":"Nov 14, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/weekly-update-001/","series":null,"tags":["DotNet"],"title":"Weekly Update #001"},{"categories":null,"content":"Have you wondered what info you are leaking via your response headers?, do you want some kind of guide about what headers to set or remove altogether?\nHead on over to https://securityheaders.com/ This is a site created by security expert Scott Helme that rates a URL based on what response headers it can see.\nI am pleased to say www.funkysi1701.com is now getting an A.So how do you add/remove headers in dotnet core?\nIn my configure method in Startup.cs I have the following code block.\napp.Use(  next =\u0026gt;  {  return async context =\u0026gt;  {  context.Response.OnStarting(  () =\u0026gt;  {  context.Response.Headers.Add(\u0026#34;Permissions-Policy\u0026#34;, \u0026#34;microphone=()\u0026#34;);  context.Response.Headers.Remove(\u0026#34;Server\u0026#34;);  context.Response.Headers.Remove(\u0026#34;X-Powered-By\u0026#34;);  context.Response.Headers.Remove(\u0026#34;X-AspNet-Version\u0026#34;);  return Task.CompletedTask;  });   await next(context);  };  }); I have only included a few of the headers I am adding as the excellent https://securityheaders.com/ can tell you which headers you should add and what options you might want.\n","date":"Sep 26, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/security-headers/","series":null,"tags":["DotNet","Security"],"title":"Security Headers"},{"categories":null,"content":"My last blog post was over six months ago.\nCovid 19 has hit the world, and I will be honest I have found it a challenging time.\nMy Blog had gotten into a bit of a mess. It had become fragmented with different versions of the same thing; I will attempt to explain what has become of my blog.\nThe original WordPress site can currently be found at https://www.pwnedpass.com/ I would prefer it to be on a sub-domain of funkysi1701.com but for some reason I haven\u0026rsquo;t been able to get that to work, not sure if it is a limitation of my hosting package. I like WordPress, it is very flexible, easy to get blog posts out there. But I want to write content about development and having a site I can tinker with is important to me.\nMost of my WordPress blogs have been imported into dev.to and a few extra have been written on this platform. I like dev.to it is a wonderful place to share content and it has one or two extra features I like.\ndev.to has an integration with Stackbit/Netlify and this became https://dev.funkysi1701.com . I like having a personal site, but having the same content as dev.to. To add content to this site all I need to do is write it on dev.to and some magic will go on behind the scenes and new content will be published.\nHowever, as a developer I don\u0026rsquo;t like magic, I want to understand what is going on a fiddle with all the settings and make it do what I want.\ndev.to has an API, I can build a site in .Net Core and make API calls to fetch the content I want. I understand APIs, I understand .Net and can customise my site exactly how I want it, plus play about with a .net website. This is what https://www.funkysi1701.com is now.\nSo what have I built so far. I have a Server Side Blazor site running .Net 5. Why Server Side and not Client Side I hear you ask? Well only because I have more experience with Server Side and know how to quickly create a website with that technology, I may change it as time goes by, but we will see.\nI have two pages a list of my blog posts and a page that displays the content. Both of these use the dev.to API. I lied, there is a third page I hacked together to do some page redirection from the WordPress URLs. This is something I will change as time goes on.\nThere are lots of improvements I want to do, there are probably also lots of broken images or links as well. Hopefully, this will result in a good platform to blog about as well as on.\n","date":"Sep 25, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/back-to-blogging/","series":null,"tags":["Blogging","Blazor","DotNet"],"title":"Back to Blogging"},{"categories":null,"content":"Let\u0026rsquo;s Encrypt is amazing, you can easily add SSL certificates to any website and automate the renewal process. I have talked before about how impressive it is.\nOnce you start adding SSL certificates to your production sites however you may want to check when they expire so you don\u0026rsquo;t get caught out. You can always open your site in your favourite browser and view the certificate information and expiry date.\nHowever there is a way to automate this check.\n[Fact] public void IsSSLExpiring() {  var handler = new HttpClientHandler  {  ServerCertificateCustomValidationCallback = CustomCallback  };  var client = new HttpClient(handler);   HttpResponseMessage response = client.GetAsync(\u0026#34;https://www.example.com\u0026#34;).GetAwaiter().GetResult();  Assert.True(response.IsSuccessStatusCode); }  private bool CustomCallback(HttpRequestMessage arg1, X509Certificate2 arg2, X509Chain arg3, SslPolicyErrors arg4) {  var now = DateTime.UtcNow;  var expire = arg2.NotAfter;  var diff = (expire - now).TotalDays;   Assert.InRange(diff, 30, 1000);  return arg4 == SslPolicyErrors.None; } This code gets the SSL expiry date from https://www.example.com and will fail the xunit test if the expiry date is less than 30 days in the future. I then schedule my tests to run regularly on all my environments with a Let\u0026rsquo;s Encrypt Certificate and this gives me advanced warning if a SSL certificate is about to expire.\nThe Assert.InRange(diff, 30, 1000) line will fail the test if the expiry date is less than 30 days or greater than 1000, but as the default expiry for Let\u0026rsquo;s Encrypt certificates is three months it will never be greater than 1000 days even with a freshly installed certificate. These values can be tweaked to suit your use case, however 30 days is enough time for me to investigate what is happening.\nTo execute my tests I use a scheduled build in Azure DevOps, but anything that regularly can run your tests will do the job.\nThe code above is just a simple example to get your started for my purposes I have put all my URLs into config files and just pass these into my tests, so I don\u0026rsquo;t need a custom test for every different URL.\n","date":"Mar 3, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/testing-for-expiring-ssl-certificates/","series":null,"tags":["Security","SSL","Testing"],"title":"Testing for expiring SSL Certificates"},{"categories":null,"content":"Automated releases of software are great but how can we add an element of feedback so only good releases go live.\nI have been using Azure DevOps to release my PwnedPass android app to the Google Play Store for a while now. There are options to deploy to the alpha, Beta or Production tracks and even to set % of users to target. For the full range of options check out the Google Play extension for Azure DevOps.\nMy release starts by publishing to 10% of users on the production track, my next step makes use of the increase rollout option to increase this %, you can have as many of these additional steps as you want until you reach 100% of your users.\nNow if you run this release now it will just run through each of the steps one after the other. Now of course you can add a pre or post approval to your pipeline but this just adds a manual dependency to your release. Whoever does the approving needs to check things are working before approving or worse just approves regardless.\nAzure DevOps has the concept of gated releases which allows you to add automated checks before or after a release happens. These automated checks can be any of the following:\n An Azure Function A Rest API call Azure Monitor Alert Query Work Items Security and Compliance Assessment  We are going to make use of the Azure Monitor Alert, to create an alert from your Application Insights data and only continue the rollout if no failures are detected.\nOpen up your application insights resource in the Azure portal and look in alerts. Click add new alert rule.\nSelect your application insights resource in Resource, In Condition choose a condition to check, I chose Failed Requests, so every time a failure is registered in my API I can stop the deployment. The exact criteria you want to use is entirely up to you.\nCreate an action group, I just set my alert to send an email to myself but there are other alert actions you may want to try. Give your alert a name and description and click save.\nNow all we need to do is make Azure DevOps make use of this alert. In your release pipeline select the pre-deployment conditions of your second step and open up the Gates section.\nChoose a suitable time to evaluate, I have been using something long like 12 or 24 hours so if there are problems there is time for it to be noticed. Choose Version 1 of the task (I was not able to get it to work with Version 0)\nNow select your Azure subscription and Resource Group and leave the rest of the settings as they are. Now your Deployment will stop and analyse application insights for any Failed requests and will halt if it finds any.\nI am still testing this out but it will take a few days to figure out if this what I want due to the large time scales involved. I feel this is going to be an improvement of manually approving release steps.\n","date":"Apr 5, 2019","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/04/image.png?fit=662%2C116\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/gated-release/","series":null,"tags":["Azure","API","ApplicationInsights","DevOps"],"title":"Gated Release"},{"categories":null,"content":"Its been a while since I first released Pwned Pass so lets have a look at where we are now.\nWe are very close to 500 Downloads from Google Play and we have recently smashed past 100 active installs, peaking at 116 and even now we are still over 100. I have had 9 reviews (6 x 5*, 2 x 1 * and a 4 *) which averages out at 4 * Over Christmas I released a UWP version that can be found in the Microsoft Store . This has currently had 9 downloads and even had a download to windows mobile (someone out there still likes the platform!) I have a fairly smooth deployment process using Azure DevOps. After every check in of code a build runs which compiles the UWP and Android versions. The build also increments the version numbers that is required to deploy to either of the app stores.\nEvery successful build of the master branch will kick off a release to the Beta track of Google Play. If I am happy I then release to 10% of the Production track, which can then be increased to 100% (or halted). The release to Microsoft Store happens after the Beta track of Google Play. Only reason for this order is that there isnâ€™t a beta area for UWP apps so I want to quickly test change on android before rolling out for windows.\nAll these steps require confirmation by me before proceeding and often donâ€™t get further than the beta track.\nA further development is that I have open sourced the source code to github do take a look if you are curious or want to contribute. With the purchase by Microsoft there are easy ways to connect github repositories to Azure DevOps. Once I create a Pull Request in github it creates a build in Azure DevOps and all the build and release steps can happen.\nI am still not 100% sure if I want to keep my bug and issue tracking in github or Azure DevOps as both have features for doing so.\nOne future improvement I want to make is to automate the creation of screenshots. When I create a new feature and it gets checked in. I would like to automatically created screenshots of the key pages and submit them to the different app stores. Currently I am not sure if this is possible or how to go about it. I have some ideas to experiment with so we will see what I can do.\n","date":"Jan 23, 2019","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/01/image.png?w=662\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/pwned-pass-update/","series":null,"tags":["Xamarin","Android"],"title":"Pwned Pass Update"},{"categories":null,"content":"This is my annual lets make some goals for the new year blog post. So in no particular order.\nImprove House I canâ€™t outdo last years goal of buying a house but I can continue to make efforts to improve it. I was going to put something about decorating or doing something to our bedrooms, however today I spotted a hole in the roof so this may well absorb most of my home improvement budget.\nConnect with local area Since we moved in October we havenâ€™t really connected with our local area yet. My only interaction with my neighbours was when my car blocked his drive!\nI am going to make efforts to change this in 2019. I donâ€™t exactly know how yet, but I have some ideas. My son starts nursery and later school this year so could be one way to meet other parents and teachers. I want to connect with a local church, so far this hasnâ€™t happened so need to make more effort with this. There are other local groups that put on child friendly events which would be good to attend.\nLightning Talk I failed last year to do this so it is back on my list. I know I must have stuff of value to share so going to try again on this.\nA lightning talk is a very short talk or presentation given at a conference or user group. I am not a natural public speaker so this is a step out of my comfort zone, however I am going to start small and see what happens.\n##Family Holiday As always we Fosters are going to have a holiday in the summer and have some quality family time. I also want to take the boys to London for a weekend.\nBlogging This blog has taken a bit of a backseat in 2018 so I want to try and refocus it for 2019. My commitment is for 12 blog posts in 2019, that is one a month. It doesnâ€™t sound like much but I know how easily life can get in the way.\nHaving said that my log has had record views this year. 7159 views. However I believe this number to be incorrect and has been inflated by my app development and some of the automated processes taking place.\nRoutine Last goal is to try a get more of a routine going. With two small boys, one of which is starting school in September a routine is going to be essential and will allow all of us all a chance to get more things done.\n","date":"Jan 1, 2019","img":"","permalink":"https://www.funkysi1701.com/posts/lets-see-what-2019-can-do/","series":null,"tags":["Goals",""],"title":"Lets see what 2019 can do!"},{"categories":null,"content":"Writing SQL queries is typically done with SQL Management Studio (SSMS). However this tool is a bit of a beast so letâ€™s look at how you could use Visual Studio Code instead.\nVisual Studio Code is a free text editor but it is so much more than just a text editor. VS Code can be downloaded from https://code.visualstudio.com/Download To work with SQL Server download the mssql extension. Press CTRL+SHIFT+P and then Select Install Extension and type mssql.\nIntellisense in Visual Studio Code is brilliant, better than SSMS. Lets look at how to get it all set up.\nCreate a new file and set the language type to SQL (Press CTRL+K,M )\nOpen the command palette, CTRL+SHIFT+P and type SQL to show the mssql commands. Select the Connect command.\nThen select Create Connection Profile , this creates a profile to connect with your SQL Server. Follow the prompts to get it all setup.\nLook in the bottom right corner of the status bar and you should see you are connected.\nNow if you type sql you will see a long list of SQL code snippets that you could use.\nChoose a snippet to create, and edit it as required. When you are happy press **CTRL+SHIFT+E ** to execute.\nThis is basically all there is to it. However this is an incredibly powerful way of working, the intellisense instantly tells you what database objects you can use in your query and there is a wealth of different snippets you can use.\nWhen returning data you get a similar view to SSMS but you can save as Excel, CSV or JSON.\nSSMS is a very graphical way of doing things, you can double click a table and see its columns or indexes. VS Code relies on TSQL commands but you have access to exactly the same information.\nFor more information about VS Code and the mssql extension check out https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-develop-use-vscode ","date":"Nov 6, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/sql-with-visual-studio-code-1k3psql-with-visual-studio-code/","series":null,"tags":["Code","Database","SQL","Programming"],"title":"SQL with Visual Studio Code"},{"categories":null,"content":"I tried to resist but I am going to have to write about the new Star Trek series Discovery. Warning this post is going to include SPOILERS. If you read on you have been warned.\nI watched the first two episodes of the brand new Star Trek TV show. As I am an international viewer I used Netflix, if you are from the US you need to use CBS All Access. I have heard a lot of complaining that the show is not free to watch. CBS is making Star Trek Discovery to make money, if they donâ€™t make money they will stop making it. Its as simple as that.\nNetflix costs Â£5.99 per month and CBS All Access costs $5.99 per month. I donâ€™t believe this is a lot of money. The 5.99 above allows you to watch any of the 726 Star Trek episodes at any time of day whenever you want as many times as you want, plus you have access to all the other movies or TV shows available. I donâ€™t think this is too much to ask.\nBack to Discovery. I loved it! It felt like Star Trek. I had feared it might only share the name and would be an action filled TV show that had little in common with what he had seen before. I was wrong this is definitely a show that can proudly call itself Star Trek.\nThe Klingons\nWhen it was first announced that the Klingons would feature in the new show I was a bit â€œmehâ€. We had done lots with the Klingons before and they had never been my favourite alien race. The look of the Klingons was also going to be changed, I have to admit this didnâ€™t bother me. Klingons have had their look updated before. In 1966 Klingons had dark faces and smooth foreheads, in 1979 the forehead ridges were added as the first Star Trek movie was made, and now Discovery has removed all hair from Klingons. I can explain it away as productional changes, I donâ€™t need an on screen explanation like we had on Enterprise (or even DS9).\nWhat I have seen so far is a menacing alien race that fits with what we have seen before with lots of references to honor, Kahless and speaking Klingon. I must admit reading all the subtitles is getting a bit tiresome, but that is a minor issue.\nConflict\nHistorically Star Trek has not featured conflict between the main starfleet characters due to the idea that humanity has evolved beyond this. DS9 got round this by having Odo and Kira who are not starfleet characters so can have a little bit of conflict. Discovery has completely abandoned this idea.\nIn the pilot episode, the Michael Burnham commits mutiny on her captain, even attacking her with a vulcan nerve pinch. In the third episode we finally meet the discovery crew. Gone are the TNG days where the ships crew are like a family, I am not sure I can think of a single character that would call another character â€œfriendâ€. We are in a time of war so this would be expected, however I do hope we see deepening friendships form between characters.\nI am OK with the change to feature more conflict. I must admit the darkness of the third episode, did stop to make me think a bit, however by the fourth episode I was won over.\nCast\nThe story is concentrated around Michael Burnham, so I do worry that other characters wonâ€™t get a look in. However from what I have seen Michael Burnham is a great character. She is a strong female character, with a intriguing back story relating to Sarek and possibly also Spock.\nSaru the alien character on the show, played by the very tall Doug Jones is great. From the trailers he was all about sensing death, but there is far more to him than that. I am looking forward to learn more about him and his threat ganglia.\nAs the show has gone on we have started to learn about the other characters like Captain Lorca, Tilly and Paul Stamets.\nTitle Sequence\nI donâ€™t like the opening title sequence, it feels very cheap like a draft version which hasnâ€™t been finished yet. I was looking forward to a title sequence that would show off the USS Discovery, maybe like TNG where it warps around our own solar system. What we see is some of the tech that features in Star Trek which is nice but I want more. The theme tune while great that it features parts from the classic theme doesnâ€™t stand on its own. I can hum all the other TV shows themes, and now after 4 episodes would struggle to do that.\nHowever I expect that after a while I will grow to like this more.\nOverall\nI like Discovery and I will keep watching it. I want to find out what happens to the characters. There are no annoying characters like other shows have had. There is character development, none of the characters are going to finish where they started. I want more friendship between characters, but I expect that will come, I want more exploring and doing Star Trek stuff, but most of all I want more episodes, roll on next weeks show.\nOther things I noticed\n Sound effects on the bridge are awesome, it makes me feel at home The USS Discovery doesnâ€™t feature in the first two episodes despite being the title of the show. Others have noted that Captain Georgiou has books on her shelf which feature classic episode titles  ","date":"Oct 10, 2017","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/05/DAD0hTKUAAAUkTP.jpg?resize=662%2C366\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/discovery-my-thoughts-so-far/","series":null,"tags":["StarTrek","Discovery"],"title":"Discovery â€“ My thoughts so far"},{"categories":null,"content":"In November 2015 it was announced that a new Star Trek series was going to be launched. It has been a long wait with multiple delays but Star Trek Discovery is finally here.\nStar Trek Discovery launches in the US on 24th September and in the UK on 25th September.\n Lets have a look back how we got here.\nStar Trek was created by Gene Roddenberry in the 1960s. He imagined a show where we would explore the galaxy while also exploring the human condition. He described it as a wagon train to the stars. However it was cancelled after 79 episodes and three seasons due to budget cuts and low viewers.\nAfter cancellation its popularity boomed in reruns. In the mid 1970s an animated series was created. But demand for Star Trek continued until a new Star Trek show was commissioned called Phase Two.\nIn 1977 Star Wars came out, and the phase two idea was cancelled and turned into a movie. In 1979 we got Star Trek The Motion Picture. It went massively over budget and failed to really capture the Star Trek spirit.\nA much cheaper sequel was created in 1982 Star Trek II: The Wrath of Khan with Roddenberry taking more of a back seat. A film widely considered to be one of the best trek films. Four more feature films starring the original cast were made.\nHowever in 1987 Gene Roddenberry teamed up with Rick Berman and The Next Generation was created with a brand new cast. The series ran for seven seasons set roughly a century after the adventures of Kirk and Spock.\nPopularity of Star Trek soured and this led Rick Berman to help create the spin off series Deep Space 9, Voyager and lastly Enterprise along with four feature films continuing the adventures of the TNG cast. When Enterprise was cancelled in 2005, Star Trek appeared to be dead after 18 years on TV.\nHowever in 2009 movie creator JJ Abrams recast the classic crew for the big screen. The film was big on action, but took place in an alternative timeline allowing for familiar characters to have new adventures without interfering with events depicted in the TV shows.\nTwo sequels to this film have been produced and are bringing Star Trek back into popularity. This has given rise to Discovery. Star Trek is returning to the TV screen and it is going to be great.\n","date":"Sep 22, 2017","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/09/gqojtfz1dhmxoiri7g8p.png?resize=662%2C372\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/star-trek-is-back-with-discovery/","series":null,"tags":["StarTrek","Discovery"],"title":"Star Trek is back with Discovery"},{"categories":null,"content":"I think Azure is great, but there is loads to it so I can never know about all of its features. There is a video series hosted by Scott Hanselman called Azure Fridays which I have started to watch in an effort to keep more up to date about some of its cool features.\n I watched this video recently and it is all about application insights and new ways you can debug your web applications by creating snapshots. I am a big fan of application insights so adding extra ways to debug my apps is a big win for me. Once I get this feature working in my code I will no doubt blog about it.\n","date":"Sep 18, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/azure-friday/","series":null,"tags":["Azure","Programming","ApplicationInsights"],"title":"Azure Friday"},{"categories":null,"content":"I recently blogged about using Azure Web Jobs , Azure Function is another way of doing the same thing, lets look at how they work.\n(Sorry its been a while since I blogged but I suspect an erratic schedule will continue for the next few months.)\nTo create an Azure Function go to the Azure portal and click add new and search for \u0026ldquo;Function App\u0026rdquo;\nGive your app a name and select the usual resource group and location settings.\nNow when you open Function Apps you should see your new app.\nI want my Function App to run on a schedule so I clicked the + next to functions and selected TimerTrigger. I am a c# programmer so I selected this option as well.\nGive your function a name and specify using the usual cron notation how often it should run. I want mine to run at 9.30pm each night so use 0 30 21 * * *\nNow comes the code bit. By default you get a window with the following code in it\nusing System;  public static void Run(TimerInfo myTimer, TraceWriter log) {  log.Info($\u0026#34;C# Timer trigger function executed at: {DateTime.Now}\u0026#34;); } It is entirely up to you what you get your function to do. In my case I just wanted to call a URL on a schedule so I created some code that used httpclient.\n using System; using System.Net.Http;  public static async Task Run(TimerInfo myTimer, TraceWriter log) {  log.Info($\u0026#34;Buffer 0 function executed at: {DateTime.Now}\u0026#34;);  HttpClient client = new HttpClient();  var result = await client.GetAsync(\u0026#34;URL\u0026#34;);  string resultContent = await result.Content.ReadAsStringAsync();  log.Info(resultContent); } Once you have created your app and it has run you can use the monitor section to view success and failures.\nThere is loads more you can do with Azure Function but this is a good place to start.\n","date":"Sep 12, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/azure-functions/","series":null,"tags":["Azure","Programming","DevOps"],"title":"Azure Functions"},{"categories":null,"content":"Pwned Pass is now available from the Google Play Store .\nPwned Pass is a simple android app that allows you to type in a password and tells you if it has been used in a data breach.\nTroy Hunt of Have I Been Pwned? recently added a new API to his website which allows you to search his extensive database of pwned passwords, 306 million of them. I have simply created a Android frontend to this API.\nThe API itself takes a SHA1 hash of the password and either returns a HTTP 200 if the password is found or a HTTP 404 if the password does not exist in the HIBP database. For more details of how Troy Hunt created this check out his blog post .\nMy app simply generates a SHA1 hash of anything that is typed in and then passes this to Troy Huntâ€™s API. I then get the HTTP return code so I know if the password exists or not.\nIt should be noted that: Do not send any password you actively use to a third-party service â€“ even this one! I donâ€™t log anything that you type into my app and all I am then doing is passing a SHA1 hash over SSL to HIBP. However you shouldnâ€™t trust my word alone.\nThe app itself is written in Visual Studio with Xamarin Forms in a similar fashion to the app I talked about last week .\nAs I am using Xamarin Forms there is the potential that I may develop iPhone or UWP versions of this code in the future. With that in mind I have made use of interfaces for the android specific parts of the code.\nI also make use of the modernhttpclient nuget package due to problems I encountered with httpclient and SSL. This is due to limitations of what libraries are available in mono and what has been implemented, I suspect there are better ways to solve this but that is all part of the fun.\nPlease do have a look at Pwned Pass and let me know what you think. Especially if it doesnâ€™t work or throws errors. I would like to spend time making this app as good as I can make it.\n","date":"Aug 14, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/pwned-pass-available-from-the-play-store/","series":null,"tags":["Android","Mobile","Programming","App"],"title":"Pwned Pass â€“ Available from the Play Store"},{"categories":null,"content":"For the past week or so I have been playing around with Xamarin and creating an android app.\nWell I now have an app in the Google Play Store. Check out https://play.google.com/store/apps/dev?id=6148298088834956775 . Before you rush and download the app I must warn you that it doesnâ€™t do much yet. It displays some content that is on my website and there are a few links to allow sharing of content. I have some ideas to display content from my blog and allow sharing. I also have some other ideas for apps that might actually be useful to people that are not me. If you have ideas or feature requests do let me know.\nOk how did I go about creating this app and getting it in the app store?\nXamarin is now part of Visual Studio so step one is install all the Xamarin features to Visual Studio and build an app.\nNext I wanted to monitor my app. Now I know Application Insights doesnâ€™t support apps so what tools are out there? HockeyApp is something I had heard of but they are in the process of being replaced with Visual Studio Mobile Centre .\nIt was relatively easy to hook up my app to Visual Studio Mobile Centre. First install the required nuget packages. Then add using statements and the following line to your MainActivity.cs file (these instructions are available on the Mobile Centre)\nusing Microsoft.Azure.Mobile; using Microsoft.Azure.Mobile.Analytics; using Microsoft.Azure.Mobile.Crashes; using Microsoft.Azure.Mobile.Distribute; using Microsoft.Azure.Mobile.Push; MobileCenter.Start(\u0026#34;[Unique ID]\u0026#34;,typeof(Analytics), typeof(Crashes), typeof(Distribute), typeof(Push)); Now you can connect the Mobile Centre to your source code (VSTS in my case) and get it to run a build for every commit.\nOne complexity of the build is that you need to supply a keystore file (basically a certificate to digitally sign your app). I found the best way to do this was to use Visual Studio to create the file.\nIn VS2017 there is a option called Archive Manager under the tools menu. In here click the distribute button and select Ad-hoc. In the signing identity section you can create a keystore file. Enter a few details and a keystore file will be created in AppData\\Local\\Xamarin\\Mono for Android\\Keystore[keystore name][keystore name.keystore]\nOnce you have added the keystore file to your build you can enable the distribute option. Now you will get an email after every build with a link to install your app.\nEvery time your app crashes the details will be logged in the crashes section for you to explore and fix the issues.\nThe Analytics section allows you to explore how your app is being used. You can also add Analytics.TrackEvent(\u0026ldquo;Feature X\u0026rdquo;) to measure the usage of different features.\nThere are more things you can do which I will explore more at another time along with how to get your app into the Google Play Store.\n","date":"Aug 7, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/android-app-development-and-the-visual-studio-mobile-centre/","series":null,"tags":["App","Azure","Programming","Android"],"title":"Android App Development and the Visual Studio Mobile Centre"},{"categories":null,"content":"For a while the Async and Await commands in c# have confused me.\nLike most things the best way to learn about something is to use it in a real world example. I am currently adding an email alert feature to a website. This is an ideal example of something that would benefit from Asynchronous programming. There is no need for the webpage to wait to send 1000s of emails, lets just send a call to get started and allow the browser to carry on as normal.\nThis is my first try at using async and await so feel free to suggest best practises in the comments.\nLets start with a Send method in my EmailController.\npublic ActionResult Send(int id, int pageId, int userID) {  if (!Authorize.checkPageIsAuthorised(userID, (Authorize.PageIds)pageId))  {  return Redirect(\u0026#34;/login\u0026#34;);  }  else  {  Task\u0026lt;string\u0026gt; t = SendNotifications(id,userID);  return Redirect(Request.UrlReferrer.ToString());  } } This simply checks to see if you have permission to the page. If not redirects to the login page otherwise it makes a method call and redirects back to the page it came from.\nLets have a look at that method call in more detail.\nTask t = SendNotification(id, userid);\nSendNotification doesnâ€™t return a normal string it returns a Task, so lets look at how we are creating this.\npublic async Task\u0026lt;string\u0026gt; SendNotifications(int id,string type,int userid) {  //logic ommitted  await ef.SendEmail(model, emailHtmlBody);  return \u0026#34;OK\u0026#34;; } The return type is set to Task but it has the aysnc keyword appended to it. It also makes a call with the await keyword.\npublic async Task SendEmail(EmailModel model,string emailHtmlBody) {  //logic removed  await smtp.SendMailAsync(message); } So that is it. My first bit of code that uses Async and Await. My controller calls a method asynchronously which then calls another method asynchronously which sends emails asynchronously.\nAsync â€“ This enables the Await keyword to be used in the method\nAwait â€“ This is where things get asynchronous. The await keyword allows the code to wait asynchronously for the long running code to complete.\n","date":"Jul 24, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/async-and-await/","series":null,"tags":["Async","Await","Asynchronous","Programming","C-Sharp"],"title":"Async and Await"},{"categories":null,"content":"I recently watched Troy Huntâ€™s What Every Developer Must Know about HTTPS course on Pluralsight. Its very good and really make you think about SSL certificates and how to correctly implement them.\nOne thing in particular Troy mentions is the website SSL Labs . This website allows you to test a websites implementation of SSL. A grade of A to F is assigned with A being the best and F being the worst.\nTroy Hunt has a blog post where he discusses how Australian Banks score. Lets look at a few UK banks.\n   Bank SSL Certificate Grade Home Page Under SSL     HSBC www.hsbc.co.uk  B Y   Nationwide onlinebanking.nationwide.co.uk C N   NatWest www.nwolb.com  C N   Barclaycard www.barclaycard.co.uk  A- Y   Barclays bank.barclays.co.uk A- N   Lloyds Bank www.lloydsbank.com  A N   Royal Bank of Scotland www.rbsdigital.com  C N   Standard Chartered www.sc.com  C Y   Virgin Money uk.virginmoney.com A+ Y   Santander retail.santander.co.uk A- N    On the whole the ratings are all quite good with all being in the range A-C. However I have also indicated if they have SSL on the home page. Only 4 out 10 website listed above have the home page load under SSL.\nWhy does this matter as long as the login is under SSL? Any page that loads over http is potentially at risk from a man in the middle attack. A fake malicious home page could contain links to any page and trick users into entering personal information.\nIf you want to test a bank or other website not listed here. Go to https://www.ssllabs.com/ssltest/index.html and type the address that is on the SSL certificate in to the search. The good news is that this site scores a A.\nTroy mentions that there is rapid growth in the adoption of SSL, there is also rapid growth in improving ratings. One of these banks went from a C to an A during the course of writing this blog.\n","date":"Jul 17, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/how-good-is-your-ssl/","series":null,"tags":["Certificates","SSL","Security"],"title":"How good is your SSL?"},{"categories":null,"content":"Whenever I write a new test I have to think how best to do it. Hopefully I can summarise a few tips here to help get started.\nArrange Act Assert The first thing I think about when writing a test is Arrange, Act, Assert. Arrange, Act, Assert is a pattern for writing the tests.\nArrange â€“ This gets things in order ready to execute the test.\nAct â€“ This executes the method you want to test.\nAssert â€“ This compares the value produced in the Act step with a known value typically with a method similar to the following\nAssert.AreEqual(expected value, actual value)\nSay for example you wanted to test a method called ReturnsTrue() which does nothing but returns a value of true. This method is in a class called ReturnsTrueClass\nThe Arrange step in this example would be.\nReturnsTrueClass t = new ReturnsTrueClass(); The Act step in this example would be.\nvar result = t.ReturnTrue(); The Assert step in this example would be.\nAssert.AreEqual(true, result); This is a stupidly simple example but hopefully you get the idea of how you can build all your tests with these three steps.\nRecently I saw a tweet complaining that someone has mixed up expected and actual in the Assert statement.\nThere is a minor but special hell reserved for those who mix up the expected and actual parameters in Assert.Equals\n\u0026mdash; Keith Williams (@zogface) July 5, 2017  At first glance this probably isnâ€™t the worst mistake to make as if your tests are all passing actual and expected are the same.\nHowever tests will fail, that is the whole point of them, you can then fix bits of code. If you have mixed up actual and expected it adds extra time to debugging and figuring out what values are produced from your code and what you are expecting it to produce. It may be your test uses a mocking framework and somewhere in there, there is an issue, with mixed up expected/actual you may assume a problem in your code rather than the test.\nAlso, how do you make such an error? When I type Assert.AreEquals() in Visual Studio, Visual Studio tells me what each parameter does, it takes a matter of seconds to do this, just by hovering over the code.\nOne last tip to say about tests. Write your tests to test the behaviour of your application.\n","date":"Jul 10, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/writing-your-first-test/","series":null,"tags":["Act","Arrange","Assert","Programming","Testing"],"title":"Writing your first test"},{"categories":null,"content":"I am a fan of Azure but today I have been looking at AWS. Specifically how to upload and download files.\nAWS S3 stores files in Buckets. I already had an AWS S3 account setup with a Bucket. I am going to assume you have got a bucket setup and concentrate on the code to get files in and out.\nFirst step is to use nuget to install the AWS packages. In nuget the packages you want are called AWSSDK.Core and AWSSDK.S3.\nThe using statements you want to use are called Amazon.S3 and Amazon.S3.Transfer, not sure why this doesnâ€™t match nuget, this difference caught me out a couple of times.\nNow to the code that uploads files\nAmazonS3Client AWSclient = new AmazonS3Client(accessKeyID, secretAccessKeyID, Amazon.RegionEndpoint.EUWest1); TransferUtility fileTransferUtility = new TransferUtility(AWSclient); using (FileStream streamWriter = new FileStream(path, FileMode.Open)) {  TransferUtilityUploadRequest fileTransferUtilityRequest = new TransferUtilityUploadRequest  {  BucketName = \u0026#34;flawlessimages\u0026#34;,  InputStream = streamWriter,  Key = fileName  };  fileTransferUtility.Upload(fileTransferUtilityRequest); } Lets break it down and look at what it does.\nAmazonS3Client AWSclient = new AmazonS3Client(accessKeyID, secretAccessKeyID, Amazon.RegionEndpoint.EUWest1); This creates an instance of AmazonS3Client, we are passing the Access Key and Secret Access Key both of which can be found from your Amazon S3 account My Security Credentials section. Amazon.RegionEndpoint.EUWest1 specifies the amazon data centres that your bucket is located in.\nTransferUtility fileTransferUtility = new TransferUtility(AWSclient); This creates an instance of TransfterUtility using the AmazonS3Client instance we created in the previous step.\nusing (FileStream streamWriter = new FileStream(path, FileMode.Open)) { This opens up a filestream from a files path and specifies that the file should be opened.\nTransferUtilityUploadRequest fileTransferUtilityRequest = new TransferUtilityUploadRequest {  BucketName = \u0026#34;flawlessimages\u0026#34;,  InputStream = streamWriter,  Key = fileName }; fileTransferUtility.Upload(fileTransferUtilityRequest); This last step specifies which bucket to upload to, what input stream to upload and the Key to use. Key is just AWS way of referring to files, more commonly referred to as the filename.\nThis is all you need to do to upload a file to your Bucket. The file will be located at https://s3-eu-west-1.amazonaws.com/[bucketname]/[filename] , however by default it will not be downloadable until you set Read permission to everyone, once you do that anyone who has the link will be able to download your file.\nThis is the same permission level as any file you have on your webserver, however AWS has a better way.\nusing (s3Client = new AmazonS3Client(accessKeyID, secretAccessKeyID, Amazon.RegionEndpoint.USEast1)) {  GetPreSignedUrlRequest request1 = new GetPreSignedUrlRequest  {  BucketName = bucketName,  Key = filename,  Expires = DateTime.Now.AddMinutes(5)  };  urlString = s3Client.GetPreSignedURL(request1); } Here we are generating a url to download the file, but we are specifying that it is only valid for 5 minutes. This means that if you share the url it will only work for 5 minutes, after that AWS will give an access denied message.\nThis is much better security than you have on a typical web server, and easy to implement, every time a user clicks on a download link you generate a new presigned url and send the download to the browser, as long as this process doesnâ€™t take longer than 5 minutes the user will never know.\n","date":"Jul 3, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/uploading-files-to-aws/","series":null,"tags":["AWS","C-Sharp","Programming"],"title":"Uploading Files to AWS"},{"categories":null,"content":"I keep hearing about Azure WebJobs but I have never used them. Time to change this.\nWebJobs are a feature of Azure App Service that can run a script at a specific time. In my case I would like to hit a specific url of my website at the same time every day.\nTo the right you can see an example of the WebJobs form on the Azure portal that you need to fill in.\nYou need to supply a name for your webjob.\nYou need to upload the script that will run in my case I used a powershell script. My script consisted of which basically just loads the url specified.\n$progressPreference = \u0026#34;silentlyContinue\u0026#34;; $result = Invoke-WebRequest -Uri (\u0026#34;https://www.google.com\u0026#34;) -Method Get -UseBasicParsing; Type refers to if your job will be triggered or run continuously, I want it to be triggered.\nTriggers refers to if you want it to be scheduled or manual, something that you can run on an ad hoc basis. I of course want scheduled.\nIf you are familiar with the linux CRON then the next box will make sense to you for everyone else I will try and make sense of it. The box consists of 6 numbers which can either have a value or a *. The numbers correspond to the following {second} {minute} {hour} {day} {month} {day of the week}.\nA hourly job would be expressed as 0 0 * * * *, ie every day of week, every month, every day, every hour and only when minute and second equals zero. For more help with this check out the MSDN docs about it. I want to use 0 30 21 * * * to run daily at 9.30pm.\nThatâ€™s it everything setup, now time to wait and see if it works.\nOh no!\nIt failed to run at the specified time.\nThe reason for this is the scheduler requires the feature Always On to be turned on which is not available in the free App Service. Before you reach for your wallets, I found a solution on this blog post that allows them to run on the free tier.\nThe thinking behind this solution is you need to keep the website alive throughout the day so Tom has created a script that does this. His script can be found on his blog or on his github page .\nSet this script up to run every 5 minutes (0 */5 * * * *) like the example above.\nThe nextthing you need to do is create a Custom connection string in the Application Settings blade called SecretThing. Tomâ€™s script references this to access the website and keep it alive. The password you need to put in SecretThing can be found in you publish profile (downloaded from the Overview blade in the Azure portal). For more details and a better explanation check out Tomâ€™s blog .\nOne last thing to mention about WebJobs is that you can see details about when they have run at https://[YourWebAppName].scm.azurewebsites.net/azurejobs/#/jobs and this can be a great place to help debug your scripts.\n","date":"Jun 26, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/using-azure-webjobs-to-automate-stuff/","series":null,"tags":["Automate","Azure","Cron","Programming"],"title":"Using Azure WebJobs to Automate Stuff"},{"categories":null,"content":"A while ago I blogged about promoting my blog with Buffer. At the time I made use of the nuget package BufferAPI but lets look at some improvements I can make.\nThe BufferAPI package worked great from my console app, but when I tried to use it from a Controller in an MVC app I never got it to work. Lets look at the API docs and see if I can rewrite it.\nThere are two main types of API calls GET which gets data from the server and POST which posts data to the server. These come from the types of HTTP requests.\nI quickly figured out how to use the GET API call to authenticate using https://api.bufferapp.com/1/profiles.json?access_token=XXXX However POST was defeating me. That was until I remembered Fiddler .\nI had heard Troy Hunt (and others) talk of using Fiddler to examine what data is being passed among websites. Troy uses it to do a man in the middle test to see what information can be stolen.\nIt is really easy to setup, install Fiddler, click yes to a few security warnings and you can see what information is being passed from your code to remote APIs.\nOnce I had Fiddler installed I could compare what information is being passed between a successful API call using the BufferAPI nuget package and an unsuccessful API call using my code.\nFiddler also showed that passing my authentication token in a POST request is much better. Despite both GET and POST being encrypted when using HTTPS, anything at either end that logs URLs will have a log of your username and password.\nIf you have not tried Fiddler, give it a try especially if you are doing things with API calls.\n","date":"Jun 19, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/fiddler-and-apis/","series":null,"tags":["",""],"title":"Fiddler and APIs"},{"categories":null,"content":"Last week I talked about Power BI , what it is and some of the different services you can use with it. This week lets add some of that data to a simple web page.\nFor this example I am going to add the google analytics data from this website to this page.\nFirst login to your Power BI https://app.powerbi.com/ Click the get data link at the bottom left.\nClick My Organisation to bring up the app search box.\nClick the Apps tab and search for â€œgoogleâ€ in the search box, you should then see Google Analytics, click into this and then click the get it now button.\nLog into your google account. If you have multiple google accounts I found it worked best to sign out of all of them or run this in an incognito window.\nOnce you are signed in you should see a list of the different google analytics data you have, select the one you want to use and click import.\nPower BI will then go away and start loading the data.\nOnce loaded go to Reports and select the Google Analytics that has been loaded. If you have more than one, it is a good idea to rename each one eg Corporate Site Google Analytics, Blog Google Analytics so you wonâ€™t get mixed up.\nIn the file menu select Publish to web and agree that you are OK for this to be made public.\nYou will then be given a piece of HTML code that starts with \u0026lt;iframe copy this onto your web page. Reload your webpage and you should see something similar to mine below.\nIt should be noted that while you can but the webpage containing the iframe behind a login page, the data could still be accessed if you knew the url contained within the iframe, this is why the link can be emailed and continue to work.\n","date":"Jun 12, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/how-do-i-add-power-bi-data-to-a-webpage/","series":null,"tags":["PowerBI",""],"title":"How do I add Power BI data to a webpage?"},{"categories":null,"content":"For the past few weeks my software developing has been taking a back seat as I planned and coordinated the IT requirements of an office move.\nThe company I work for has been working out of a converted barn, but as the company has grown we have outgrown the building and for quite a while it has been a real squeeze to get everyone in. We now have a shiny new offices with plenty of room for growth.\nIt was at the start of the year when I first started getting involved with the new office. At that time building work was about to begin. The office had concrete floors so in order to get network and power points in the floor, channels would have to be cut into the concrete for the floor boxes and cable runs.\nAt the old office our servers were in a cabinet in the corner of the room which meant they could be quite loud especially when the fans were going full pelt. Our American owners were supplying brand new IT equipment and we would have a dedicated server room.\nSoon a weekly conference call was setup so we could coordinate with the IT people in America and the various different contractors that would help deliver our new office.\nOne thing I was particularly proud of was YDS. York Data Services (YDS) is an ISP I have worked with in the past at a previous job and I was able to continue my relationship with them and they were contacted and became our primary internet supplier.\nUnfortunately most ISPs have to deal with BTOpenreach who have a bit of a monopoly on getting phone lines or leased lines installed. The initial estimate we were given was two weeks after we were due to move in. It was looking like we would move in and two weeks later we would get an internet connection. We were in the process of getting quotes for a temporary internet connection when BT contacted us and could get us connected up. Soon afterwards a huge cabinet and two pallets full of equipment was delivered. Two days later a team was dispatched to rack everything up. On the rack we had the following equipment: 4 x UPS, 4 x Network Switches, 2 x Routers (for 2 x Leased Lines from different providers, the secondary connection has not been installed yet), 2 x Palo Alto Firewall devices, 2 x New Servers.\nJust prior to our move we had a rack full of equipment but no patch cables, nothing connected up and I was beginning to get concerned that we would not be ready in time.\nThankfully a team was dispatched to help out over the weekend of the move, with the patch cables being delivered only hours beforehand. I worked closely with them and we worked late into the night. By the end of the Friday night our domain controller had been moved and its IP address updated. DHCP scopes had been setup for the new network. Once that was done we could get the WiFi points connected up (These had already been fitted to the ceiling).\nThe following day we moved our existing servers and updated their IP addresses and got everything patched up. Furniture started arriving so desks could start being setup and phones connected up. Everything was slotting into place, and for the first time our new office was starting to look like an office.\nBy the time it got to Monday the only problem was the phone number had not transferred as requested (something else to blame on BT) All staff could either connect via an Ethernet cable or connect to our brand new WiFi network and had access to all the IT services they had at the old office. Another minor issue was the NAS we used for backups had to be reset as it couldnâ€™t communicate with the domain so no one could login, luckily this didnâ€™t affect the data on it.\nFrom my perspective the move was a massive success considering how complex and how many different people had been involved.\n","date":"May 1, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/office-move/","series":null,"tags":["ITAdmin","Routers","Servers","OfficeMove"],"title":"Office Move"},{"categories":null,"content":"I am trying to understand interfaces and when to use them in my code.\nAn interface defines a contract and any class that implements that interface agrees to fulfil that contract.\nLets look at an example as this tends to be how I learn best.\nMost applications require some sort of data to work from so lets start by defining IData which can load data.\npublic interface IData {  Blog LoadData(string Connectionstring); } My interface defines one method LoadData and outputs an object called Blog (I will explain why in a minute)\nA common data source is a SQL database so we could define a SQL class that implements IData.\npublic class SQL : IData {  public Blog LoadData(string Connectionstring)  {  Blog blog = new Blog();  using (SqlConnection con = new SqlConnection(Connectionstring))  {  con.Open();  //etc We could also get data from an RSS feed of a blog (hence why I called the object Blog earlier)\npublic class XML : IData {  public Blog LoadData(string Connectionstring)  {  XmlDocument myXmlDocument = new XmlDocument();  myXmlDocument.Load(Connectionstring);  Blog blog = new Blog();  foreach (XmlNode RootNode in myXmlDocument.ChildNodes)  {  //etc Both classes implement IData and have a method called LoadData which has a string parameter and outputs a blog object. The string parameter is either a connection string to a SQL database or the URL of the rss feed. Not sure if there is a better way of doing this bit, maybe the name of the string needs making more generic.\nNow we have some classes that implement an interface what can we do with them. Lets write a class called GetData which gets data but doesnâ€™t care if it comes form the rss feed or a SQL database.\npublic class GetData {  private IData _repo;  public GetData(IData repo)  {  _repo = repo;  }   public Blog LoadData(string Connectionstring)  {  var original = _repo.LoadData(Connectionstring);  return original;  } } When we call GetData we can either pass in XML or SQL as the class is not tied to either implementation. We could even write other classes that implement IData for testing purposes.\nMy full code can be found on github .\nThe advantages of writing code in this way include code that is easier to extend, easier to test and easier to maintain. This is only the start of my understanding and I am sure this is going to be a topic I come back to in the next few weeks.\n","date":"Mar 27, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/interfaces/","series":null,"tags":["C-Sharp","Interface","Programming"],"title":"Interfaces"},{"categories":null,"content":"Today I spent some time learning the R language.\nThe problem I was trying to solve was to convert local prices of some items into Euros. I had been using a fixed exchange rate for all data, but as exchange rates fluctuate so much this is incorrect.\nMy first though was to find a free API that I could query to get the values I wanted. The first API I found didnâ€™t cover all the currencies, the next one I found I burnt through the free allowance in one pass.\nA colleague of mine mentioned using R to solve this, he sent me some links and I set out to write my first piece of R code.\nMy finished code can be found on github and I will attempt to explain some of it.\nR defines functions fairly simply\nnameoffunction \u0026lt;- function(arg1, arg2) { arg1 * arg2 } I have created a function that takes 2 parameters date and currency. I know I have about 10 different currencies that I want to get currencies for and I want to loop through each day so I will need to pass in a date.\nThe source of my exchange rate information is the www.xe.com website, its historical exchange rate page passes currency and date into the query string so I should be able to build up a string containing all the different elements.\nAll programming language can concatenate strings and R is no different. R uses paste()\nvar \u0026lt;- paste(\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;) However R has an annoying feature in this function. I would expect that var in the above example would contain \u0026ldquo;HelloWorld\u0026rdquo;, it doesnâ€™t it contains \u0026ldquo;Hello World\u0026rdquo;. Why it automatically adds a space I donâ€™t know?\nvar \u0026lt;- paste(\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;, sep=\u0026#34;\u0026#34;) I am not entirely sure what all of the code does but I can take a good guess.\nread_html() I would guess loads a html page, html_nodes() finds all the html tags of a certain type on the page, in my case \u0026lt;table\u0026gt;, html_table() reads the first table it finds.\ntable1[2] selects the second column, and head() selects a specific number of rows. I want the first row and second column so I combine these two as head(table1[2],1)\nNow that I have found my exchange rate what do I do with it? R can read and write to SQL Server so why not store this info in a SQL lookup table. I can then use this data in a stored procedure when I process my data.\nTo query sql you can use sqlQuery(), it has two parameters, a sql connection and a TSQL command (eg a SELECT, INSERT, UPDATE statement)\nI use a while loop to loop through every day between 1st October 2016 and today and look up the exchange rate for each currency.\nFor now I am manually running this R script, however there are ways to run R directly from SQL Server which I may well investigate. I could then have a SQL job to run this on a schedule, maybe once a day to get the latest exchange rates. I also would like to do something a bit cleverer like only getting exchange rates for the days that I need them by querying existing database tables.\n","date":"Feb 27, 2017","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/02/Exchange-Rate-Calculator.jpg?resize=300%2C202\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/learning-r/","series":null,"tags":["ExchangeRates","R","Programming"],"title":"Learning R"},{"categories":null,"content":"I am a big fan of Azure but I know zero about its biggest rival â€“ Amazon Web Services or AWS.\nSo lets sign up for a free trial and see what it can do. The AWS free trial is available from https://aws.amazon.com/free/ and lasts for 12 months. From memory I think the Azure free trial lasted only one month.\nTo start you need to login with your amazon account and create an AWS account. This requires your name and address and your payment info (you will only get billed if use services not covered by your free trial).\nInterestingly AWS requires you to verify your identity via an automated phone call. (I donâ€™t recall doing anything like this for Azure but please correct me if I am wrong.)\nOnce you are logged in you get a series of links displaying all the different services that are available. First impression is this is a simpler view to Azureâ€™s portal with a similar amount of services. At the top right is an option to select which region you want to use, in Azure I use North Europe and West Europe, AWS has Ireland and Frankfurt.\nCreate a Web App First thing to try is setting up a website. I selected create a web app and I get a page asking me for its basic details (very similar to Azure, however AWS asked what language your code is written in, Azure handles all of these) AWS websites appear to support a host of different options similar to Azure.\nThe actual creation of your website takes a few moments (like on Azure). However the default URL for websites is similar to http://test.vjbbimyv7w.eu-central-1.elasticbeanstalk.com/ which is not quite as nice as the Azure equivalent http://test.azurewebsites.net Azure has a host of command lines available via powershell. AWS has a similar command line interface called AWS CLI, including the option to deploy from git to your website.\nAWS Toolkit for Visual Studio is an extension that allows for the publishing of websites to AWS. (Just like you can publish to Azure)\nAs I learn more about AWS I will continue to blog about it. Amazon Web Services Pt 2 ","date":"Jul 21, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/amazon-web-services/","series":null,"tags":["AWS","Azure","Amazon","Programming"],"title":"Amazon Web Services"},{"categories":null,"content":"Thatâ€™s right this is the one hundredth post that I have written on this blog.\nSo what have I learned in the past 100 posts?\n It is easier to write a blog before you become a parent. More recently I am increasingly finding it difficult to find the time to blog. It used to be that I could write on an evening, but now James is around I often prefer to play with him, or more often stop him crawling where he shouldnâ€™t. I like my job. The inspiration for most of this site is my day job and as you can read, I do a wide variety of different things, but I have a lot more to learn as well. Finding your niche is hard. 100 posts in and I am still not sure what my niche is. I started out with the broad niche of IT and what I do, I then considered something about IT and fatherhood but I donâ€™t think that topic is really me. My current thinking is maybe DevOps especially as my role these days fits squarely between Development and Operations. Its time for a refresh. I have been meaning to change the theme of this site for some time and I feel after 100 posts now is as good as time as any. I want to emphasise my skills and what I am learning and increase the emphasis on DevOps, which I think will be my niche. Watching visitor numbers is addictive. Every day I look at how many people have looked at my blog, but I have yet to see a pattern between what I write and how many reads I get. Is my writing getting better? I donâ€™t know. Are more people reading? Probably not. Will I keep going? Yes  So what is next?\nHopefully a refreshed look in the next few months. Hopefully regular posts. If there is something you want to see on here drop me a message via any of the social media links or put a comment below.\nWhat is my favourite post?\nMaybe User Groups and F# which proved very popular and got me to start going to user groups something I still enjoy today. I also like Coding Myself Into A Corner which got me to start thinking more if I was giving myself future problems. But there are many others I like such as James and Becoming a father ","date":"Jun 2, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/i-m-100-blog-posts-old/","series":null,"tags":["100","Blogging"],"title":"Iâ€™m 100 blog posts old"},{"categories":null,"content":"I use SQL Server Management Studio all the time for writing queries, getting data and general SQL development.\nI have enjoyed seeing the improvements that each new version of SQL Server Management Studio (SSMS) introduced. One great improvement was intellisense.\nThis feature saves typing and reduces errors by automatically suggesting tables, column names or other database objects.\nA common query that I get asked to write is provide a spreadsheet that gives the information that satisfies certain criteria. This is easy to do in SSMS, you can write the query, click execute and the rows that satisfy the criteria are displayed. These rows can then be easily copy/pasted into Excel or other spreadsheets.\nA common data item that gets stored in databases is addresses and addresses often contain line breaks to make the data display better. In the earlier versions of SSMS, when you copied and pasted these line breaks were ignored and the data displayed the same in SSMS as it did in Excel. However in the more recent version, theses line breaks got copied across breaking your spreadsheet and making it hard to see what data corresponded with what.\nNow I donâ€™t know if this should be described as introducing a bug or fixing one. I can easily argue both sides. If your data contains a line break and you copy this data it should include the line break in the destination, but if it does that it displays badly in Excel.\nThe fix I have been using until recently is to use the following TSQL command in my queries.\nSELECT REPLACE(REPLACE(@str, CHAR(13), \u0026#34;), CHAR(10), \u0026#34;) This command replaces any line breaks with an empty string. Both Char(10) and Char(13) are needed because you can have different types of line breaks. This is great if you are writing the script from scratch but isnâ€™t great if your are running a stored procedure or your query has a lot of columns.\nThe answer to this is to use Visual Studio to run your SQL query. In Visual Studio you can write and run queries via Server Explorer and the results produced donâ€™t contain line breaks. I have only just discovered this solution, but so far it has worked and is very easy to do, plus as I do most of my development in Visual Studio anyway it saves me having to open SSMS to test my queries.\n","date":"Dec 3, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/sql-server-management-studio/","series":null,"tags":["Database","Programming","SQLServer"],"title":"SQL Server Management Studio"},{"categories":null,"content":"As you have probably heard Star Trek is coming back to TV. I recently joined with the Sci Fi Waffle podcast to discuss.\nSci Fi Waffle: Episode 12 â€“ STAR TREK IS BACK!!\nIn this episode we discuss the great news that Star Trek IS BACK!!.\nShawn and I are joined again by James Roberts and TrekMates News feed Editor Simon Foster to speculate about the announcement of a new Star Trek TV series due to debut in January 2017.\nJames can be found on our sister podcast The Battle Bridge and Simon can be found on Twitter @funkysi1701 To listen go http://www.trekmate.org.uk/sci-fi-waffle-episode-11-star-trek-is-back/ , to leave feedback go http://forum.trekmatefamily.com/ ","date":"Nov 12, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/11/3f448330857e4ca7ba40ab5465a3c2cb62cdb98c.png?resize=620%2C260","permalink":"https://www.funkysi1701.com/posts/star-trek-is-back-in-2017/","series":null,"tags":["StarTrek","podcast"],"title":"Star Trek is back (in 2017)"},{"categories":null,"content":"Today is my day off, but I wake up and have a quick look at nagios to see if there is anything I need to worry about. Yes there is, SQL Server has run out of disk space on its data disk.\nI race downstairs and VPN onto the server to find out what has happened. One of my monitoring databases has had runaway log growth and is over 80Gb is size.\nBACKUP LOG [DBName] WITH TRUNCATE_ONLY DBCC SHRINKFILE(\u0026#39;DBName_Log\u0026#39;) Free disk space is back to normal, all users will be unaware of the problem and everything is fine again. I create a daily job that runs the above code, that way it should stay a manageable size.\nNext I need to find out why it happened and to prevent it happening again in the future (Next time I have a day off I want to lie in!)\nI check the SQL logs and notice\nBACKUP LOG WITH TRUNCATE_ONLY or WITH NO_LOG is deprecated. The simple recovery model should be used to automatically truncate the transaction log.\nThen I remember what I have done to cause this issue. I have a separate disk for my backup files and earlier in the week I noticed this disk was filling up, a large amount of space was taken up by transactional backup files. I thought I donâ€™t need to backup the transactions for this non critical database, I will just do a full backup at the start of everyday.\nHowever what I forgot is that a transactional backup keeps the log file under control, once this backup was stopped the log file grew uncontrollably. The answer, change the database from FULL mode to SIMPLE. This is my understanding of how backups work in FULL mode. A full backup is done at the start of the day which resets the log file, then changes in the database are stored in the log file, this is backed up into a transactional backup and the log file gets reset. If you have regular transactional backups throughout the day the log file doesnâ€™t grow too much, however with no transactional backups your log file contains an entire days worth of changes and so for a monitoring database this could be quite large.\nIn SIMPLE mode you canâ€™t do transactional backups and the log doesnâ€™t grow uncontrollably. This shouldnâ€™t be used for production databases as if there is a problem you could loose data.\n","date":"Jun 12, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/runaway-sql-log-growth/","series":null,"tags":["ITAdmin","Backups","SQLServer"],"title":"Runaway SQL Log growth"},{"categories":null,"content":"I have spent most of the day tweaking my Azure websites. Lots of fun!\nLast week unfortunately Azure had some problems and many websites that were running in the North Europe data centre were unavailable for several hours. And you guessed it my websites were hosted here.\nAll hosting providers are going to have downtime from time to time and this is just something you have to take on the chin. The important thing to do in times like these is communicate with your customers about what is going on and that you are doing everything you can to restore service.\nHowever Azure has some amazing features that you can configure to help manage when downtime occurs.\nAzure is Microsoftâ€™s global cloud platform. And it really is global, there are data centres in North Europe, West Europe, Brazil, Japan, two more in Asia and five in the US. In the event of problems it is highly unlikely that more than one of these would go down at once. If all of these are unavailable, I expect the planet earth is facing some kind of cataclysmic event and the fact that my website is down is not a priority.\n To take advantage of these multiple data centres, Azure has something called a Traffic Manager.\nTraffic Manager has various settings but I am using it in failover mode. This means that if one website goes down, the next one is used.\nAll you need to do is create a traffic manager, add two or more websites to it (called endpoints) and choose a page that needs to be monitored so Azure knows which websites are up and which are down.\nIf you are using SSL or custom domain names, there are a few extra steps you need to do. Your custom domain name needs pointing at the traffic manager, not the individual websites. The websites themselves have three domain names, the traffic manager address, the azure address and the custom domain name. The SSL certificate can then be assigned to each website that you have added to the traffic manager.\nThat was easy wasnâ€™t it, and now if a website goes down traffic manager will use the next one. While testing this, the transition to the next website was almost immediate. I did notice that if you had a browser showing the website open during a problem you sometimes got an error page, I think this was probably due to browser caching, reopening a tab or browser fixed this issue.\n","date":"Mar 12, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/azure-traffic-manager/","series":null,"tags":["Azure","Clouds","DevOps","ITAdmin"],"title":"Azure Traffic Manager"},{"categories":null,"content":"My desktop is always a mess. I constantly download files there and forget all about them.\nEvery now and then I copy files into sub directories, so my desktop looks sane for a day or two before it gets out of control again.\nWhy donâ€™t I write a script that I can schedule to do this for me. Then my desktop will always be tidy.\nI have written a few simple batch scripts, but of course the best scripting language out there at the moment is PowerShell. Lets use that.\nWindows provides a nice little utility for writing scripts called the Windows PowerShell ISE, so let\u0026rsquo;s start by loading that up.\nPS has lots of help included to help you, just run Get-Help [name of ps command]\nTo move files you can use Move-Item which works very similar to copy, specify source and destination. In my case I moved files based on their file extension.\nMove-Item *.pdf folder\nNow all I need to do is schedule this script to run either every day or so, or maybe every time I login or switch my computer on.\nPowerShell can do lots more interesting things which hopefully I will blog about soon.\n","date":"Mar 11, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/tidying-my-desktop/","series":null,"tags":["PowerShell","Desktop","Programming","ITAdmin"],"title":"Tidying my desktop"},{"categories":null,"content":"Last week I blogged about the Game of Life.\nWell it took some searching through dead hard drives and old USB storage but I found the program I wrote, and better than that I have figured out how to turn it back into source code.\nThe file date is November 2004 and the source code has no comments so I donâ€™t know what was going through my head when I wrote it, or even what some of it does.\nTo view the source code download Life.src , to download the finished program download Life .\nI am actually surprised I was expected the code to be much worse than it was, I am not saying it is good, but I can see more than one method and I canâ€™t see any code that obviously is repeated. I can see some terrible variable names. Note to future self donâ€™t use tmp676_674!\nAssuming that I wrote this in 2004 what would I have been doing then? It was before I started working in IT. (Hard to imagine I know!) It may even have been before I started living with Keith, before he recruited me for my first IT role. I think I was probably working as an administrator for Defra at this point.\nIf we put 2004 in technical terms, it was before I joined facebook, Office 2003 and XP would have been installed on my PC, and probably felt brand new.\n","date":"Feb 21, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/02/Life.jpg","permalink":"https://www.funkysi1701.com/posts/source-code-for-game-of-life/","series":null,"tags":["Java","Programming","SourceCode"],"title":"Source Code for Game of Life"},{"categories":null,"content":" You may not have heard of Nagios but it has saved my bacon quite a few times.\nNagios is an open source server monitoring application that runs on many linux flavours.\nI canâ€™t remember exactly when I first installed nagios but I am guessing it was sometime in 2007/8. My boss gave me a book about it (which I never read) and told me to create a system to monitor the companies servers.\nNagios is not simple to set up. It relies on setting up various Hosts and services. Hosts are usually physical servers that you want to monitor and services are all the services you want to monitor. As this is a linux program all these can be configured by editing the right config file\nNagios is very flexible and can be expanded easily with the use of plugins, if you want to monitor something there is usually a plugin available. If you have a dell server running openmanage software there is even a plugin that allows the temperature of your server to be monitored.\nIf you want to monitor windows servers the use of nsclient++ is a real advantage. This is a simple client that runs as a service on your windows server. This allows nagios to track memory, cpu, disk space, performance and services, in fact almost everything that you would want to monitor.\nOver the years I have kept a close eye on Nagios and added extra checks as new services were added or problems encountered. A few years ago I dabbled with sending alerts out via SMS message and once I got a smart phone found an app to keep track of Nagios 24/7.\nBut recently I have started wondering if Nagios is the best way to monitor modern servers like 2012 or remote services like Azure. I want something that is easy to expand as your IT infrastructure expands. Something that relies on running on a linux OS requires your IT staff have a knowledge of linux and you keep that server maintained and updated.\nMy question is: Is Nagios still the best way to monitor my servers?\n","date":"Sep 24, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/09/nagios.png","permalink":"https://www.funkysi1701.com/posts/i-love-nagios/","series":null,"tags":["ITAdmin","Monitoring","Servers","Nagios"],"title":"I love Nagios"},{"categories":null,"content":"My real name is Simon Foster but my on-line persona is Funky Si a nickname I was given during my university days which has somehow stuck. I am a Developer working in the North of England. I have been working in IT departments since 2006 and have a wide range of experiences. More recently in the Software Development space but also SysAdmin, Server Infrastructure, Databases and DevOps.\nIn my spare time I have created Pwned Pass a Xamarin Forms mobile app that makes use of the Have I Been Pwned? API to allow searching for breached emails and passwords. This is available from the Google Play store.\nCertifications \nTech I have used  .Net C# SQL Server Blazor Javascript jquery Azure Azure DevOps AWS Terraform Powershell git plus many others\u0026hellip;  Recommendations Simon is an excellent developer, always looking for new methods and technology to improve existing solutions, along with his own skills. He is open to new ideas, though not afraid to share his thoughts and feedback and guide others to the best solution possible. Itâ€™s worth noting that Simon has vast knowledge on hardware and infrastructure which really does compliment his skill as a developer. - Alasdair Thomson (Head of Business Data Solutions at NPD Travel Retail)\nSimon worked for my company for a number of years and was a very valued member of staff, and later on management. He is an intelligent and conscientious worker who gives over and above on a regular basis. He leads by example and was respected by his peers and the directors of the company. I would not hesitate to recommend Simon both in terms of the quality of his work and his ethics and values in life in general. - Alison Davies (CEO at Eurosafe UK)\nIf you want a developer who really understands DevOps, then Si is your man. With a mixture of operations, support, development and management experience, he can bring a balanced skillset to a team, as well as excellent knowledge of Azure, .NET, SQL and the web. - Keith Williams (Development Manager at IRIS for Cascade HR and KashFlow Payroll)\n","date":"Jan 1, 0001","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/09/1922276.jpg","permalink":"https://www.funkysi1701.com/posts/about/","series":null,"tags":null,"title":"About Me"},{"categories":null,"content":"Using the data supplied by Troy Hunt and his Have I been pwned? website Pwned Pass allows you to check to see if any password has appeared in a data breach.\nFor more details about Have I been pwned? check out haveibeenpwned.com and www.troyhunt.com.\nPwned Pass is a simple Xamarin app that allows you to type in a password and tells you if it has been used in a data breach.\nTroy Hunt of Have I Been Pwned? recently added a new API to his website which allows you to search his extensive database of pwned passwords, over 306 million of them. I have simply created a Android frontend to this API.\nIt should be noted:\u0026nbsp;Do not send any password you actively use to a third-party service â€“ even mine! I donâ€™t log anything that you type into my app and this app makes use of the\u0026nbsp;k-anonymity feature to avoid transmitting passwords.\nAs well as using Troyâ€™s new API I also take advantage of his existing APIâ€™s. You can search his extensive database of email addresses to see if you have been affected by a data breach all from my android app.\nPwned Pass is now available from the Google Play Store.\n\n","date":"Jan 1, 0001","img":"","permalink":"https://www.funkysi1701.com/posts/pwned-pass/","series":null,"tags":null,"title":"Pwned Pass"}]