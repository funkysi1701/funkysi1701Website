[{"categories":null,"content":"I have been wanting to produce a diagram of the architecture of my side project for some time, but I have put it off as never sure what the correct tools is for this job.\nMermaid is a tool that lets you create diagrams from code and text. I first came across this tool about a year ago for use in a project. I had forgotten about this tool until the other day, when I was thinking about this problem again.\nIs it possible to embed a mermaid diagram in the markdown used in a github repo? Well the answer is yes, so lets look at how that works.\nA simple mermaid diagram looks like this:\ngraph TD; A[fff]---\u0026gt;B; A---\u0026gt;C; B---\u0026gt;D; C---\u0026gt;D; which renders like this:\nLets look at what it is doing.\nTD means the chart is top down.\nA is the name of a node in the chart, [fff] is a label being applied to it.\nThen we just define the relationships between the different nodes, you can have \u0026lt;\u0026mdash;, \u0026mdash;\u0026gt; or \u0026mdash;, or even \u0026lt;\u0026mdash;\u0026gt;\nLets look at my architecture. I have a database (cosmosDB), I have a website running on Azure Static Web Apps, I have some Azure functions for getting data into and out of my database. I also have Application Insights monitoring the whole thing. I also have a console app for doing some data import stuff. This produces a diagram like this:\nThe code to produce this and display it on my github repo is simply\n```mermaid graph TD A[Azure Static Web App]---B[Http Fn] B---D[Database] C[Timer Fn]---D D---E[Import Console App] F[App Insights]---A F---B F---C Pretty nice for a few lines of code to show what system talks with what.\nThere are improvements that can be made to the diagram, for example changing the shapes of the different services.\neg If you live in the AzureDevOps world you can add a mermaid diagram to your wiki pages (but I don\u0026rsquo;t think to your markdown files). Just use the following syntax\n::: mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? John--\u0026gt;\u0026gt;Alice: Great! Alice-)John: See you later! ::: For more info see the docs here What diagrams are you going to build?\n","date":"Jun 19, 2022","img":"https://www.funkysi1701.com/images/mermaid2.png","permalink":"https://www.funkysi1701.com/posts/2022/diagrams-with-mermaid/","series":null,"tags":["github","programming","mermaid","documentation"],"title":"Diagrams with Mermaid"},{"categories":null,"content":"On the 10th and 11th June 2022 Scottish Summit 2022 happened and I was there. It was an awesome event and I will attempt to summarise what happened.\nThursday 9th On Thursday 9th I travelled down on the train full of excitement about the event. Once I got to the hotel, I was immediately greeted by James Cook , I was obviously in the right place, this weekend is going to be good.\nFriday 10th The hotel was less then 5 mins walk to the conference, which was being held at the Technology and Innovation Centre, University of Strathclyde. Registration was simple wave your tickets (which were free, apart from a contribution to charity), and you get your badge and a wrist band. (Pink for people contact, blue for hold back) I then got chatting to Keith Atherton , who amazingly lived in part of the world where I grew up. (Amazing what you learn at conferences!)\nTime for the Keynote talk. Which of course because we are in scotland was introduced by bagpipes marching through the room! The talk was given by Connell McGinley and was titled Disability Awareness \u0026amp; Inclusivity. This talk was amazing. Connell is deaf and gave the entire talk with sign language and this was translated for the rest of us to hear. Connell talked about his career and the barriers he faced. Microsoft Teams was an amazing improvement to his work life, as the captioning allowed him to talk with customers for the first time!\nThe purple pound is the amount of spending power people with some kind of disability (15% of the worlds population) have and it is approx $13 trillion. If businesses can open their products up to more people it is a win for everyone. This was a really powerful way to open up the conference.\nNext up was David Small talking about Azure DevOps pipelines and Power Platform. I don\u0026rsquo;t know much about Power Platform however all the DevOps stuff was great, some of it reinforced stuff I already knew, some was new.\nSam Cogan was next talking about Infrastructure as Code. Resources for this talk are on his github This was a great comparison of different tools for doing Infrastructure as Code. Terraform, Pulumi, Bicep, ARM templates where all covered. The difference between stateless and stateful tools. If you have a state file you need to look after it, so if you don\u0026rsquo;t need this think about using a stateless tool. If you are only using Azure consider Azure. If you are a C# (other languages are available) think about using Pulumi (which I have used and quite like)\nGetting Started with Static Web Apps was next with Poornima Nayar this was a great overview of Static Web Apps but as I already use Static Web Apps this was mainly reinforcing what I already knew. However the Static Web App CLI was mentioned which I didn\u0026rsquo;t know about so I am going to spend some time looking at that! Another thing that was mentioned was GraphQL which is a language agnostic way of building APIs which I also want to learn more about. Resources for this talk can be found here Next was Naming stuff in Azure with Barbara Forbes , she went through the challenges of getting naming right, especially with all the exceptions Azure throws into the mix. Tagging your resources can really help. Also Azure Policy can help report on what you have (and even enforce it, if you are careful). Lots of interesting stuff, as my naming strategy is non existent at the moment!\nDefender for Cloud Apps with Charlie Gough . This was his first talk and he did really well despite technical problems. This was a security talk, I thought it was going to be about securing the apps you have built, but it was more about securing the apps that the users in your company are using. Still useful to know about, but not really my thing. Sorry Charlie!\nLast talk of the day was \u0026lsquo;What I didn\u0026rsquo;t know about Office 365\u0026rsquo; with Sharon Weaver Sharon went through Shifts, Planner and Forms all included in your Office 365 subscription.\nSaturday 11th To start the second day I had Maxim Salnikov talk about Static Web apps, if I had know it was about Static Web Apps again I may have chosen a different talk but this was still a great talk. Again the CLI was mentioned which I really need to investigate further.\nNext up was James Cook\u0026rsquo;s talk Monitor Azure DevOps and GitHub with Sentinel. This was all about securing your code repositories with Sentinel and looking through the logs at what is happening. To use this tech you need either a GitHub Enterprise subscription or Azure DevOps using Azure AD Logins as it is designed more for the Enterprise than a one man band kind like me, however I will see if I can have a play around with it and see what I can learn.\nSarah Lean gave a talk about the state of IT Ops tools in the hybrid age. I enjoyed this talk, Sarah added extra fun elements like talking about the Scottish language and the highland cow. The first tool mentioned was Windows Admin Centre. I have heard about this but never used it, must have come out after I switched from Ops to Dev. Looks very Azure like. Azure Update Management was mentioned next, which I have used and works really well. This is the next generation of Update Management, a bit like WSUS from my server days. Azure Policy was mentioned again, a great tool for reporting and/or enforcing things. Azure Arc was mentioned, a way to bring Azure to you. With all these tools consider why you want to use them, what benefits do they bring, not just because it it shiny!\nMental Health with Rory Neary . This was one of my favourite talks. Rory talked about his mental health problems, his challenges, some of his coping mechanisms some of which involved the power platform. He encouraged us all to be more open about talking about Mental Health. Lots of talks have a solution to a problem, this one didn\u0026rsquo;t, but that Ok. We are all on a journey complete with its ups and downs.\nAccessibility for Blazor with Dennie Declercq . I wasn\u0026rsquo;t as keen on this talk as some of the others, while I agree Accessibility is very important the solutions presented felt very generic and not really specific to Blazor. However I do have some ideas to try now and the accessibility browser extension sounds like a great idea.\nAfter Hours One of the best things about the conference was talking to people between sessions and afterwards. I am an introvert and tend to keep quiet but even I managed to talk to (for me) loads of people. Some of these I had previously connected to on twitter, and some were brand new on the day. I would encourage anyone reading this to reach out to the community, go to an event like this, you will be surprised how friendly and helpful people can be.\n","date":"Jun 14, 2022","img":"https://www.funkysi1701.com/images/scottishsummit.jpg","permalink":"https://www.funkysi1701.com/posts/2022/scottishsummit/","series":null,"tags":["conference","programming","microsoft","community"],"title":"Scottish Summit 2022"},{"categories":null,"content":"My 6 year old boy has been playing with Scratch, so here is a quick introduction to what we have learnt about it.\nScratch is a website that allows you to \u0026ldquo;code\u0026rdquo; by combining blocks together.\nThis is how the scratch website describes its self\nScratch is the world’s largest coding community for children and a coding language with a simple visual interface that allows young people to create digital stories, games, and animations. Scratch is designed, developed, and moderated by the Scratch Foundation, a nonprofit organization. Scratch promotes computational thinking and problem solving skills; creative teaching and learning; self-expression and collaboration; and equity in computing. Scratch is always free and is available in more than 70 languages.\nScratch can be found by going to https://scratch.mit.edu/projects/editor/ A lot of the blocks affect your character or \u0026ldquo;sprite\u0026rdquo;, the default is a cute looking cat which may also be the scratch logo.\nThe left hand side of the screen consists of a list of the blocks of code you can use and your coding area where you can arrange them.\nThe right hand side shows the results of your code, where you can run and test what your code does.\nIt didn\u0026rsquo;t take my boy long to code something like this tonight.\nThis game moves the \u0026ldquo;sprite\u0026rdquo; towards the mouse pointer and a message \u0026ldquo;got you\u0026rdquo; appears on screen when it reaches your mouse pointer.\nI won\u0026rsquo;t go through all the different blocks you can choose from, but I will highlight some of the programming concepts being learnt here.\ninputs every computer program needs inputs and there are lots to choose from, things like moving the mouse or pressing a key on the keyboard.\noutputs your code needs to do something, this code moves a sprite about the screen\nvariables my boy hasn\u0026rsquo;t heard of algebra yet, but he is already grasping concepts like variables, where a value can be assigned and maths can be applied to. In this example the score increases\nloops this program keeps looping until it reaches the mouse pointer\nlogic other logic commands are also available, like if / else statements, also \u0026lt;, \u0026gt; and = statements can be used\nThere are lots of help and tutorials on the site to help you along, and you can customise the look and even record short sound clips to play in your code.\n","date":"Apr 26, 2022","img":"https://www.funkysi1701.com/images/scratch.png","permalink":"https://www.funkysi1701.com/posts/2022/scratch/","series":null,"tags":["Scratch","programming","kids"],"title":"Scratch"},{"categories":null,"content":"Earlier this year I was a guest on the Temporal Trek Podcast.\nYour Inquiry is recognised.\nHere is this weeks episode of The Temporal Trek.\n“Dead Stop” part 2 of the unofficial Battle Damage two parter.\nSpecial Guest @funkysi1701 who was paid with three containers of warp plasma. Bargain!https://t.co/up4zAKOpZy pic.twitter.com/0EUTAWmqgl\n\u0026mdash; The Temporal Trek Podcast(s) (@rider_coattail) March 18, 2022 Here is the episode I recorded: Temporal Trek Podcast Season 3 episode 30 Dead Stop This week \u0026ldquo;Dan Dan\u0026rdquo; are joined by long time friend of the show, formerly from The Ten Forward Podcast, Simon Foster.\nWe discuss whether this is\nA part two of a two-parter, last of a four parter or a previosuly unseen part 5 to the Tessek Prime Quintology? Rogue full-AI or just a really super advanced computer? Station seen again in Voyager? Oh and by the way has anyone seen the guy who flies the ship, you know the one, tall bloke, loves a good prank, Space Boomer, answers to \u0026lsquo;Travis\u0026rsquo;?\nIt was great fun to record and I hope you enjoy giving it a listen.\n","date":"Mar 18, 2022","img":"https://www.funkysi1701.com/images/podcast.jpg","permalink":"https://www.funkysi1701.com/posts/2022/temporal-trek-podcast/","series":null,"tags":["StarTrek","podcast"],"title":"Temporal Trek Podcast"},{"categories":null,"content":"So today I sat the AWS Cloud Practitioner Certification Exam and passed! This is my second certification exam I have sat after the Azure Fundamentals one I sat last year.\nThis time was different than the Azure one. Colleagues at work encouraged me to look into doing this particular exam and my employer is going to cover the cost of the exam itself. Last time I did the exam in my own time and didn\u0026rsquo;t really discuss it with anyone from work.\nThe other difference was this time I went to a testing centre. The centre was a 5 minute walk from the office. So I decided to work from the office today and 20 minutes before the exam started I walked to the centre. I signed a couple of bits of paper, had my photo taken, had my passport and driving license laughed at, put my belongings in a locker and I was free to start the exam. No worrying about internet connectivity or kids making noise outside my room, or the endless photos at the prep stage of an exam from home. I think I prefer the exam centre experience.\nThe other difference was the first exam was Azure, this one was AWS. Both exams cover similar concepts, they are both introductions to Cloud Computing. It\u0026rsquo;s ten months since I did the Azure one, however they both had multiple choice questions and gave you the results at the end. I feel overly familir with Azure, while I don\u0026rsquo;t feel that was with AWS yet. A lot of questions relied on you knowing what AWS services are called. eg AWS Pipeline, AWS CodeStar, AWS CodeBuild, Amazon CodeGuru are all AWS services and are easily mixed up, especially as I have only used a few AWS services, while I have used a lot more of the Azure ones.\nMy memory is the Azure gave you a breakdown of your score at the end, while I am still waiting for that from todays exam. This difference may of course be due to it being in a testing centre.\nI still prefer Azure to AWS, however today has proven that I can learn AWS stuff as well. For my next exam I want to get more hands on with my learning, as most of the services I have learnt about so far I haven\u0026rsquo;t seen in action.\n","date":"Mar 17, 2022","img":"https://www.funkysi1701.com/images/aws.png","permalink":"https://www.funkysi1701.com/posts/2022/aws-cloud-practitioner/","series":null,"tags":["AWS","Certification"],"title":"AWS Cloud Practitioner"},{"categories":null,"content":"This morning I was listening to a podcast where the new features coming out for SQL Server 2022 were being discussed. This starting me thinking about what would be involved in upgrading.\nUpgrading production environments is complex and there are licensing considerations to take into account. However for non production workloads like development this isn\u0026rsquo;t a problem so lets look at that first.\nIn the past I have installed SQL Server Devloper Edition onto my laptop, this is fine but I have found that unless you are very careful you may end up with multiple different versions of SQL Server sitting around, and it is difficult to cleanly remove them without a fresh install of the OS.\nHowever in this day and age, Docker and Containers are king. My current development environment makes use of Docker and has a docker compose file which sets up SQL Server for this particular application, lets take a look.\nsqlserver: image: mcr.microsoft.com/mssql/server:2019-latest container_name: Sql ports: - \u0026#34;5432:1433\u0026#34; networks: - my-network volumes: - sqlvolume:/var/opt/mssql This defines which docker image to use, in this case 2019-latest, sets up ports and the name and saves the data on a docker volume.\nIf we then run SELECT @@VERSION on this instance of SQL Server we get told:\nMicrosoft SQL Server 2019 (RTM-CU13) (KB5005679) - 15.0.4178.1 (X64) Sep 23 2021 16:47:49 Copyright (C) 2019 Microsoft Corporation Developer Edition (64-bit) on Linux (Ubuntu 20.04.3 LTS) \u0026lt;X64\u0026gt; What if we change the docker-compose file to use 2022-latest?\nmanifest for mcr.microsoft.com/mssql/server:2022-latest not found: manifest unknown: manifest tagged by \u0026#34;2022-latest\u0026#34; is not found SQL Server 2022 hasn\u0026rsquo;t been released yet so there is no docker image for it yet. Try this command again in a few months when it is available.\nOK so what else can we try? What about a downgrade to 2017-latest? Will that work?\nSQL Server 2017 starts but the following error gets logged.\n2022-02-23 21:41:15.30 Server Software Usage Metrics is disabled. 2022-02-23 21:41:15.30 spid6s Starting up database \u0026#39;master\u0026#39;. 2022-02-23 21:41:15.34 spid6s Error: 948, Severity: 20, State: 1. 2022-02-23 21:41:15.34 spid6s The database \u0026#39;master\u0026#39; cannot be opened because it is version 904. This server supports version 869 and earlier. A downgrade path is not supported. Doh we can\u0026rsquo;t downgrade the existing database we have. Probably a good thing really.\nMicrosoft release regular updates for SQL Server called CU\u0026rsquo;s (Cumulative Updates), you can see above we are on CU13. Is there a CU14 or CU15 we could try?\nUpdate the docker compose to: mcr.microsoft.com/mssql/server:2019-CU14-ubuntu-20.04\nAt this point I actually got an error\n2022-02-23 21:50:20.71 Server Error: 17058, Severity: 16, State: 1. 2022-02-23 21:50:20.71 Server initerrlog: Could not open error log file \u0026#39;/var/opt/mssql/log/errorlog\u0026#39;. Operating system error = 5(Access is denied.). This is caused by trying to use SQL Server 2017 but it is easy to fix.\nIn docker desktop there is a volumes section, find the volume you are using with SQL Server and delete the errorlog mentioned above.\nNow if you retry SQL will start OK.\nRepeating the SELECT @@VERSION gives us a new CU\nMicrosoft SQL Server 2019 (RTM-CU14) (KB5007182) - 15.0.4188.2 (X64) Nov 3 2021 19:19:51 Copyright (C) 2019 Microsoft Corporation Developer Edition (64-bit) on Linux (Ubuntu 20.04.3 LTS) \u0026lt;X64\u0026gt; Microsoft SQL Server 2019 (RTM-CU15) (KB5008996) - 15.0.4198.2 (X64) Jan 12 2022 22:30:08 Copyright (C) 2019 Microsoft Corporation Developer Edition (64-bit) on Linux (Ubuntu 20.04.3 LTS) \u0026lt;X64\u0026gt; How much easier is this than manually installing updates and rebooting or attempting to uninstall and reinstall SQL Server. As SQL Server 2022 isn\u0026rsquo;t out yet I can\u0026rsquo;t say for certain what issues I may encounter but hopefully it will be as easier as this. And I don\u0026rsquo;t need to backup or restore and databases they are all available as before!\n","date":"Feb 23, 2022","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2022/header-01.png","permalink":"https://www.funkysi1701.com/posts/2022/updating-sqlserver-with-docker/","series":null,"tags":["Docker","SQL"],"title":"Updating SQL Server with Docker"},{"categories":null,"content":"Today Microsoft celebrated 20 years since the first version of dotnet was released with a special live stream event.\nA lot has happened in the 20 years of dotnet (or .Net). It is my understanding that .Net was created to rival Java. When the .Net Framework was first created it was a windows only thing, but today modern .Net is a modern run anywhere platform, with .net applications running everywhere from PCs and Laptops, Raspberry Pi\u0026rsquo;s, Mobile phones and tablets (via Xamarin Forms), to Websites and Microservice APIs running on every Cloud platform out there.\nIf you want to celebrate this achievment tweet with the hashtag #DotNetLovesMe or download some of the digital swag available from github On Thursday the 17th Feb the first preview of dotnet 7.0 is going to be released, with the latest version 6.0 only being released last November.\nThe above graphic comes from the following tweet.\nThis is 20 years of #dotnet releases. #dotNETLovesMe pic.twitter.com/Zxfe1SdWTq\n\u0026mdash; Khalid 🎟 (@buhakmeh) February 14, 2022 What do you like about dotnet? When did you first start using it? What are you going to build with it next?\n","date":"Feb 14, 2022","img":"https://www.funkysi1701.com/images/FLjBrnPXwAQE8BN.jfif","permalink":"https://www.funkysi1701.com/posts/2022/dotnet-is-20-years-old/","series":null,"tags":["DotNet"],"title":"dotnet is 20 years old"},{"categories":null,"content":"Is it to share my ideas? Is it to learn new technologies and techniques? Is it to create a following? Is it to educate others? Is it to build some kind of service? Is it some combination of all of these.\nHistory Back when the web was young and I was first learning HTML. I hand crafted web pages, adding photos I had taken with captions. If I needed a new page I just added a new html page and linked to it from another page.\nAs time went on I started to learn mysql and php and my website became a hand crafted php nightmare. I also applied what I learned to help my father run the website for his camera club.\nAt some point I started playing with WordPress. I have had various WordPress websites or blogs over the years. WordPress is very powerful you can do so many things, install so many plugins. WordPress runs on php and mysql and as my career started to centre around the .net space, I started to want something that was similar, so I could apply things I had learnt to my own website.\nThis has led me to the current state of my website. I have a WordPress blog, with most of my oldest content, my newer content lives on dev.to and I have a Blazor webassembly site that uses the dev.to api to run my new website.\nBlazor webassembly is great, however it has some limitations which I am starting to push against. To host this as cheaply as possible I am using Azure static web apps, so no .net backend all the website is front end. I have some Azure Functions that does the backend bits that I need.\nGoogle and other bots are not able to find any of my pages except index, due to the way Blazor works. I have got round this by pre rendering the content using https://prerender.io/ My next difficulty is how to generate a sitemap.xml or a rss feed for my blog. This has started to make me question my architecture decisions.\nI could use a hosted solution like ghost which is popular with a some of my peers. This would solve many of the problems I am currently facing but I wouldn\u0026rsquo;t be able to play with everything as it is hosted and therefore someone else\u0026rsquo;s problem. How important that is I will look at later.\nAnother option would be to use github pages, there are quite a few ways to publish a github page, Jekyll and Hugo appear to be the most popular. Both produce static content and both are a new for me to learn. Interestingly I could also publish either to Azure Static Web apps if github pages ends up not being suitable.\nSplit in two I think my website needs to be split in two. I need a stable blog platform probably using Hugo and github pages. This is what I want to get indexed by the search engines and be the primary way people find out about what I am doing.\nI then have additional sites, that I use as my playground for learning new tech. I can easily link between them and I can tweak the style so they \u0026ldquo;fit\u0026rdquo; nicely together.\nI am still considering what to do with dev.to. I like that I am using it as the backend for my blog posts, and its API gives me that flexibility to display that content where I want.\n","date":"Jan 25, 2022","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2022/WHAT-IS-THE-CORPORATE-WEBSITE.jpg","permalink":"https://www.funkysi1701.com/posts/2022/why-do-i-have-a-website/","series":null,"tags":["website",""],"title":"Why do I have a website?"},{"categories":null,"content":"I\u0026rsquo;ve been running my website on Azure Static Web Apps for a while and it is pretty cool.\nWhen you create a Static Web App on Azure you get asked for the github repo of your source code and even the branch to use. Once you have selected this, you get asked for the type of code to deploy, mine is Blazor Web Assembly but you can use Angular, React or Vue.\nYou now have three variables to fill in the location in your code of the Website, the location of your Azure Functions and the output location usually wwwroot. Once you have set these three you can preview the GitHub Actions file that will be created and added to your repository.\nI get something like this\nname: Azure Static Web Apps CI/CD on: push: branches: - feature/tempbranch pull_request: types: [opened, synchronize, reopened, closed] branches: - feature/tempbranch jobs: build_and_deploy_job: if: github.event_name == \u0026#39;push\u0026#39; || (github.event_name == \u0026#39;pull_request\u0026#39; \u0026amp;\u0026amp; github.event.action != \u0026#39;closed\u0026#39;) runs-on: ubuntu-latest name: Build and Deploy Job steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_\u0026lt;GENERATED_HOSTNAME\u0026gt; }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Client\u0026#34; # App source code path api_location: \u0026#34;Api\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### close_pull_request_job: if: github.event_name == \u0026#39;pull_request\u0026#39; \u0026amp;\u0026amp; github.event.action == \u0026#39;closed\u0026#39; runs-on: ubuntu-latest name: Close Pull Request Job steps: - name: Close Pull Request id: closepullrequest uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_\u0026lt;GENERATED_HOSTNAME\u0026gt; }} action: \u0026#34;close\u0026#34; This github action will run when you create a Pull Request to the branch mentioned in the file, or if you push code into the branch. This code get added into the .github/workflows/ folder and is the location that all github action workflows live.\nI haven\u0026rsquo;t done much with github actions, however I have used Azure DevOps quite a bit. Over on the Azure DevOps side I have created a pipeline that deploys to a Dev environment, then a Test environment and finally a production environment.\nLets have a look at the workflow that I ended up with and with can break down how it all works. Note I am new to Github actions so if there is a better way of doing this do let me know.\nname: Azure Static Web Apps on: push: branches: - main - develop - feature/* jobs: dev: runs-on: ubuntu-latest environment: name: Dev name: Dev steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_ORANGE_POND_09B18B903 }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Blog\u0026#34; # App source code path api_location: \u0026#34;Blog.Func\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### test: if: github.ref == \u0026#39;refs/heads/develop\u0026#39; runs-on: ubuntu-latest environment: name: Test name: Test steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_WITTY_DUNE_0A1A77903 }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Blog\u0026#34; # App source code path api_location: \u0026#34;Blog.Func\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### prod: if: github.ref == \u0026#39;refs/heads/main\u0026#39; runs-on: ubuntu-latest environment: name: Prod name: Prod steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_BRAVE_ROCK_0AAC63D03 }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;Blog\u0026#34; # App source code path api_location: \u0026#34;Blog.Func\u0026#34; # Api source code path - optional output_location: \u0026#34;wwwroot\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### The first thing I did was create three Azure Static Web Apps, I am using the free tier so while this is trippling my costs it is all still free! Doing this created three github action workflow files, I deleted two and edited the third, but before I deleted them I made a note of the AZURE_STATIC_WEB_APPS_API_TOKEN. If you look in your settings -\u0026gt; secrets for your repo you will see secrets have been created, this is the secure token that github uses to update your static web app.\nWhile we are in settings we might as well look at environments. I created a Prod, Test and Dev environment that I was going to use in my github actions.\nEnvironments can have various rules setup on them.\nRequired reviewers - this is like an approver, a user specified here must aprove for the workflow to be deployed Wait time - I didn\u0026rsquo;t use this, but it looks like a certain amount of time can be set to pause the deployment. (I assume to do some kind of manual check) Deployment Branch - specify what branch are allowed to be deployed to what environments. I specified develop, main and feature branches could be deployed to the Dev environment, develop and main could go on Test and main could go on Prod Environment secrets - I didn\u0026rsquo;t use this as my secrets were already created, however it looks like your secrets can be associated with a specific environment Now that we have the static web apps setup and the environments lets look at the github action file.\nFirst of all I removed the PR stuff and just concentrated on pushes. I wanted my workflow to be.\nPush to feature branch Deploys to Dev env PR feature branch to develop Once merged code gets pushed into develop Deploys to Test env PR develop to main Once merged code gets pushed into main Deploys to Prod env (after approval) The approval on deploying to production I think is probably overkill, but I still have it setup like that for now.\nMy gh action has three jobs defined as dev: test: and prod: they are all the same except they have the azure_static_web_apps_api_token that is correct for their environment.\nThey also each have a environment defined eg\nenvironment: name: Prod Lastly Test and Prod have an if test setup, if the test is false the job won\u0026rsquo;t run. Importantly it won\u0026rsquo;t fail it just won\u0026rsquo;t run.\nFor Prod this needs to only run on main branch so we have\nif: github.ref == \u0026lsquo;refs/heads/main\u0026rsquo;\nFor Test this needs to only run on develop so\nif: github.ref == \u0026lsquo;refs/heads/develop\u0026rsquo;\nI could have a test for develop to only run on feature/* but I have allowed it to run everytime.\nThere is loads more you can do with github actions, but hopefully this gives you a taste of some of the things you can do. I currently have a mix of Azure DevOps and github actions so I will be working on getting github actions to do more.\n","date":"Jan 10, 2022","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bj77rx0jetjdf7c24nhk.png","permalink":"https://www.funkysi1701.com/posts/2022/using-github-actions/","series":null,"tags":["GitHub","DevOps"],"title":"Using GitHub Actions"},{"categories":null,"content":"Lets have a look at what my goals were for 2021. I had eight of them, lets look at them one by one.\nAzure certification. In May of this year I sat and passed the Azure Fundamentals exam. I am calling this goal as achieved.\nMentoring. I didn\u0026rsquo;t do anything about working with others or mentoring, so not sure I achieved this one. However I do now work in a development team and I have been reviewing others code and having my own reviewed. We have a junior developer and I am enjoying the opportunities I have to work with him and share my wisdom.\nF#. I have done zero work with F# in 2021, so this one I didn\u0026rsquo;t achieve.\nCosmos db/Mongo db. I have worked with both of these technologies in 2021. I used Cosmos the most with my website, and storing data for it. I used Mongo/Atlas for auditing for a project I did for my previous job. I wouldn\u0026rsquo;t say I was expert in either of these, but I am starting to get a flavour of non SQL Server databases. I should also note that I am using mysql a lot in my latest job, so another non SQL Server technology that I get to use on a daily basis.\nGive a talk. I said last year that maybe I would make baby steps towards doing this, and I have. In the interview for my new job I gave a short presentation (I thought it was bad, but others didn\u0026rsquo;t!) I also gave a short introduction to Blazor talk which went down well.\nMandlebrot Generator. Did nothing on this one as well. I may have googled the code but that is as far as I got.\nPwned Pass Mobile App. I got an increase in users in 2021 and hence it it still running and I still pay for the API key. I am still considering what to do with it.\nTime for me. Achieved this one, had plenty of time for myself.\nNot a bad year but what are my goals for 2022? Video. I have just purchased a green screen, so my first goal is to learn how to use OBS, how to light myself properly without getting horrible reflections. I have the content for my first talk, the talk I gave at work about Blazor.\nConference. Attend an in person conference. I am booked to attend Scottish Summit, however it has been postponed due to COVID-19.\nBlog More. I have been neglecting writing blog posts a bit recently, so I want to do more.\nMetrics. I have been recording various metrics from twitter, github etc and I would like to expand this and make it a service.\nProfile Pic. My profile pic is getting a bit old, so it would be nice to update the images I use online and improve my personal brand.\n","date":"Jan 1, 2022","img":"","permalink":"https://www.funkysi1701.com/posts/2022/2022-goals/","series":null,"tags":["Goals"],"title":"2022 Goals"},{"categories":null,"content":"How exciting my Lynx Computer from my childhood has come home, all 96k of it pic.twitter.com/aeN38KBiS8\n\u0026mdash; Simon Foster (@funkysi1701) December 26, 2021 I can\u0026rsquo;t remember the syntax for BASIC, luckily I have been able to find the Manual .\nAll the commands are listed inside so lets see what we can do.\nThe Lynx presents you with a command prompt in which you can type text. Back in the 80s we had a tape player to load programs from tape, however I don\u0026rsquo;t have one today so only programs I write can be run.\nPRINT - To write Hello World, you can just type PRINT \u0026ldquo;Hello World\u0026rdquo; and Hello World appears on the screen. To Write a program that displays Hello World, you just write the line number first.\n10 PRINT \u0026#34;Hello World\u0026#34; To run this you type RUN To view the code you type LIST\nTo Edit a specific Line you can use Ctrl+E and type the line number, or you can just write the line out again.\nCLS - This command clears the screen\nINPUT N - stores text typed by the user and stores it in the variable N\nGOTO N - Execution of code continues at Line Number N\nThe first Program I wrote with a bit of help from my boys.\n10 CLS 20 PRINT \u0026#34;What is your Age?\u0026#34; 30 INPUT N 40 IF N\u0026gt;5 AND N\u0026lt;41 THEN PRINT \u0026#34;a good age\u0026#34; 50 ELSE IF N\u0026lt;6 THEN PRINT \u0026#34;a spaceman\u0026#34; 60 ELSE IF N\u0026gt;40 THEN PRINT \u0026#34;too old\u0026#34; My 4yo didn\u0026rsquo;t like being \u0026ldquo;too young\u0026rdquo; in the original version, so my 6yo helped me change him to be a \u0026ldquo;spaceman\u0026rdquo;.\nNot bad and it was fun pair programming with a 6yo, all my typos were quickly spotted, and he easily understood the logic of IF/ELSE/THEN statements.\nThe Lynx comes from 1983 and has just 96k of memory. I am very lucky it actually still works, however I have been able to find an emulator so I can write Lynx BASIC from the comfort of my laptop. jynxemulator , it is also on github but it doesn\u0026rsquo;t include the ROMs so getting from the website is a better option.\nThe developer experience today is so much nicer than it must have been in the 1980s, however back then distractions must have been much reduced.\nNo internet or google to get answers to your questions No Copy/Paste of text No Load/Save (unless you have a working disk drive or tape player!) No IDE No Build or Release process just type RUN I then have additional sites, that I use as my playground for learning new tech. I can easily link between them and I can tweak the style so they \u0026ldquo;fit\u0026rdquo; nicely together.\nI am still considering what to do with dev.to. I like that I am using it as the backend for my blog posts, and its API gives me that flexibility to display that content where I want.\n","date":"Dec 28, 2021","img":"https://www.funkysi1701.com/images/FHi_NyOXEAo9YbG.jfif","permalink":"https://www.funkysi1701.com/posts/2021/back-to-basic/","series":null,"tags":["BASIC","History","Programming","Lynx"],"title":"Back to BASIC"},{"categories":null,"content":"The final episode of Star Trek: The Next Generation features a few scenes set 25 years into the future. That episode aired May 1994. The newest Star Trek TV show Star Trek Picard has just aired its first episode and this is 25 years and 8 months after that episode. This article may contain minor spoilers for the first episode of Star Trek Picard, you have been warned.\nI thought it would be interesting to compare the two time periods.\nFate of Picards crew In All Good Things\u0026hellip; all of Picards crew are still alive except one. Deanna Troi passed away but it is never revealed how.\nIn Star Trek Picard, we know Data has died as featured in the feature file Star Trek Nemesis. From the trailers we know Riker and Troi are still alive. We do not know the fate of Crusher, LaForge and Worf yet, but I am going to assume they are still alive somewhere.\nIn both timelines we have one dead crew member.\nLiving arrangements In both time lines Picard is living in France at his vineyard.\nIn All Good Things\u0026hellip; we do not know who Picard is living with, it is assumed he lives alone, which is why Geordi feels the need to check on him, once he learns about his Irumodic Syndrome.\nIn Star Trek Picard we learn that Picard is living with a Romulan couple, due to the involvement Picard made to save the Romulan people when their star went supernova. Picard appears older but for the most part in good health, he can\u0026rsquo;t for example run up a flight of stairs (he is 92 so not surprising really!) and is plagued by dreams from his past.\nRomulan Neutral Zone In All Good Things\u0026hellip; there is no Neutral Zone, the Romulan empire has been taken over by the Klingons.\nIn Star Trek Picard one would assume there is no Neutral Zone as well because the Romulan star system was destroyed. A handful of Romulans survived mostly due to the actions of Picard.\nLooks As we have seen Patrick Stewart does not appear to be ageing. Below is a comparison of how he looks in the two time frames.\nI really enjoyed the first episode of Star Trek Picard and it is going to be interesting seeing what other similarities and differences there are between it and All Good Things\u0026hellip;\n","date":"Dec 15, 2021","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2022/picard.jpeg","permalink":"https://www.funkysi1701.com/posts/2021/a-comparison-of-all-good-things-and-star-trek-picard/","series":null,"tags":["StarTrek","Picard"],"title":"A Comparison of All Good Things... and Star Trek Picard"},{"categories":null,"content":"Welcome to Day 5 of the Festive Tech Calendar! Usually at this time of year I like to take the opportunity to look back on the last 12 months, highlight some of my achievements and make a few goals for the new year.\nLike many of you 2021 has been a difficult year, but it has also been a year of change, and I am in a much better place now than at the start of the year.\nTo tell you my 2021 story I first need to give you a bit of a history lesson about my career. So, lets fire up your flux capacitor (or Tardis of similar value) and journey back to 2006.\nBack to 2006 In 2006 I got my first IT job. My housemate, who was working as an IT Manager for a small Health and Safety Company, suggested I come work with him. I was currently working an admin job with no idea what to do with my life, so I said yes.\nSo, in October 2006 I joined as a member of the IT Team. I worked mainly on first line support, but as the IT department was small, I learnt about all sorts of IT things, from fixing printers, setting up the CEOs BlackBerry mobile phone with emails, administering active directory, writing SQL queries to swapping tapes for the weekly backups.\nThe company had many faults which I won\u0026rsquo;t discuss here, but the work was varied, and I was always learning something new so I stayed with the company.\nAt the end of 2010, the IT Manager resigned, shortly followed by the rest of the IT department. In January 2011 I was IT Manager, I had no staff, plenty of IT problems and no clue what I was doing.\nThis was an amazing time for me, I learnt how to interview and hire staff, I learnt more about the different systems we used. It was approximately at this point in time that I switched from being exclusively a SysAdmin to starting to learn Development.\nI had been creating simple websites for a while, mainly using PHP and MySQL. But since I started working in IT I learned more and more about databases, SQL Server and writing T-SQL scripts. I was the companies \u0026ldquo;database guy\u0026rdquo;, so the company naturally asked me to do more and more database work. This led to learning other technologies like MS Access and C# so I could do more and solve more of the company\u0026rsquo;s problems.\nHowever, the company was small, and I would always be dragged in to fix SysAdmin problems so I never managed to spend 100% of my time doing development, so in 2016 I moved on to my first full time development job.\nThe new job was great, but I kept in touch with my old CEO, Ally and helped out with bits of work on the side, writing SQL queries, updating the odd bespoke application.\nBack where it all began In 2019 I started formulating a plan to go back to my old job as a contract developer. To my amazement Ally almost bit my arm off to get me back, agreeing to all my terms. So, in the summer of 2019 I started full time as a contract developer, mostly working from home with the occasionally meeting in the office and supporting the business with IT issues.\nMy second stint with the company was different than my first, I was able to concentrate almost exclusively on development work. As the only developer I was in full control of development, I decided to use dotnet and Blazor.\nLate in 2019 I was given advanced warning that the company was being sold. In Feb 2020 the company did get sold, the new owners kept me on to finish the project I was working on but made me a permanent member of staff. In March 2020, the whole company began to work from home due to COVID-19, with parts of the business being furloughed.\nThis was a difficult time; on top of the stress the whole world was feeling from the global pandemic I was trying to assist with various IT integrations that you would expect when any company gets sold.\nJanuary 2021 This is pretty much where you find me in January 2021. I was working remotely developing internal applications with dotnet/Blazor with the occasional Teams meeting to demonstrate what I had built so far.\nWorking on your own as a developer is great! You start work and can write code all day. There is no daily stand up, you don\u0026rsquo;t need to explain what you have been working on, or what challenges you are facing. You don\u0026rsquo;t have any process or formality to follow, only what you impose on yourself.\nI created a build/release pipelines in Azure DevOps, I created pull requests that would kick of a build, run my unit tests, and run some static code analysis. I would glance over my PRs but 99% of the time I would approve them.\nThis was the greatest weakness of working on my own, no one to suggest ideas of better ways of writing code, no one to bounce ideas off, no one to encourage or be encouraged when I figured out something clever or offer to help you overcome a problem.\nRedundancy In Mid July I was told that the development function was no longer required, and my job was at risk of redundancy. As I was the only one working in development it sounded very much like a done deal. I had a couple of redundancy meetings to attend and I would need to talk through how various applications worked as part of a handover.\nTo be honest I agreed with the decision to make me redundant. New development work was not coming through, although the users of the applications I had developed loved what I had created, upper management were keen to migrate off these systems and use more centrally managed systems. If I was in charge, I would probably have made a similar difficult decision.\nAs soon as I found out I was at risk of redundancy I called up a couple of recruitment agents I had spoken to before. They told me that it was a great time to be looking for a job and immediately arranged some interviews for the next week. I also took advantage of social media and asked on twitter if anyone was looking for developers like me. To my surprise this resulted in at least one interview.\nOne thing I stressed in all my conversations with recruiters and companies I spoke with, was that I was looking for a team. I wanted to work in a team, bounce ideas off others, mentor others, learn from others. This was the most important thing I wanted in whatever my next role was going to be.\nOther important things for me was continuing to use Azure or similar cloud technologies. However, I always think that the tech stack and technology can be learnt on the job, tech moves so quickly these days that you have to be constantly learning to stay relevant.\nAbout two weeks after I first found out about my impending redundancy I contacted (or more likely was contacted by) a third recruitment agency. It was this third one that eventually got me a job, however all three were brilliant, and kept me updated and answered all of my questions.\nThe next few days were packed with phone calls, interviews, tech tests. I had a lot of different types of companies that I spoke with, from Software Consultancy companies, FinTech companies, Energy suppliers, Legal companies and many others.\nDuring this time of interviews, I can think of only two in person interviews. The rest made use of Teams, Zoom, Google Meet and even Skype (yes at least one company still uses it for video chats!)\nThe usual way the process went was a conversation with the recruiter about a role after which my details were sent to the company, If the company liked the sound of me an initial informal interview was arranged, after that a second more technical interview was held, sometimes there would be a tech test, sometimes it was more technical questions being asked in the interview. Some companies had more stages that this, but this was what I typically encountered.\nI lost count of the actual number of interviews I had, but I used a spreadsheet to try and keep it all straight in my mind. The last thing I wanted was to ask a question about the wrong company!\nPositive No I got plenty of noes from these interviews, however all were phrased as a positive no. That being they liked me as a person but something about my skill or experience wasn\u0026rsquo;t quite right. A friend once described me as an odd mix of junior and senior developer. I agree with that statement, I have a lot of experience in some areas and a real lacking in others.\nA common one was working in a team. Being a lone developer is not good for gaining experience working in a team. However, each time I spoke with companies I stressed that the team was what I was looking for.\nAnother weakness of mine is front end. When I am building something, functionality is more important than getting it looking good. I came from a database and backend start, so it is only natural that I am more at home working on these things. Also, my most recent projects are working with Blazor, which is brand new and few companies are doing much with it yet.\nI tried really hard not to be discouraged by these noes and that the right company was just around the corner. One no hit harder than most. I had an initial chat with the team of a FinTech Company and then there was a second interview with the CTO which I felt had gone OK.\nFrom what I had heard it ticked a lot of boxes, the team sounded good, with lots of support and development opportunities and they were moving to the Cloud, so my Azure experience sounded ideal. The CTO liked me and thought I would be a good fit, but I was part way between a junior and a senior, so he had gone away to try and make a bespoke role for me, eventually it would be a no.\nLooking back, it is easy to see, that this was really encouraging. They tried to create a bespoke role for me! However, at the time all I could think about was all the noes I was hearing.\nThe job for me About the same time, I had my initial interview with the company I would eventually accept a role with. I kept being told that there was a tech test to do, but for some reason it never got sent to me, so the interview featured a lot of tech questions and they really grilled me. I came away without much of a sense of if it went well or not.\nHowever, they liked me and wanted me to do a presentation at their office. I am rubbish at public speaking and doing presentations, so I thought I will do this presentation, but it is very unlikely they will like what I do.\nThe topic of my presentation was how I upskilled a team on a new project or technology. As a lone developer what an earth could I do for a topic like that? I thought about what I had done to upskill myself on various things, however that doesn\u0026rsquo;t really address the question, as they wanted to know more about what I had done to upskill a team.\nI thought back to a time when I helped get a largely un-version controlled codebase into source control and automated the build and release pipeline using Azure and Azure DevOps. This involved talking to SQL and data developers so could be a better option for a presentation.\nI arrived at the office to do my presentation. First of all, I parked next to the factory where all the forklifts were, I was directed to the correct place to park. Then I tried to go to the wrong building, a very helpful employee helped me go to the correct place. I was introduced to the people who would be interviewing me, one of whom I had worked with in the past, who immediately said you should hire this guy, he is an excellent developer! As introductions go, that\u0026rsquo;s not bad!\nIn my opinion the presentation was bad. I didn\u0026rsquo;t have a PowerPoint or similar to go with my talk, so I just waffled on for ten minutes about what I had done a few years back about how I helped get source control being used.\nThe next day I got the job offer. I was very surprised to get the offer as I thought I had done so badly the day before they would give me another positive no. I took a few hours to think about it, however all my other applications were either not started or waiting for the next stage, so I was 99% sure I would accept.\nI had no idea what to expect when I started, as I didn\u0026rsquo;t really ask many questions at the last interview as I was convinced, I was going to get a no after my presentation.\nI worked my notice period and at the start of October I started my new job. My contract only arrived the day before I started, so in the days before I started my mind was making up reasons why the job would disappear.\nThe job started with a two-day induction. Things that were covered on the induction were, mental health, sleep cycles, company values, health and safety and of course a bit of form filling and photocopying passports that you would expect on a first day.\nConclusion Back when I had been job hunting, each time I stressed the importance of team. The company I had joined had an amazing focus on supporting its staff and getting the best from each other. I had got the best possible fit for the kind of team I was looking for.\nUsually when I start a new job, I get a large dose of imposter syndrome. Now, two months after I started, I can say, I didn\u0026rsquo;t really get that this time. Maybe it is how the team functions, maybe it is my level of experience, or maybe its just something else.\nI am happy in my new job. There is a lot to learn, both technology and how to work well in this team. But I am supported, there are people to ask question, I am contributing almost from day one. I have already shared a brief talk about my experience with Blazor, which went down better than I expected. I am excited to learn what is next for me.\nIf you have read all the way to here, Thank you! This is my story and I hope you have found it interesting. If you are looking for a new role, I would encourage you to focus on the soft skills you are looking for next, if you concentrate on team like I did, there is a good chance you will land in an amazing team.\nDon\u0026rsquo;t forget to check out some of the other great content that is being created by all the amazing Festive Calendar 2021 contributors.\n","date":"Dec 5, 2021","img":"https://raw.githubusercontent.com/gsuttie/festivetechcalendar/main/calendar.jpg","permalink":"https://www.funkysi1701.com/posts/2021/lone-developer-to-senior-developer-my-2021-story/","series":null,"tags":["FestiveTechCalendar2021","Career","LifeStory"],"title":"Lone Developer to Senior Developer, my 2021 story"},{"categories":null,"content":"We are over halfway through 2021, let\u0026rsquo;s have a quick look at some things I have done this year:\nDeployed a new version of the Pre Qualification questionnaire website for my employers\nPassed Azure Fundamentals Exam\nMongo DB Atlas - experimented with using it for auditing my application\nSonarCloud - tried to improve the quality of my code\nAzure DevOps API -\u0026gt; built an application to show builds and releases from my Azure DevOps organisation, code is on github\nExperimented with replacing SQL Agent Jobs with Azure Functions, ended up abandoning this project but was a great learning experience\nPlayed with MS Graph - displaying profile pic from Azure AD but I am the only one in my org with a photo so abandoned this as well\nAdded a captch to a website to reduce spam\nCharted my Gas and Electricity usage using the Octopus Energy API\nCharted various metrics from services I use, eg twitter followers, github commits, blog posts published\nStarted learning react, easily connected it with Azure AD\nIntegrated my employers project management system with a postcode API and google maps so can see where in the country different projects are located\nTested dotnet 6 preview and the latest Visual Studio 2022 preview\nI was made redundant and got a new job\nWrote my first piece of dotnet code to run on a Raspberry Pi\nKept cost of Azure down, by removing unused services, refactoring and optimizing existing services.\nWow, I have done loads this year, what is next?\n","date":"Aug 31, 2021","img":"","permalink":"https://www.funkysi1701.com/posts/2021/more-than-halfway-through-2021/","series":null,"tags":["Goals"],"title":"More than halfway through 2021"},{"categories":null,"content":"So today I sat the Azure Fundamentals Certification Exam and passed! Really pleased with myself at achieving this. It was one of my goals for 2021 so I can tick that off.\nBack in January I booked my first exam, however due to technical problems I didn\u0026rsquo;t get as far as the Exam. In order to be able to sit an exam like this you need a webcam and microphone so you can be monitored remotely to check that you are not cheating. Something in my network was blocking them from seeing my video so it got cancelled before it started.\nMy theory is that either my internet connection was playing up at the time, My Pi Hole was blocking something, or something else on my network was blocking the video feed.\nThis cancellation put me off Certifications, as I wasn\u0026rsquo;t sure how to debug the issue and find out exactly what the problem was.\nA few months back I won a free Azure Certification for taking part in a skills challenge run by Gregor Suttie and Richard Hooper and this was due to expire at the end of this month so I thought why not book a second exam and see if I can get past the technical problems. I didn\u0026rsquo;t do any exam prep as I thought, I wouldn\u0026rsquo;t get that far.\nThis time I connected directly to my router, bypassing most of the network, firewall and other items on my network that could possibly cause an issue.\nI cleared my desk, took photos of my ID, took pictures of my desk from every orientation, and waited for the technical issues to start.\nI was in a queue waiting for the exam to start.\nA connection issue has occurred you have been sent to the back of the queue. Oh here we go again!\nBut no I connected with the invigilator, who asked to confirm my monitors were disconnected and to move my wallet out of reach off my desk.\nAnd we are off, I am answering questions from the exam.\nI won\u0026rsquo;t go into detail about the questions, but I whizzed through them, most made me think, some I guessed at. I wasn\u0026rsquo;t expecting to pass, I thought maybe half marks or just under due to my familiarity with Azure. (I have been using it for years which must count for something!)\nI was wrong I passed comfortably and now I have my first certification.\nMy advice for you if you have been using Azure for a while is to book this exam and see how you do, you may well pass like me. There are plenty of opportunities to get a free exam, attending conferences like Ignite often qualify you for one, look out for challenges and competitions by #AzureFamily people on twitter as they are very encouraging and helpful in your Certification journey.\nAnd yes I am thinking about what Certificate to do next!\n","date":"May 11, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eytphopn3inx2x77m9n5.png","permalink":"https://www.funkysi1701.com/posts/2021/my-road-to-certification/","series":null,"tags":["Azure","Certification"],"title":"My road to Certification"},{"categories":null,"content":"I have had a Raspberry Pi for a few years and recently I connected it up again, I plugged in the camera and everything worked.\nTo start off you can view photos from the camera with the raspistill command. With a bit of clever scripting and the crontab I got the Pi taking pictures every 60 seconds. Even managed to take a nice picture of a robin.\nHowever scripting isn\u0026rsquo;t really programming, and I would like to write a bit more code. Dotnet can run everywhere these days and it made sense to see if it would run on a Raspberry Pi.\n@pete_codes has written a nice guide to getting started with dotnet on a Raspberry Pi https://www.petecodes.co.uk/install-and-use-microsoft-dot-net-5-with-the-raspberry-pi/ This guide and the nuget package https://www.nuget.org/packages/Unosquare.Raspberry.IO/ was all I needed to get started taking pictures with my Pi.\nMy initial goal is to take some wildlife pictures, stick my camera to a window and take pictures of what flies/crawls/jumps past the window.\nThe code I have written so far is available on github https://github.com/funkysi1701/RaspberryPiDotNet So far the code takes a picture, uploads this file to Azure Blob Storage (so as not to fill up the Pi with too many image files) and deletes the image locally.\nRun a dotnet publish -c Release and then cron can run dotnet RaspberryPiDotNet.dll (with full paths to the relevant files)\nI then use crontab to execute the code every 60 seconds.\nMy code has an appsetting.json file which has a couple of settings that need completing for my code to work.\nStorage: This is the connection string for Azure Blob Storage LocalPath: This is the path to where the camera will save its photos to, something like /home/pi/ is all you need but feel free to specify what you need.\nOnce the photos are in blob storage I plan to display them somewhere, add options to delete what I don\u0026rsquo;t want, maybe do something timelapsey.\nI also don\u0026rsquo;t have a proper release pipeline and this grates on me a bit. I have been doing a mixture of writing code in VS and pushing that to github and then doing git pull on the Pi, and also writing code directly on the Pi. (VS Code can connect via SSH which is pretty cool!)\n","date":"May 10, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/e21z7vbamy6w6akhhiwd.jpg","permalink":"https://www.funkysi1701.com/posts/2021/dotnet-on-a-raspberry-pi/","series":null,"tags":["RaspberryPi","Programming","DotNet"],"title":"DotNet on a Raspberry Pi"},{"categories":null,"content":" Back when I was a kid, I used to record our weekly gas and electricity meter readings in a little notebook. We then typed these reading into a spreadsheet (this was in the pre-Excel days), which allowed plotting as a line graph.\nHow would I go about doing a similar thing today? First off, I have a smart meter that submits meter readings every 30 minutes or so. However, I do not know anyway to get access to these readings directly, short of manually recording them like I did 30 years ago!\nOctopus Energy have a public API which allows you to pull your consumption readings. The smart meter sends your usage to your energy supplier, in my case Octopus, they then process these readings and allow them to be queried with an API they have created. It is not a direct connection to your data, but it is the next best thing.\nOther energy suppliers will hopefully follow this example and allow users access to their consumption data.\nHow do I use the API? Using the API is straight forward. Octopus supply you with a secret which you use to authenticate against the API with Basic Auth, no password just a username. Then you just need to pass some details of your meters to get an object containing the last few days meter readings.\nAPI Docs GET /v1/electricity-meter-points/{mpan}/meters/{serial_number}/consumption/ GET /v1/gas-meter-points/{mprn}/meters/{serial_number}/consumption/ {mpan}/{mprn} of your gas or electricity meter, and {serial_number} is the serial number of the meters.\nSomething to be aware of, I initially collected the last days consumption, which worked, however on one day I encountered a gap in the data for electricity. So, I changed to collect and store the last month\u0026rsquo;s data. I can then query this for what I need.\nUsually, the last 24 hours of data is available after midnight of that day. e.g. at midnight 2nd March all the data for 1st March should be available. This is not guaranteed so don\u0026rsquo;t rely on it, however I see no problem with having a few days delay between charting your usage.\nI am still testing this out but so far, I have three charts for gas (and the same for electricity), the first chart covers a 24-hour period, the next covers a day total over 2 weeks, the final chart covers a total for each month (as I write this I have less than a month\u0026rsquo;s worth of data!)\nFor the day and 2 weeks charts, I plot a comparison line of the previous period so you can easily compare the current and previous usage. From my limited testing I have already discovered my usage is very similar day to day.\nAnother point of interest is that gas consumption is in m^3 and electricity is in kW/h. If you are interested in trying the Octopus Energy API, here is a referral link .\n","date":"Mar 7, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/63ep8hp6ipyn2l4emiop.png","permalink":"https://www.funkysi1701.com/posts/2021/charting-my-energy-usage-with-the-octopus-energy-api/","series":null,"tags":["API","OctopusEnergy"],"title":"Charting my Energy usage with the Octopus Energy API"},{"categories":null,"content":"Azure DevOps release pipelines have lots of options to do things how you want. One of my favourites is the option for approval.\nThere are two ways you can do approvals Pre and Post deployment. Lets look at both.\nPre Deployment Approval Lets imagine you have a simple deployment pipeline that deploys to a test/development environment before deploying to a production environment.\nPre Deployment Approval happens immediately before the release so in this example, click in the ellipse before the Prod release step.\nYou will get a screen like the above, you can select what users need to approve it and how long approval waits before timing out, the default is 30 days, but I tend to use a shorter time out of 3 days.\nPost Deployment Approval Post Deployment Approval happens immediately after the release so in this example, click in the circle after the Test release step.\nYou will get a screen like the above, with the same settings as before.\nThat is pretty much all there is to approvals so either option will prompt you to approve before anything gets deployed to your production environment.\nDeployment Hours To complicate matters I make use of the following setting to define deployment hours. This setting will start the Prod deployment at 3am Mon-Fri.\nIf I configure Post Deployment Approval, as soon as my deploy to Test has completed a request for Approval is sent.\nIf I configure Pre Deployment Approval, at 3am Mon-Fri a request for Approval is sent (not ideal if you tend to be asleep at 3am)\nSo it looks like Post Deployment Approval is more useful for my use case. However if you deny approval either in Pre or Post approval this will mark the deployment as failed and show Red in your list of deployments.\nFrom a casual glance it looks like the deployment to Test is failing, it isn\u0026rsquo;t I am just opting to not continue my deployment to production.\nMy Pipeline This is how I have my pipeline setup. Deployment happens on Test and doesn\u0026rsquo;t have a post approval step.\nAfter Test an empty stage called Approval runs and that has a post deployment approval, this happens immediately after Test so you get asked straight away for approval.\nProd does not start as I have my deployment hours configured. Once it is time for deployment to Prod to start it executes.\nNow a casual look at my past releases, you can easily see which have been stopped by approval and which have failed due to whatever issue, and which have run all the way through to Prod.\nAnd deployments to Prod can only ever run during my defined deployment window.\nI am interested to hear how you have your deployment pipeline setup. Do you make use of Pre or Post Approvals? Do you ensure deployments always happen at specific times?\n","date":"Feb 14, 2021","img":"https://dev-to-uploads.s3.amazonaws.com/i/9k6vo6pfv434u7yq3mt4.png","permalink":"https://www.funkysi1701.com/posts/2021/azure-devops-release-pipelines-pre-and-post-approval/","series":null,"tags":["AzureDevOps","DevOps","Azure"],"title":"Azure DevOps Release Pipelines Pre and Post Approval"},{"categories":null,"content":"I didn\u0026rsquo;t make any goals for 2020, or if I did, I didn\u0026rsquo;t officially announce them. 2020 has been a hard year for all of us, 2021 is going to be better.\nHere are a few ideas for my goals for the year ahead.\nAzure certification Mentoring F# Cosmos db/Mongo db Give a talk Mandlebrot Generator Pwned Pass Mobile App More time for me I want to get a certificate to show how much I know. The obvious area for this is Azure. I spend a lot of time playing with Azure, building and deploying to the platform I should be able to get certified in this area. I actually had an exam booked in 2020 but it was an in-person exam so was cancelled when Covid hit. I plan to sit the foundation exam in the first quarter of 2021 and I will take it from there.\nI have had a few times recently where I have been reminded that I don\u0026rsquo;t have much experience working with others. Mostly because I work in a one-man development team so there are limited opportunities in the workplace. I have a few ideas to change this, but this is one of my top priorities for 2021.\n\u0026amp; 4) My experience is very Microsoft and C# so I want to spend some time exploring and learning tech that is adjacent to this. A functional language like F# sounds like a good compliment to my existing skills and my data skills are also very SQL server so some document database skills would be a good place to spend some time.\nDoing a talk has been on my list for years. I am terrified of doing one and with everything online now I don\u0026rsquo;t know if that makes things easier or harder. I don\u0026rsquo;t think 2021 is going to be the year for this but maybe I will take baby steps towards this goal.\nTwitter recently reminded me about watching a Mandlebrot generate one pixel at a time in the 1980s. I would love to explore the code used to generate them and how fast they currently are to produce.\nMy Google play store Xamarin forms app that uses the HIBP API has less than 100 users and I am considering closing it down, especially as I pay a monthly fee for API keys. I haven\u0026rsquo;t really decided what to do, maybe I will spend some time improving it, maybe I will close it down, maybe I will build something else.\nFor many people 2020 has been a hard year, I am one of those\n","date":"Dec 28, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/goals-for-2021/","series":null,"tags":["Goals"],"title":"Goals for 2021"},{"categories":null,"content":"I saw a tweet about building a twitter clone being harder than you would think. So this of course started me thinking how I would go about building something like that.\nOk so where would I start? First a few assumptions.\nDevelopment by a lone developer ie me Tech stack will be dotnet and other tech I am familiar with Database backend, probably SQL Server but I might use table storage for cost reasons should I try and actually build this. However if I design this well this should be something that could be swapped out as the system grows User accounts on the system will be small as I can\u0026rsquo;t imagine anyone ever signing up. Why sign up to a social media platform with no users? I guess the next question is what is Twitter? A website that allows you to share 280 characters of text with your followers, allows you to follow other users updates and allows other user to follow your updates.\nIt also has an API that allow you to do almost everything that you can with the website.\nThen there are of course mobile apps to consider but I am going to assume this is out of scope, however assuming a good enough API then this shouldn\u0026rsquo;t be a problem for future development.\nFirst Steps To start off with I would concentrate on the API, and then build a web client that makes use of the API.\nSo what would my MVP (minimum viable product) be?\nUser can authenticate with my API to get a token which allows access to other API endpoints User can create a tweet User can view own tweets User can view tweets of another user User can view tweets in their timeline User can follow/unfollow other users User can search for other users User can search for keywords in tweets I think that is probably sufficient to build my MVP for the API.\nAn interesting side note is that I could use the OAuth Twitter authentication to allow users to login to my twitter clone with the real twitter login details. However this makes no sense to me as we are essentially adding a dependency on the real twitter.\nSo what would I use for the frontend? I would start off with a Client Side Blazor frontend. Once I had a proof of concept that worked, I would think about styling and adding the UI elements that are familiar to twitter users.\nThe beauty of Client Side Blazor is that I can host cheaply in azure storage and distribute around the world via a CDN.\nDue to the high number of times that follower and following count and other stats are queried I would consider storing these in the database and include a regular job to recalculate them so they don\u0026rsquo;t get out of sync with the data.\nHaving said all of this I am very tempted to fire up Visual Studio and see how far I get, and what problems I encounter along the way.\n","date":"Dec 22, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/building-a-twitter-clone/","series":null,"tags":["Twitter","Design","Architecture"],"title":"Building a Twitter Clone"},{"categories":null,"content":"It has been a bit of a mad week this week. I joined a new team so lots of time learning what\u0026rsquo;s what and also being pulled in two directions as usual demands come through on top of that.\nMy blog runs on Blazor and I have been making use of JavaScript interop to update the html headers and update the page title to match the blog post article. This works great, I load the page and check the headers and they were saying what I wanted.\nThe problem was I wanted to add tags for twitter cards This means that when I paste a link to my blog on twitter you get a nice preview and pic of me in the tweet. This was not working at all even though I had the correct headers.\nI eventually figured out that the problem was the fact I was using JavaScript to update my headers after the page had been initially loaded. Twitter was fetching my page before these headers got added and therefore couldn\u0026rsquo;t see the twitter card headers.\nMy solution was to use invalid html. Not ideal but it works. I added the required html tags in the body of my page using Blazor/C# instead of using JavaScript to add them into the header. Twitter appears to not be fussy in finding them in the wrong place.\nTwitter provides a validator tool at Twitter Card Validator which my website now passes.\nNot much else to say this week, apart from I am missing Visual Studio and C#, I have been mostly using VS Code on Linux and looking at php which isn\u0026rsquo;t as much fun as my usual day job.\n","date":"Dec 12, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/005-twitter-cards/","series":null,"tags":["Twitter"],"title":"#005: Twitter Cards"},{"categories":null,"content":"I use sp_send_dbmail to send results of sql queries by email to business users. Recently an issue was raised that data was being cut off after 255 characters. To fix this I added @query_no_truncate = 1, however this stopped the column headings from being included. No idea why you can\u0026rsquo;t have all the data and column headings but there you have it.\nWhat I am doing now is running 2 queries, one to get the headings, and one to get the data. In theory you should be able to combine them with a Union however you then have datatype issues for non text columns so I gave up with that idea.\nMy results have 60 something columns (don\u0026rsquo;t ask its for a data import into a third party system!) so I am not typing them all out. I can shove query results into a temporary table and then execute to get a list of columns.\nSELECT name FROM tempdb.sys.columns WHERE object_id = object_id(\u0026#39;tempdb..#TempTable\u0026#39;) However I need my list to be horizontal so I can use as column headers. I can use dynamic SQL and a pivot to flip them round.\nDECLARE @cols AS NVARCHAR(MAX), @query AS NVARCHAR(MAX) SELECT @cols = STUFF((SELECT \u0026#39;,\u0026#39; + QUOTENAME(name) FROM tempdb.sys.columns WHERE object_id = object_id(\u0026#39;tempdb..#TempTable\u0026#39;) FOR XML PATH(\u0026#39;\u0026#39;), TYPE).value(\u0026#39;.\u0026#39;, \u0026#39;NVARCHAR(MAX)\u0026#39;),1,1,\u0026#39;\u0026#39;) SET @query = N\u0026#39;SELECT \u0026#39; + @cols + N\u0026#39; FROM ( SELECT name FROM tempdb.sys.columns WHERE object_id = object_id(\u0026#39;\u0026#39;tempdb..#TempTable\u0026#39;\u0026#39;) ) x PIVOT ( MAX(name) FOR name IN (\u0026#39; + @cols + N\u0026#39;) ) y\u0026#39; ","date":"Dec 6, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/weekly-update-004/","series":null,"tags":["SQL"],"title":"Weekly Update #004"},{"categories":null,"content":"Been a quiet week, so wasn\u0026rsquo;t expecting to have much to write on here, however a few things happened worth talking about.\nMy First PR A comment was made to me to do something with the postcodes that are in the system I am developing. Find out what projects have postcodes near each other, and that way work can be grouped together and reduce potential mileage costs of staff that need to visit these projects.\nA quick google search found https://postcodes.io/ which has an API that returns nearby postcodes. It also has a C# wrapper https://github.com/markembling/MarkEmbling.PostcodesIO A comparison of what was being returned from the wrapper and what the API said should be returned revealed that the distance between postcodes wasn\u0026rsquo;t being returned.\nAs the code was on GitHub I could easily see how easy or difficult it might be to add the missing bit of information. It was easy! So, I forked the repo and made the change. I published the change to a private NuGet repo in my Azure Dev Ops account. That way I could try my revised package to check it did what I wanted.\nI left a message on the GitHub project letting the owner know I had a potential fix for an issue. The project hadn\u0026rsquo;t been updated in over a year, so the owner may not be interested, or the project may have been abandoned.\nI was in luck just 17 hours after I left a message the project owner said to create a Pull Request, which I did and shortly afterwards my code had been merged in and an updated version of the package existed in the public NuGet feed.\nI have been thinking about contributing to open source for a while. However, I had not seen a project I wanted to contribute to, or a problem that I knew how to fix until now that is.\nPHP This week I had a call asking me if I knew PHP?\nI did, over ten years ago, before I got my first IT job, I spent time learning PHP and MySQL. I created a blog, and I also created a website for my Dad\u0026rsquo;s camera club. The code I created back then was awful. No shared code, all the code was associated with the page, or sorts of bugs occurred and as time went by it became increasingly hard to update. The site was well liked but I eventually lost interest and moved on to learn other things.\nThis call led to me talking with the head of IT, and later a couple of the developers who have since granted me access to the codebase of a project.\nI haven\u0026rsquo;t had time to spend a lot of time looking at the code so far, however this is nothing like the PHP I had built before.\nThe project makes use of the laravel framework and the first file I opened had methods and classes, so apart from the syntax you could think you were looking at C#.\nAnother thing that interested me was the project used docker containers, it has automated builds as well. Lots of modern programming ideas that I had some ideas about. I am looking forward to learning more about this project and how I might contribute to it.\n","date":"Nov 28, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/weekly-update-003/","series":null,"tags":["OpenSource","PHP"],"title":"Weekly Update #003"},{"categories":null,"content":"I know Active Directory is fussy about clocks being in sync however not sure how todays issue happened.\nI run my docker compose file from Visual Studio and I get a weird error.\nSecurityTokenNotYetValidException: IDX10222: Lifetime validation failed. The token is not yet valid. ValidFrom: \u0026#39;System.DateTime\u0026#39;, Current time: \u0026#39;System.DateTime\u0026#39;. I deleted my containers, open and close Visual Studio a few times, nothing helps. Eventually I think to find out what the time is on my container. It has yesterday\u0026rsquo;s date. What has happened here? Surely recreating containers would have caused them to have todays date? I reboot and everything is fine again.\nTurns out that it is a know issue, see https://thorsten-hans.com/docker-on-windows-fix-time-synchronization-issue I am using WSL2 and I have now changed back to using Hyper-V and the issue hasn\u0026rsquo;t come back.\nEarlier in the week I spotted my build step was failing.\n- task: NuGetToolInstaller@0 Swapping to the next version of the step is all I needed to do to fix it.\n- task: NuGetToolInstaller@1 My guess is that support was dropped for this earlier version or there is some other incompatability with .Net 5.\n","date":"Nov 21, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/weekly-update-002/","series":null,"tags":["Docker","Azure"],"title":"Weekly Update #002"},{"categories":null,"content":"One of my favourite podcasts is Troy Hunts weekly update. In it he discusses stuff that he has been working on, plus some personal stuff. I am going to attempt to do something similar. It will probably take me a few of these before we get a look and feel that works.\nMonday A week off work, mainly to use it up before year end, plus want to get a few jobs around the house done.\nI did ask the following question on Twitter.\nHey #azurefamily and #dotnet developers how do I get more involved in mentoring?\n\u0026mdash; Simon Foster (@funkysi1701) November 9, 2020 As a one person dev team, my biggest weakness is working with others so any ideas of how to change that are great.\nTuesday Dotnet 5 is out! The latest version of dotnet is released by Microsoft and to celebrate there is dotnetconf to listen to. Due to time zones and family commitments, I haven\u0026rsquo;t listened to an awful lot of it but I did see the keynote and loved the 3 Scott\u0026rsquo;s chat.\nWednesday My youngest son was 3 today, due to Coronavirus we didn\u0026rsquo;t do much but we celebrated as a family, and he even had a zoom call.\nThursday Blazor has a new feature Virtualize where a list of items can only load the ones on screen. I have been trying to get this to work on my blog, works great running locally but not working in production yet.\nThink I know what might be happening. I use Cloudflare to do my SSL, as Custom SSL certs for the cheaper Azure Web Apps is not supported. Something in Cloudflare is caching or interfering.\nhttps://zimmergren.net/solved-asp-net-core-blazor-web-sites-does-not-work-with-cloudflare-html-minification/ Turning off HTML minification fixed my issue!\nOne additional thing I added to my Blog is the /config page which details some of the config settings. I think this probably came from https://www.hanselman.com/blog/adding-a-git-commit-hash-and-azure-devops-build-number-and-build-id-to-an-aspnet-website but it was a while ago when I first did this on another project.\nAt the moment we have .net Version, Commit and Build links.\nThe .Net Version is obtained from\n@System.Runtime.InteropServices.RuntimeInformation.FrameworkDescription A few other bits of info can be obtained from System.Runtime.InteropServices.RuntimeInformation which I have included on the page for fun. There are probably security concerns with exposing all this info publicly so something to bear in mind if you try this.\nBuild Info is passed to my code by a build step\n- script: \u0026#39;(echo $(Build.BuildNumber) \u0026amp;\u0026amp; echo $(Build.BuildId)) \u0026gt; .buildinfo.json\u0026#39; displayName: \u0026#34;Emit build number\u0026#34; workingDirectory: \u0026#39;$(Build.SourcesDirectory)/src/WebBlog\u0026#39; failOnStderr: true This simply passed the build id and number which are stored as variabled and saves them in a text file.\nI then have a class that reads them and constructs a link.\nusing Microsoft.Extensions.Hosting; using System; using System.IO; using System.Linq; using System.Reflection; namespace WebBlog { public class AppVersionInfo { private const string _buildFileName = \u0026#34;.buildinfo.json\u0026#34;; private readonly string _buildFilePath; private string _buildNumber = string.Empty; private string _buildId = string.Empty; private string _gitHash = string.Empty; private string _gitShortHash = string.Empty; public AppVersionInfo(IHostEnvironment hostEnvironment) { _buildFilePath = Path.Combine(hostEnvironment.ContentRootPath, _buildFileName); } public string BuildNumber { get { if (string.IsNullOrEmpty(_buildNumber)) { if (File.Exists(_buildFilePath)) { var fileContents = File.ReadLines(_buildFilePath).ToList(); if (fileContents.Count \u0026gt; 0) { _buildNumber = fileContents[0]; } if (fileContents.Count \u0026gt; 1) { _buildId = fileContents[1]; } } if (string.IsNullOrEmpty(_buildNumber)) { _buildNumber = DateTime.UtcNow.ToString(\u0026#34;yyyyMMdd\u0026#34;) + \u0026#34;.0\u0026#34;; } if (string.IsNullOrEmpty(_buildId)) { _buildId = \u0026#34;123456\u0026#34;; } } return _buildNumber; } } public string BuildId { get { if (string.IsNullOrEmpty(_buildId)) { var _ = BuildNumber; } return _buildId; } } public string GitHash { get { if (string.IsNullOrEmpty(_gitHash)) { var version = \u0026#34;1.0.0+LOCALBUILD\u0026#34;; var appAssembly = typeof(AppVersionInfo).Assembly; var infoVerAttr = (AssemblyInformationalVersionAttribute)appAssembly .GetCustomAttributes(typeof(AssemblyInformationalVersionAttribute)).FirstOrDefault(); if (infoVerAttr != null \u0026amp;\u0026amp; infoVerAttr.InformationalVersion.Length \u0026gt; 6) { version = infoVerAttr.InformationalVersion; } _gitHash = version[(version.IndexOf(\u0026#39;+\u0026#39;) + 1)..]; } return _gitHash; } } public string ShortGitHash { get { if (string.IsNullOrEmpty(_gitShortHash)) { _gitShortHash = GitHash.Substring(GitHash.Length - 6, 6); } return _gitShortHash; } } } } The BuildId and BuildNumber properties just fetch the details saved into the text file during the build. This can then be passed to build the build link.\n\u0026lt;a href=\u0026#34;https://dev.azure.com/{OrgName}/{RepoName}/_build/results?buildId=@appInfo.BuildId\u0026amp;view=results\u0026#34;\u0026gt; @appInfo.BuildNumber \u0026lt;/a\u0026gt; Finally, the GitHash properties need to fetch the hash and shorthash of the commit which is a bit more complex. This is achieved using the following line in your build.\n- task: DotNetCoreCLI@2 displayName: \u0026#39;Publish\u0026#39; inputs: command: \u0026#39;publish\u0026#39; publishWebProjects: true arguments: \u0026#39;--output $(Build.ArtifactStagingDirectory) /p:SourceRevisionId=$(Build.SourceVersion)\u0026#39; /p:SourceRevisionId=$(Build.SourceVersion) add the revision hash to [assembly: AssemblyInformationalVersion] during the build which can then be extracted using the gitHash property above, before being passed into the commit link.\n\u0026lt;a href=\u0026#34;https://github.com/{OrgName}/{RepoName}/commit/@appInfo.GitHash\u0026#34;\u0026gt; @appInfo.ShortGitHash \u0026lt;/a\u0026gt; ","date":"Nov 14, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/weekly-update-001/","series":null,"tags":["DotNet"],"title":"Weekly Update #001"},{"categories":null,"content":"Have you wondered what info you are leaking via your response headers?, do you want some kind of guide about what headers to set or remove altogether?\nHead on over to https://securityheaders.com/ This is a site created by security expert Scott Helme that rates a URL based on what response headers it can see.\nI am pleased to say www.funkysi1701.com is now getting an A. So how do you add/remove headers in dotnet core?\nIn my configure method in Startup.cs I have the following code block.\napp.Use( next =\u0026gt; { return async context =\u0026gt; { context.Response.OnStarting( () =\u0026gt; { context.Response.Headers.Add(\u0026#34;Permissions-Policy\u0026#34;, \u0026#34;microphone=()\u0026#34;); context.Response.Headers.Remove(\u0026#34;Server\u0026#34;); context.Response.Headers.Remove(\u0026#34;X-Powered-By\u0026#34;); context.Response.Headers.Remove(\u0026#34;X-AspNet-Version\u0026#34;); return Task.CompletedTask; }); await next(context); }; }); I have only included a few of the headers I am adding as the excellent https://securityheaders.com/ can tell you which headers you should add and what options you might want.\n","date":"Sep 26, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/security-headers/","series":null,"tags":["DotNet","Security"],"title":"Security Headers"},{"categories":null,"content":"My last blog post was over six months ago.\nCovid 19 has hit the world, and I will be honest I have found it a challenging time.\nMy Blog had gotten into a bit of a mess. It had become fragmented with different versions of the same thing; I will attempt to explain what has become of my blog.\nThe original WordPress site can currently be found at https://www.pwnedpass.com/ I would prefer it to be on a sub-domain of funkysi1701.com but for some reason I haven\u0026rsquo;t been able to get that to work, not sure if it is a limitation of my hosting package. I like WordPress, it is very flexible, easy to get blog posts out there. But I want to write content about development and having a site I can tinker with is important to me.\nMost of my WordPress blogs have been imported into dev.to and a few extra have been written on this platform. I like dev.to it is a wonderful place to share content and it has one or two extra features I like.\ndev.to has an integration with Stackbit/Netlify and this became https://dev.funkysi1701.com . I like having a personal site, but having the same content as dev.to. To add content to this site all I need to do is write it on dev.to and some magic will go on behind the scenes and new content will be published.\nHowever, as a developer I don\u0026rsquo;t like magic, I want to understand what is going on a fiddle with all the settings and make it do what I want.\ndev.to has an API, I can build a site in .Net Core and make API calls to fetch the content I want. I understand APIs, I understand .Net and can customise my site exactly how I want it, plus play about with a .net website. This is what https://www.funkysi1701.com is now.\nSo what have I built so far. I have a Server Side Blazor site running .Net 5. Why Server Side and not Client Side I hear you ask? Well only because I have more experience with Server Side and know how to quickly create a website with that technology, I may change it as time goes by, but we will see.\nI have two pages a list of my blog posts and a page that displays the content. Both of these use the dev.to API. I lied, there is a third page I hacked together to do some page redirection from the WordPress URLs. This is something I will change as time goes on.\nThere are lots of improvements I want to do, there are probably also lots of broken images or links as well. Hopefully, this will result in a good platform to blog about as well as on.\n","date":"Sep 25, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/back-to-blogging/","series":null,"tags":["Blogging","Blazor","DotNet"],"title":"Back to Blogging"},{"categories":null,"content":"Let\u0026rsquo;s Encrypt is amazing, you can easily add SSL certificates to any website and automate the renewal process. I have talked before about how impressive it is.\nOnce you start adding SSL certificates to your production sites however you may want to check when they expire so you don\u0026rsquo;t get caught out. You can always open your site in your favourite browser and view the certificate information and expiry date.\nHowever there is a way to automate this check.\n[Fact] public void IsSSLExpiring() { var handler = new HttpClientHandler { ServerCertificateCustomValidationCallback = CustomCallback }; var client = new HttpClient(handler); HttpResponseMessage response = client.GetAsync(\u0026#34;https://www.example.com\u0026#34;).GetAwaiter().GetResult(); Assert.True(response.IsSuccessStatusCode); } private bool CustomCallback(HttpRequestMessage arg1, X509Certificate2 arg2, X509Chain arg3, SslPolicyErrors arg4) { var now = DateTime.UtcNow; var expire = arg2.NotAfter; var diff = (expire - now).TotalDays; Assert.InRange(diff, 30, 1000); return arg4 == SslPolicyErrors.None; } This code gets the SSL expiry date from https://www.example.com and will fail the xunit test if the expiry date is less than 30 days in the future. I then schedule my tests to run regularly on all my environments with a Let\u0026rsquo;s Encrypt Certificate and this gives me advanced warning if a SSL certificate is about to expire.\nThe Assert.InRange(diff, 30, 1000) line will fail the test if the expiry date is less than 30 days or greater than 1000, but as the default expiry for Let\u0026rsquo;s Encrypt certificates is three months it will never be greater than 1000 days even with a freshly installed certificate. These values can be tweaked to suit your use case, however 30 days is enough time for me to investigate what is happening.\nTo execute my tests I use a scheduled build in Azure DevOps, but anything that regularly can run your tests will do the job.\nThe code above is just a simple example to get your started for my purposes I have put all my URLs into config files and just pass these into my tests, so I don\u0026rsquo;t need a custom test for every different URL.\n","date":"Mar 3, 2020","img":"","permalink":"https://www.funkysi1701.com/posts/2020/testing-for-expiring-ssl-certificates/","series":null,"tags":["Security","SSL","Testing"],"title":"Testing for expiring SSL Certificates"},{"categories":null,"content":"It has just been announced at the annual Star Trek Las Vegas (#STLV ) convention that Patrick Stewart is going to return to his role as Jean-Luc Picard in a new TV Show.\nFew other details have been announced other than the TV show will take place 20 years after the film Star Trek: Nemesis and will feature the character of Jean-Luc Picard.\nLets run though a few things I would like to see from this new show.\nLow on Action Star Trek: Discovery is a great show and has brought Star Trek back to our TV shows. However it is an action show, it zooms along so fast and with so much action I would appreciate a show that can takes its time a bit more.\nI want a show that can show Picard carefully considering some moral issue and making a decision. Something we were used to seeing on TNG.\nFeature Picard’s excellent speeches One of the best features I can think of Jean-Luc Picard is his ability to make speeches about one issue or other.\nLets look at a few examples:\nInsurection : Picard speaks about the forced relocation of a group of people.\nHow many people does it take, Admiral, before it becomes wrong? Hmm? A thousand, fifty thousand, a million? How many people does it take, Admiral?\nFirst Contact : Picard is hell bent on revenge for what the Borg have done to him and refuses to destroy the Enterprise to save his crew.\n“I will not sacrifice the Enterprise. We’ve made too many compromises already. Too many retreats. They invade our space and we fall back. They assimilate entire worlds, and we fall back. Not again! The line must be drawn here, …this far, no further! And I will make them pay for what they’ve done.”\nMeasure of a Man : Picard argues for the rights of Data.\n“You see, he’s met two of your three criteria for sentience, so what if he meets the third. Consciousness in even the smallest degree. What is he then? I don’t know. Do you? (to Riker) Do you? (to Phillipa) Do you? Well, that’s the question you have to answer. Your Honour, the courtroom is a crucible. In it we burn away irrelevancies until we are left with a pure product, the truth for all time. Now, sooner or later, this man or others like him will succeed in replicating Commander Data. And the decision you reach here today will determine how we will regard this creation of our genius. It will reveal the kind of a people we are, what he is destined to be. It will reach far beyond this courtroom and this one android. It could significantly redefine the boundaries of personal liberty and freedom, expanding them for some, savagely curtailing them for others. Are you prepared to condemn him and all who come after him to servitude and slavery? Your Honour, Starfleet was founded to seek out new life. Well, there it sits. Waiting. You wanted a chance to make law. Well, here it is. Make a good one.”\nAsk important questions We live in uncertain times. There are lots of pressures on today’s world and we need to acknowledge these. Picard is an excellent character to use to ask important questions of the day.\nWhat does it mean to be human?\nHow should we treat people that are different to us?\nHow should we react to extremism?\nDeal with his age In the last episode of TNG a future version of Picard is shown 25 years into the future. This is a similar time period so it will be interesting to compare the two. Have Picard look back on his life and ask questions about his life choices.\nBe consistent with his past Picard is a well known character. So writers please don’t change him. I want a character we know, not a character that looks like Picard but acts in a way contrary to what we would expect.\nPicard has a history we know, he served on the stargazer and of course the Enterprise, he has an artificial heart, he was assimiliated by the Borg. All\nReference 50 years of Star Trek Discovery and Enterprise are TV shows which have a problem. There is Trek going on after them which imposes a few rules on what can and can’t be done.\nThis is a TV Show that will take place after every other Trek TV and Film and can do whatever the creators want. This gives freedom, but it also gives the responsibility to reference what has gone before.\nThe 2009 Star Trek film features the destruction of Romulus and Spock travelling back in time. How about Picard dealing with surviving Romulans helping rebuild their society, continuing the work that Spock started in the TNG episode Unification.\nI look forward to finding out what the Star Trek production team has in store for us. If they have Patrick Stewart convinced I am sure it is going to be good.\n","date":"Aug 8, 2019","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2018/08/download.jpg?w=662\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2019/picard-is-back/","series":null,"tags":["StarTrek","Picard","PatrickStewart"],"title":"Picard is back!"},{"categories":null,"content":"Automated releases of software are great but how can we add an element of feedback so only good releases go live.\nI have been using Azure DevOps to release my PwnedPass android app to the Google Play Store for a while now. There are options to deploy to the alpha, Beta or Production tracks and even to set % of users to target. For the full range of options check out the Google Play extension for Azure DevOps.\nMy release starts by publishing to 10% of users on the production track, my next step makes use of the increase rollout option to increase this %, you can have as many of these additional steps as you want until you reach 100% of your users.\nNow if you run this release now it will just run through each of the steps one after the other. Now of course you can add a pre or post approval to your pipeline but this just adds a manual dependency to your release. Whoever does the approving needs to check things are working before approving or worse just approves regardless.\nAzure DevOps has the concept of gated releases which allows you to add automated checks before or after a release happens. These automated checks can be any of the following:\nAn Azure Function A Rest API call Azure Monitor Alert Query Work Items Security and Compliance Assessment We are going to make use of the Azure Monitor Alert, to create an alert from your Application Insights data and only continue the rollout if no failures are detected.\nOpen up your application insights resource in the Azure portal and look in alerts. Click add new alert rule.\nSelect your application insights resource in Resource, In Condition choose a condition to check, I chose Failed Requests, so every time a failure is registered in my API I can stop the deployment. The exact criteria you want to use is entirely up to you.\nCreate an action group, I just set my alert to send an email to myself but there are other alert actions you may want to try. Give your alert a name and description and click save.\nNow all we need to do is make Azure DevOps make use of this alert. In your release pipeline select the pre-deployment conditions of your second step and open up the Gates section.\nChoose a suitable time to evaluate, I have been using something long like 12 or 24 hours so if there are problems there is time for it to be noticed. Choose Version 1 of the task (I was not able to get it to work with Version 0)\nNow select your Azure subscription and Resource Group and leave the rest of the settings as they are. Now your Deployment will stop and analyse application insights for any Failed requests and will halt if it finds any.\nI am still testing this out but it will take a few days to figure out if this what I want due to the large time scales involved. I feel this is going to be an improvement of manually approving release steps.\n","date":"Apr 5, 2019","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/04/image.png?fit=662%2C116\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2019/gated-release/","series":null,"tags":["Azure","API","ApplicationInsights","DevOps"],"title":"Gated Release"},{"categories":null,"content":"So you have created a super API that does something amazing. How do you document it so people will use it?\nOne way of easily documenting your API is to install the Swashbuckle package.\nInstall-Package Swashbuckle.AspNetCore Install-Package Swashbuckle.AspNetCore.Swagger Then in you startup.cs add the following lines\n//In ConfigureServices services.AddSwaggerGen(c =\u0026gt; { c.SwaggerDoc(\u0026#34;v1\u0026#34;, new Info { Title = \u0026#34;API\u0026#34;, Version = \u0026#34;v1\u0026#34;, Description = \u0026#34;An API Description\u0026#34; }); c.IncludeXmlComments(string.Format(@\u0026#34;{0}\\API.xml\u0026#34;, System.AppDomain.CurrentDomain.BaseDirectory)); }); //In Configure app.UseSwagger(); app.UseSwaggerUI(c =\u0026gt; { c.SwaggerEndpoint(\u0026#34;/swagger/v1/swagger.json\u0026#34;, \u0026#34;API V1\u0026#34;); c.RoutePrefix = string.Empty; }); Now when you browse to your API you will see the swagger documentation system.\nThe RoutePrefix setting controls the path in which swagger will display. I have my docs at the root, but you might want them under the /docs or similar path.\nThe IncludeXmlComments setting from the ConfigureServices method allows you to load in any XML comments you have added to methods. For this to work you need to enable a setting to your build.\nThe XML documentation file must be ticked and contain a path. Everytime you do a build, a XML file will be generated which contains all the comment blocks you have added to your code.\nSwagger will then use this XML documentation file to produce lovely looking documentation without you having to do anything extra.\n","date":"Mar 27, 2019","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/03/image-3.png?w=662\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2019/documenting-your-api/","series":null,"tags":["Swagger","API"],"title":"Documenting your API"},{"categories":null,"content":"Azure Key Vault is a secure way of storing your keys, certificates and secrets so your application can access everything it needs to but you don’t have them being stored insecurely anywhere such as in source control.\nI have been wanting to give Azure Key Vault a try for a while now as it can make use of Azure Active Directory to give your web app an identity so it can authenticate itself into the key vault to access secrets. Pretty clever but with a lot of moving parts a bit complex.\nFor my example I am just going to connect to my Key Vault and get a secret and display it somewhere on a web page. This is of course not what you want to do as secrets are secret and shouldn’t be displayed just used to authenticate into whatever, however it is an easy way to prove I am connecting to the Key Vault and everything is working.\nLets look at some code. I have a .net core application and to start with lets install three nuget packages.\nMicrosoft.Azure.KeyVault Microsoft.Azure.Services.AppAuthentication Microsoft.Extensions.Configuration.AzureKeyVault I’ve not include version numbers as these will no doubt get updated over time but hopefully it will still work.\nNow in your Program.cs add the following code, replacing [KeyVaultName] with the name of your Key Vault.\npublic class Program { public static void Main(string[] args) { CreateWebHostBuilder(args).Build().Run(); } public static IWebHostBuilder CreateWebHostBuilder(string[] args) =\u0026gt; WebHost.CreateDefaultBuilder(args) .ConfigureAppConfiguration((context, config) =\u0026gt; { var builtConfig = config.Build(); var azureServiceTokenProvider = new AzureServiceTokenProvider(); var keyVaultClient = new KeyVaultClient( new KeyVaultClient.AuthenticationCallback( azureServiceTokenProvider.KeyVaultTokenCallback)); config.AddAzureKeyVault( $\u0026#34;https://[KeyVaultName].vault.azure.net/\u0026#34;, keyVaultClient, new DefaultKeyVaultSecretManager()); }) .UseApplicationInsights() .UseStartup\u0026lt;Startup\u0026gt;(); } Now all you need to do is look at your configuration to pull out secrets from your Azure Key Vault. If you have a secret called AppSecret then you can use the following code snippet to retrieve its value, assuming _configuration is an implementation of Microsoft.Extensions.Configuration.IConfiguration.\n_configuration[\u0026#34;AppSecret\u0026#34;]; Now if you do all of this and run from an Azure Web App or run locally it will fail to pull anything from the Key Vault. You need to give your web app an identity and configure your key vault to allow access from that identity.\nOnce my code has been deployed to an Azure Web App I get the following error.\nLets look at fixing that, first lets give my web app an Identity. Open up the Azure portal and find the identity section of your web app and turn the setting on.\nNow you need to grant that identity permission to your key vault. In the portal open up Access Policies in your key vault and click add Policy, select the identity of your web app in the principal box and select the following settings to grant access to your secret.\nNow you have a website that can pull secrets out of Key Vault but only that unique identity. Anyone who has access to your source code will not have access to your secrets, even if they push your code to a different Azure Web App.\n","date":"Mar 19, 2019","img":"","permalink":"https://www.funkysi1701.com/posts/2019/azure-key-vault/","series":null,"tags":["Website","Security","Azure"],"title":"Azure Key Vault"},{"categories":null,"content":"While at Microsoft Ignite I heard about a lot of cool tech that I want to know more about. The best way to learn something is use it to solve a problem.\nSo what can I build that is both useful and will let me play with some new tech?\nI have a Xamarin Forms app Pwned Pass that has over 500 downloads on Google play and over 80 downloads on the Microsoft Store . This has given me a small user base that I can use to make use of whatever I build.\nMy app makes use of the HIBP API created by Troy Hunt. I am going to build my own API, initially it will just make calls to the HIBP API. Building this will give me experience of building something with .net Core from design to deployment. I have made a start already on doing this, I have an empty .net core API project which deploys to an Azure web app using the build and release pipelines from Azure DevOps.\nYou may be wondering why I am not making use of Azure Functions to build this API. Azure functions is certainly a great technology that is worth learning about. However I have done a little bit with them in the past and I don’t believe I would be able to learn all the things I want to if I used Azure Functions. My primary goal is learning and sharing that learning via this blog. It may well be I move to using Azure Functions later on.\nAnother tech I am keen to learn more about is Azure Key Vault . This is a technology that allows the securing of keys, connection strings and certificates. I want my app to securely get keys and security information without any of it having to be committed to source code or shared insecurely.\nMonitoring my app is also a key learning from me. I use application insights already, but I would like to extend my understanding of this so telemetry can be fed back into the build and bad deployments stopped.\nBelow is my complete list of learning and tech I want to touch on. It is a long list and I imagine it will get longer as I work through it. I want to regularly blog and share what I have been working on. I currently have a working build and release pipeline but nothing of note to build or release. I know Key Vault needs looking at early as the identity of the website in Azure is key to getting that tech working correctly.\nBuild API with .net core Add build and release pipeline Make use of Azure KeyVault for secrets, connection strings etc Plugin My Xamarin app to make use of it Monitor my API with Application Insights Secure it with CSPs and log this into Report URI Consider building a web frontend to my API using a javascript library or framework. Maybe react but this can be decided later. Dockerize the API and add the creation of docker images to the build/release pipeline ","date":"Mar 5, 2019","img":"","permalink":"https://www.funkysi1701.com/posts/2019/technology-i-want-to-learn-more-about/","series":null,"tags":["C-Sharp","Azure","Programming","Security"],"title":"Technology I want to learn more about"},{"categories":null,"content":"I have just spent the first day at the conference Microsoft Ignite | The Tour. The conference was free I just needed to arrange travel and accommodation. Microsoft have really looked after me and all the other guests, breakfast and lunch has been provided, I got a free t-shirt, coffee (or tea) all day plus a beer or glass of wine to end the day. And that’s before you collect any free stuff the vendors are giving away.\nThe first session was Designing resilient cloud applications with Matt Soucoup and talked about some cloud technologies like Azure key vault and serving static files from blob storage. Unfortunately this had a few technical issues with the demos. I think it was just connectivity with the MongoDB backend but this slightly spoiled the session. As this was the only technical problem I noticed all day I can let it pass.\nNext was a session on Azure DevOps mainly build and release pipelines called Deploying your application faster and safer with Brian Benz . A lot of this I knew but good to reinforce I am doing things correctly.\nNext was a session on Application Insights called Detecting application anomalies with Telemetry with Matt Soucoup.\nProbably the most useful session was on Docker and Kubernetes called Integrate containers and Kubernetes into your Azure DevOps build and release model with Marco De Sanctis . Going to spend some time looking through the examples from this session.\nA session on Serverless covered Azure Functions, Azure Logic apps and the other Azure Serverless offerings. Investing in Serverless: less servers, more code with Simona Cotin .\nLastly was a panel discussion on the changes facing IT Pros and SysAdmins. What is the future of the IT Pro in a DevOps \u0026amp; Cloudy world? with Jennifer Stirrup , Baki Onur Okutucu , Amy Boyd and Stephen Thair . Should they learn to code, how should they make sure they keep up.\nTomorrow I have a loads more sessions, including ones about mental health, Azure pipelines, more Kubernetes stuff and dealing with failure.\n","date":"Feb 26, 2019","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/02/D0UTj08XgAEo1YJ.jpg?fit=662%2C440\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2019/microsoft-ignite-the-tour-london/","series":null,"tags":["Conference","Azure","Programming","Microsoft","DevOps"],"title":"Microsoft Ignite | The Tour – London"},{"categories":null,"content":"I have been using Azure DevOps (Or VSTS or VSO etc) for a while now and one of the great features is doing automatic builds with every check-in. This is more commonly known as a CI (continuous integration) build.\nMore recently I have started playing about with creating my build using YAML files instead of using the web user interface to create my build.\nWhy? You may wonder why go to the effort of learning the YAML syntax when you can just create the build in Azure DevOps and then forget about it.\nMostly it is because the build changes over time and you shouldn’t just forget about it. If something changes over time then you might want to version control it, or look at a previous version.\nLets say you create a pull request that replaces a .net 4.7 web service with a .net core web service. If you have a CI build this PR will fail because it won’t build. If you change the build first any other builds going on will fail. What you want in this case is the build to be associated with that branch or PR. Any builds before you merge this change in will continue to work and any after this change will also work.\nHow? How do you get started with YAML builds? Well the first thing is to make sure that YAML builds are turned on, as I write this I believe they are still a feature you can turn on or off. Have a look in Preview features and make sure they are turned on.\nNext look at any of your existing builds and click the View YAML link. This will show you an example YAML file of your existing build. You could just save this as azure-pipelines.yml and checkin to the root of your project. You can also click the add new build pipeline option, this will give you some templates to start you off.\nThe YAML file consists of a series of build steps usually called tasks, with a few settings before to configure things like parameters or build agents. Detailed docs about the syntax of the file can be found here .\nFor my mobile app my YAML files consist of downloading nuget packages, building the solution, building specific projects with desired settings, running powershell or other scripts to set things up and finally publishing the results of the build as artifacts so that they can be used in any releases.\nSecure It! Avoid committing passwords and secure keys into source control. I have found you can upload secure files via Azure DevOps and then add a download secure files step at the start of your build. This allows the secure file to be used during the build but the contents of the file can’t be viewed by anyone with access to the source code.\nI find it often takes a bit of thinking about how to achieve this, but it is usually possible to keep keys and secrets secure.\n","date":"Jan 31, 2019","img":"","permalink":"https://www.funkysi1701.com/posts/2019/yaml-builds-on-azure-devops/","series":null,"tags":["CI","Build","Programming","Azure","DevOps","AzureDevOps"],"title":"Yaml Builds on Azure DevOps"},{"categories":null,"content":"Its been a while since I first released Pwned Pass so lets have a look at where we are now.\nWe are very close to 500 Downloads from Google Play and we have recently smashed past 100 active installs, peaking at 116 and even now we are still over 100. I have had 9 reviews (6 x 5*, 2 x 1 * and a 4 *) which averages out at 4 * Over Christmas I released a UWP version that can be found in the Microsoft Store . This has currently had 9 downloads and even had a download to windows mobile (someone out there still likes the platform!) I have a fairly smooth deployment process using Azure DevOps. After every check in of code a build runs which compiles the UWP and Android versions. The build also increments the version numbers that is required to deploy to either of the app stores.\nEvery successful build of the master branch will kick off a release to the Beta track of Google Play. If I am happy I then release to 10% of the Production track, which can then be increased to 100% (or halted). The release to Microsoft Store happens after the Beta track of Google Play. Only reason for this order is that there isn’t a beta area for UWP apps so I want to quickly test change on android before rolling out for windows.\nAll these steps require confirmation by me before proceeding and often don’t get further than the beta track.\nA further development is that I have open sourced the source code to github do take a look if you are curious or want to contribute. With the purchase by Microsoft there are easy ways to connect github repositories to Azure DevOps. Once I create a Pull Request in github it creates a build in Azure DevOps and all the build and release steps can happen.\nI am still not 100% sure if I want to keep my bug and issue tracking in github or Azure DevOps as both have features for doing so.\nOne future improvement I want to make is to automate the creation of screenshots. When I create a new feature and it gets checked in. I would like to automatically created screenshots of the key pages and submit them to the different app stores. Currently I am not sure if this is possible or how to go about it. I have some ideas to experiment with so we will see what I can do.\n","date":"Jan 23, 2019","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2019/01/image.png?w=662\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2019/pwned-pass-update/","series":null,"tags":["Xamarin","Android","PwnedPass"],"title":"Pwned Pass Update"},{"categories":null,"content":"This is my annual lets make some goals for the new year blog post. So in no particular order.\nImprove House I can’t outdo last years goal of buying a house but I can continue to make efforts to improve it. I was going to put something about decorating or doing something to our bedrooms, however today I spotted a hole in the roof so this may well absorb most of my home improvement budget.\nConnect with local area Since we moved in October we haven’t really connected with our local area yet. My only interaction with my neighbours was when my car blocked his drive!\nI am going to make efforts to change this in 2019. I don’t exactly know how yet, but I have some ideas. My son starts nursery and later school this year so could be one way to meet other parents and teachers. I want to connect with a local church, so far this hasn’t happened so need to make more effort with this. There are other local groups that put on child friendly events which would be good to attend.\nLightning Talk I failed last year to do this so it is back on my list. I know I must have stuff of value to share so going to try again on this.\nA lightning talk is a very short talk or presentation given at a conference or user group. I am not a natural public speaker so this is a step out of my comfort zone, however I am going to start small and see what happens.\n##Family Holiday As always we Fosters are going to have a holiday in the summer and have some quality family time. I also want to take the boys to London for a weekend.\nBlogging This blog has taken a bit of a backseat in 2018 so I want to try and refocus it for 2019. My commitment is for 12 blog posts in 2019, that is one a month. It doesn’t sound like much but I know how easily life can get in the way.\nHaving said that my log has had record views this year. 7159 views. However I believe this number to be incorrect and has been inflated by my app development and some of the automated processes taking place.\nRoutine Last goal is to try a get more of a routine going. With two small boys, one of which is starting school in September a routine is going to be essential and will allow all of us all a chance to get more things done.\n","date":"Jan 1, 2019","img":"","permalink":"https://www.funkysi1701.com/posts/2019/lets-see-what-2019-can-do/","series":null,"tags":["Goals",""],"title":"Lets see what 2019 can do!"},{"categories":null,"content":"As 2018 starts to draw to a close let’s look at some of the highlights from the past year.\nNew Home - I started 2018 with the goal of buying my own home and I managed it. At the end of October I actually moved in but it was months of looking at houses, speaking to estate agents, saving, packing and unpacking. There is more I want to do to my home in 2019 so this is only really the beginning. New Job - Another goal was to kick my career up a notch and this happened in June. It has been great working as part of a large team and I have learnt loads, there is much more I want to learn and contribute to so 2019 should be great on that front. New Car - Not a goal that I achieved as unfortunately in October I was involved in a car accident, no one was seriously hurt but this was a horrible experience to go through. My car was written off which gave me an excuse to buy a new car. See more of family was a goal, I am not sure if I really achieved this. However family have been around loads for us during our move and we do live closer so hopefully I can work on this some more. Celebrate 5 years of marriage. It hasn\u0026rsquo;t been an easy year for me and the wife due to moving house and all the stress that involved plus with two children she was diagnosed with Postnatal Depression which as a family we are still dealing with. All that said we had a fab weekend away and we so need to do something similar again soon. Family. 2018 has included many great times with my two boys. It\u0026rsquo;s been amazing seeing them grow, in September we had a thanksgiving service for them and in July we had our annual holiday. Lightning Talk. This is the only thing I mentioned as a goal that didn\u0026rsquo;t happen, it\u0026rsquo;s on my 2019 list so hopefully I can kick myself into action but with everything else I achieved something had to slip. ","date":"Dec 27, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/looking-back-at-2018/","series":null,"tags":["C-Sharp","Programming","Home","Family","Goals"],"title":"Looking back at 2018"},{"categories":null,"content":"Recently I have been investigating what all the fuss is about Docker and it has been well worth my time as Docker is pretty awesome for automating stuff.\nMy development environment has typically required installing SQL Server. SQL is a bit of a beast with lots of options and takes time to setup how you want.\nHowever since Microsoft have now created a version of SQL Server that runs on Linux you can run SQL Server in a Linux container with only a few commands.\nI am going to assume you already have Docker for windows installed on your development machine. If not head over to Docker and find out how.\nThe Microsoft guide to setting up SQL Server in a Linux container can be found here .\nFirst you need to download the image. In a powershell window run:\ndocker pull mcr.microsoft.com/mssql/server:2017-latest This downloads the latest sql server image.\nTo run this image run the following:\ndocker run -e \u0026#34;ACCEPT_EULA=Y\u0026#34; -e \u0026#34;SA_PASSWORD=password\u0026#34; -p 1433:1433 --name sql -d mcr.microsoft.com/mssql/server:2017-latest To run a SQL Server image you are required to accept the terms and conditions and set a default sa password. These are added as environment variables with the -e flag.\nYou also need to set the ports that your container will run on (1433 is the default SQL port) and give your container a name, in this case \u0026ldquo;sql\u0026rdquo;.\nIf you have already installed SQL Server you will not be able to run the container on the same port as your local install. To solve this you can select a different port.\ndocker run -e \u0026#34;ACCEPT_EULA=Y\u0026#34; -e \u0026#34;SA_PASSWORD=password\u0026#34; -p 1434:1433 --name sql -d mcr.microsoft.com/mssql/server:2017-latest -p 1434:1433 maps the 1433 port on the container to port 1434 of your local environment.\nOnce you have run this command you can connect SQL Server Management Studio (SSMS) to (local) or (local),1434 if you are using a different port using the credentials you provided and execute any SQL you like.\nIf your development environment requires windows authentication this of course is not for you, if it doesn’t you are good to go.\nThe development environment I have been using has various powershell scripts for setting things up. These assume windows auth. However I have adapted them to take custom credentials.\n$credential = Get-Credential $server.ConnectionContext.LoginSecure=$false $server.ConnectionContext.set_Login($credential.UserName) $server.ConnectionContext.set_SecurePassword($credential.Password) The Get-Credential command creates a dialog where you can enter SQL credentials, this is then stored in a variable and used in the rest of the script.\nHow do I restore a backup file to my container?\nRun:\ndocker exec -it sql mkdir /var/opt/mssql/backup docker cp database.bak sql:/var/opt/mssql/backup This creates a backup folder and copies a backup file from your local environment to the container. You can then use management studio to restore the backup file (or you could write a sql script to do it). One thing to note when restoring databases, make sure the files are restored to Linux locations not windows locations.\nThe only issues I have encountered so far are the lack of support for SSIS packages and no windows auth. There are sql server windows images available which I haven’t tried yet which may work better with some of these options.\n","date":"Nov 5, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/running-sql-server-on-a-linux-container-using-docker-for-windows/","series":null,"tags":["Windows","Programming","Docker","SQL","Linux"],"title":"Running SQL Server on a Linux Container using Docker for Windows"},{"categories":null,"content":"Remember me? I used to write blog posts but somehow life got in the way. So much has happened this year, lets have a look at my goals for 2018 and see how far we have got with them.\nBuy a house My first goal for 2018 was to buy a house. Well so much progress has been made with this goal.\nWe looked at a few houses, and have made an offer, had it accepted and are waiting for the last few things to get arranged before we start arranging to move in.\nThe location we decided upon was Thorne . This is about an hour from York where most of our friends are and my wife has lived all her life, and also an hour from where my family live. I have tried my best to balance location and size of property and we think our new home is going to be a good balance but only time will tell.\nI have a bit of a mammoth time ahead of me getting everything we own packed up and moved to our new home so wish me luck, but we will get there it is just going to be hard work.\nNew Job In June 2018 I started a new job. This was slightly later than I had hoped but I believe it has been worth the wait.\nMy new job has lots of perks. Finish early on a Friday (1pm), On site canteen, Cool air-conditioned office, Flexible working hours, Part of a big development team.\nThe main benefit is that I am part of a big development team. I have a huge list of things to learn both in terms of technology and how large development teams function. The team is about to finish the first sprint that I have contributed to. My contributions have been very small. A bug fix here, a tweak of an API there. But the important thing is that I am making a contribution and learning along the way.\nI have also started asking questions about deployment and how this could be automated. Something that interests me, especially with the sysadmin and devops background. I have been tasked with learning docker , so expect some blog posts on this topic in the near future.\nSee more of family This one I am going to count as a miss. I have seen family a few times this year, but my efforts have been more on the first two items in this list. Hopefully this will change as things settle down.\nCelebrate 5 years of Marriage Me and the wife had a great weekend away to celebrate our wedding anniversary.\nCelebrate my two children Not happened yet but it is booked and we just need to arrange a few more details.\nLightning Talk I have made no effort to do a talk and this is going to be a goal for 2019 I think. Most of the reason is concentrating on the other areas above, but also because I am not a natural speaker and I don’t like stepping out of my comfort zone. But I will do it, I just need to wait for other things to calm down a bit first.\nFamily Holiday We had a great week away at the start of July. The weather was really great (and still is) and we had lots of time doing things as a family.\n","date":"Jul 28, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/2018-the-story-so-far/","series":null,"tags":["Career","Programming","Home","Family","Job","Goals"],"title":"2018 The Story So far"},{"categories":null,"content":"Recently I was asked how to convert a number to a string. Let\u0026rsquo;s look at a few ways of approaching this problem.\nMost objects in c# have a method called ToString() which displays the string representation of that object. This is because of inheritance, all objects inherit from System.Object which defines ToString().\nInt32 is a struct so it inherits from System.ValueType which also inherits from System.Object\nso in code\nint a = 9; string b = a.ToString(); Now let\u0026rsquo;s look at the reverse. However the reverse runs the risk of throwing an error, let\u0026rsquo;s look at why.\nstring b = \u0026#34;9\u0026#34;; string c = \u0026#34;a\u0026#34;; string d = \u0026#34;two\u0026#34;; All are valid strings but only one can be converted to a number. Use the TryParse method to convert to a number.\nint.TryParse(\u0026#34;9\u0026#34;, out int e); TryParse will not throw an exception if the conversion fails, if it succeeds variable e will contain the result. Note an earlier version of c# required you to define the out parameter before using it with TryParse.\nint.Parse exists to do the same thing however it will throw exceptions if a conversion is not possible. The same is true if you use Convert.ToInt32(\u0026ldquo;two\u0026rdquo;);\nCasting Casting is a way to explicitly telling the compiler that a type is actually another type and you are aware data loss will occur.\ndouble x = 4.5; int y = (int)x; However it is not possible to cast a string to a number format as a string can contain any character not just number characters.\n","date":"May 7, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/casting-and-converting-between-types/","series":null,"tags":["C-Sharp","Programming","Casting","Convert"],"title":"Casting and Converting between types"},{"categories":null,"content":"Let’s Encrypt is a free way to get a SSL certificate onto your website and until recently I had never tried it. It is very easy and I think it is awesome.\nIIS is the web server software the Microsoft include with Windows 10 and Windows Server. I have it installed on my laptop and it displays the default IIS page.\nIt is not really a good idea to host websites on your laptop, use a dedicated web server, or host with a hosting company, however the techniques are the same and it gives me something to write about!\nIn order to point a domain name at what IIS on my machine was serving up I did the following:\nDo a google search for “whats my IP”, this will return your public IP. Most residential ISPs use dynamic IPs so it may change over time, (which is another reason not to host a website on your laptop!) Add an A record on a domain with the IP address you have just got Your public IP most likely points at your router not your laptop so enable port forwarding of port 80 and port 443 to the internal IP of your laptop (something like 192.168.0.11 etc) Now comes the fun Let’s Encrypt stuff!\nFirst you need a Let’s Encrypt client, there are a lot of them out there mostly for linux flavours, however a bit of googling found a windows one. Go to https://github.com/PKISharp/win-acme/releases and download the zip file and unzip it.\nRun the executable from the zip file and follow the onscreen prompts.\nPress N to create a new certificate.\nThen press 1 to bind to single website found in your IIS setup\nAnd now magically Let’s Encrypt knows what you have setup in IIS.\nNow all you need to do is enter an email address incase a renewal fails and agree to the let’s encrypt terms and you are all setup.\nHow awesome and easy is that for getting your websites working with a SSL certificate. If you have IIS configured on a server, give it a try and you can SSL all your things.\n","date":"Apr 30, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/lets-encrypt-is-awesome/","series":null,"tags":["DevOps","Programming","SSL","Security"],"title":"Let’s Encrypt is awesome"},{"categories":null,"content":"DNS is the backbone of the internet and as such I believe every developer should know something about the basics and not just leave it for the sysadmin to sort.\nWhat is DNS? DNS or Domain Name System is what translates Domain names to IP addresses and vice versa.\nWait what is an IP address and what are domain names? You do realise this is a developer blog? An IP address is a unique address on the internet and a domain name is a user friendly label for one or more of these.\nAn example might be google.com which for me resolves to 216.58.204.14\nHow it works When your browser makes a request to google.com it makes a request to your ISPs DNS Servers. This resolves google.com to 216.58.204.14\nIn more detail your ISPs DNS server will forward the DNS query to another DNS server and will cache the results for a set amount of time. This is the TTL or Time To Live. Next time the ISP DNS Server will be able to reply directly without needing to forward requests.\nThis forwarding and caching is what makes making a DNS change not instantaneous. The TTL needs to be reached so that no results are still being fetched from the cache of DNS servers across the globe.\nDNS Records Now we know roughly how DNS works let’s look at the most common type of records\nA A (Host) records are the most simple records which translate domain names to IPs\neg www.google.com to 216.58.204.14\nCNAME A CNAME (Canonical Name) record is different to an A record in that it maps a domain name to another domain name when no A record exists.\neg www.google.com to somethingelse.google.com\nTypically Azure makes use of CNAMEs for many of its services especially adding a custom domain name\nMX MX stands for Mail Exchange and is used for configuring email\nName Server Every domain has a number of Name Servers which tells you what servers control the DNS settings for that domain. If you change your Name Servers then the new Name servers will be where you can change your DNS settings.\nIf you want to use a service like DNSimple instead of 123reg or where ever you registered your domain then all you need to do is change your Name servers.\nAAAA Like A record but for ipv6\nWhat next? Want to put some different DNS records into practise? Buy a domain name and publish some content to it. Check out my previous post about programmatically adding records . Want an SSL certificate? Get a wildcard one and then you can apply it to any subdomain you add to your domain.\nIf you have a new website you want to publish consider which of the following is better:\nhttps://www.example.com/newsite https://newsite.example.com I much prefer the second option, it looks cleaner, there is no potential conflict with the parent site, no subfolder issues between production and development.\n","date":"Apr 9, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/dns-for-developers/","series":null,"tags":["DNS","Programming","DevOps"],"title":"DNS for Developers"},{"categories":null,"content":"Reviewing code is a great habit to get into. Code reviews help share knowledge between your team members and help catch bugs before they get into production. But how do you get into the habit of reviewing and avoid the we don’t have time to do this mentality?\nVisual Studio Team Services (VSTS) has some great options that can help make code reviews second nature.\nPull Requests A lot of source control systems have the concept of pull requests. This is where you request others to review your code usually in a branch and if they approve it, merge it into a main branch.\nTo create a pull request in VSTS go to the Code section and select Pull Requests. Often VSTS will make a suggestion of what branch to make a pull request for, if you don’t see this just click the New Pull request button.\nSelect a branch you want to merge from and a branch that should be merged into (usually you merge into master from a feature branch). Give your pull request a title and description and select who should review your code, this can either be an individual or a group of people. You can also review all the changes that will be reviewed so you can make any last minute changes before it is reviewed.\nNow if you are anything like me you want your code merged in as soon as you have created your pull request and there is nothing stopping you reviewing your own code and clicking approve and merge on your own pull request. However branch policies is a way around this problem.\nBranch Policies Branch policies allow you to specify how your code gets merged in.\nGo to the list of branches in VSTS and select branch policy and you will see a whole host of options to customise the merge process. If you do this on the master branch you will not be able to commit any changes to master without it going through a pull request.\nThe first option enables you to select how many reviewers are needed on your code. If no one else works on your project best not setting this, but for everyone else setting at least one person to review your code is a great practice.\nNext you can ensure that your pull request is linked to a work item, this helps keep ensure you are actually fixing issues and not just making change for the sake of it.\nCheck for comment resolution is a good setting to enable. This ensures that if your reviewer has commented about you needing to change this line here, it ensures that you do.\nEnforce merge strategy allows you to choose between fast forward merge or squash merge.\nBuild validation enables the code to be built using a build definition you have configured. This is a great way to check code builds or tests pass before it gets merged in.\nThe last two options allow you to specify code reviewers and third party external services.\n","date":"Apr 2, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/codereviews/","series":null,"tags":["Git","Programming","Visual Studio"],"title":"Code Reviews"},{"categories":null,"content":"A friend asked me how to get started in Android Development and I thought I might have a go at answering that question here.\nI am by no means an expert in Android development, I do have an app in the play store so I know something.\nManifest File This is probably the easiest option and also doesn’t actually create an android app so I am not sure if it should be included in this list of not.\nIf you have a website and you want to create an app for that you could just create a manifest file and add this to your website.\nOnce your website has a manifest file, if you visit your website using a mobile phone or tablet you will get the option to add a shortcut to the home screen. You then have an app like experience in that you can click an icon to launch your website.\nA manifest file is a simple text file which specifies a few settings like the icon size, filename, what page loads when clicked and name of your “app”\nMore information Visual Studio This is the option I know most about as is what I have used.\nIf you are familiar with Visual Studio you can use the Xamarin Forms software to create your app in C#. Xamarin Forms allows you to easily create cross platform apps that run on Android, IOS and windows phone. So far I have only experimented with Android but it should be relatively easy to extend my code to run on other platforms.\nXamarin Forms allows you to write one a single codebase that can be compiled to run on the different platforms. Xamarin requires the use of XAML a XML like markup language for designing UI elements.\nMore Information on Visual Studio , Xamarin Forms Android Studio I don’t know much about this option so do correct me if I don’t get the details correct.\nAndroid Studio can be downloaded from Google this allows you to create java code to run directly on an android device. From what I know this is fairly similar experience to Visual Studio but instead of writing your code in C# you use Android Studio and write it directly in java.\nMore information Cordova Cordova allows you to use HTML, CSS and javascript to create cross platform apps. I have no idea why I haven’t heard of this technology until today as it sounds very flexible especially if you know a little bit of javascript.\nMore information To summarize there are lots of different options available to create an android app. What you choose depends on what you want to build, what language and experience you have and if your app needs to be cross platform.\n","date":"Mar 26, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/android-development-options/","series":null,"tags":["Cordova","Programming","Android","Cross Platform","Manifest","Xamarin"],"title":"Android Development Options"},{"categories":null,"content":"For a while I have been mentoring a friend and I thought I might share some top tips I have implemented in my career so far.\nHard Work At school I was told: “Only in the dictionary does Success come before Work“. Yes it is a bit of a corny saying however it has stuck in my head and it’s true. If you want to get anywhere you need to work at it. For many years I just drifted along, but as soon as I knuckled down and worked hard my career started going somewhere.\nWrite a Blog Writing is an important skill that you need to work at to improve. If you commit to writing one blog post a week this will help to improve this skill. Even after 150+ blog posts I still feel I need to improve. Not only that it will also help others learn something from you and shows a willingness to contribute to the community. If a Hiring Manager reads your blog he can see some of the stuff you know or care about and may help give you the edge over more experienced candidates.\nSoft Skills This is one I need to work on but you should concentrate on the so called “Soft Skills“. Soft Skills are the skills you have for dealing with others. Are you good at talking to people? Or good at getting requirements out of business owners? If so you have some soft skills, don’t underestimate the value that these bring to a team and keep developing them.\nBuild a Side Project Start building a side project, it doesn’t matter what it is but start building something. I started building a Xamarin app but I have learnt loads of other connected things while doing this, ViewModels, APIs, Build and Deployment processes, Azure Functions. It also gives you ammunition for things to blog about. If you are short of ideas try solving a problem or rebuild something you use.\nUse your spare time During the day there are moments you can reclaim for learning stuff. There are lots of podcast which discuss useful development topics, listen to these while driving to work. Subscribe to pluralsight and listen to this while washing up. Don’t get too concerned with learning everything, however think how much more you are learning than not listening at all.\nI’m not ready! We are never ready to take that next step, it is so easy to make excuses like I need to learn x, or know y. The only way to know if you are ready is to try and to keep trying. Iterate as you go so you keep improving yourself. If you wait before you start trying you will end up waiting forever. If you start trying now you will make some small progress and have a better idea of what you need to do to achieve your goals.\n","date":"Mar 19, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/tips-for-developing-yourself/","series":null,"tags":["Blogging","Programming","Goals"],"title":"Tips for Developing Yourself"},{"categories":null,"content":"Who was your hero growing up? Mine was Stephen Hawking.\nWhat is a hero? A lot is said in the media about “Heroes”. Often the sportsperson of the hour is described as a hero. I don’t like this definition and I think there is far more to being a hero than that. One definition I saw describes a hero as A person who is admired for their courage, outstanding achievements, or noble qualities.\nWhy Hawking? As a kid I thought I was pretty smart (Not really just above average), I liked Star Trek and science so Hawking was a pretty obvious choice. I remember reading all about Hawking and his life fascinated me. A brilliant man was struck down by motor neurone disease in his early twenties, however this did not stop him and he completed his doctorate and became the famous physicist we all know and love.\nMy physics career was fairly short lived ending with an undergraduate course at University of York, however I really enjoyed learning the concepts that he described about space and time and was definitely a driving factor for me to learn as much as I did about Physics.\nHow heroic is it to be given 2 years to live and then not only survive but to expand our understanding of the universe without being able to speak or move without the aid of technology?\nI can not imagine what it must be like to not be able to communicate without the use of a computer, but this has been Stephen Hawkings world for over 30 years. His computer system has undergone various upgrades over the years however his synthetic computer voice has continued as he identified with it and prefered it.\nStephen Hawking had an immense intellect, I would probably describe him as one of the cleverest people on the planet.\nWho are your heroes? What kind of person is your hero, is it a scientist or engineer? The world contains some very clever people who will inspire and excite young people to strive towards something. I am hopeful that my sons can be inspired even if it’s only a small way by someone like Stephen Hawking.\n","date":"Mar 16, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/heroes/","series":null,"tags":["Stephen Hawking","Hero"],"title":"Heroes"},{"categories":null,"content":" A new version of Pwned Pass is available from Google Play .\nA couple of weeks ago Troy Hunt released V2 of Pwned Pass onto his haveibeenpwned website. There are now over half a billion passwords that have appeared in data breaches for you to search.\nThis time Troy has included information about how many times a password has appeared in his data. So “password” returns a value of 3,303,003 so is a really, really bad choice of password to use. However the password “windows 10” has only appeared twice so is much better, however I would still recommend avoiding using it.\nOne of the great things about the internet is that on the day of release I was tweeting Troy so I could get my android app updated to take advantage of these new features. I even helped him identify an issue with the documentation on his site. He has also kindly add a link to my app onto his website.\nThat conversation and link has lead to a massive increase in app downloads. Before I was going steady on about 14 installs, now I have 86 active installs. I am not sure if this will increase anymore but if you are reading this because you downloaded my app, then thank you.\nMy app has four main features:\nPassword Check – this allows you to search a password and see if it has appeared in a data breach and also the number of times that password has appeared.\nEmail Check – this allows you to search your email address and see which data breach it has been involved in.\nList of Data Breaches – this lists the data breaches from haveibeenpwned.com you can also sort by Date Added, Number of accounts and name.\nCalendar of Breaches – this shows a github like chart of what days breaches are added\nIf you like my app do let me know. I have received a few ratings on google play it would be great to get a few more. If you want me to add a feature or have ideas of how I could improve it let me know as well.\nTo download take a look at https://play.google.com/store/apps/details?id=pwnedpasswords.pwnedpasswords ","date":"Mar 5, 2018","img":"https://lh3.googleusercontent.com/qZTLiWRfQwS7pT6gExLMTvU1pl8NtTU-kbeoSzD68p1D-EPY3Pg7cz2AN66QJpTTp1c=h900-rw","permalink":"https://www.funkysi1701.com/posts/2018/new-version-pwned-pass/","series":null,"tags":["Android","PwnedPass","App","Programming"],"title":"New version of Pwned Pass"},{"categories":null,"content":"The code base I am working on contains a huge if block. By huge I mean 77 if statements one after the other, each if checks to see what page id you are on and loads different content. This is not easy to maintain and I want to refactor it.\nOne option would be to replace the if statements with a switch block. However this is just as unmanageable as the huge if block. Lets look at a better option.\nPolymorphism is where you create a base class and then create sub classes from it. In my case I created an interface IPage with a single method CreateContent.\npublic interface IPage { string CreatePageContent(); } and then create 77 classes for each page which implemented this single method.\nNow comes the fun bit how do I call the correct page class from my original code?\nI created a dictionary than maps page ids to the class names.\npublic static Dictionary\u0026lt;PageIds, Type\u0026gt; PageIdToClass = new Dictionary\u0026lt;PageIds, Type\u0026gt;() { { PageIds.HomePage, typeof(HomePage) }, { PageIds.ContactPage, typeof(ContactPage) }, //etc } This is the one step I am not 100% happy with as I think it may be possible to remove or simplify this step.\nNow I have a way to map ids to classes I can write a class to do this.\npublic class MyPage { IPage _repo; public MyPage(int pageId) { PageIds p = (PageIds)pageId; Type t = PageIdToClass[p]; ConstructorInfo constructor = t.GetConstructor(new Type[] { }); _repo = (IPage)constructor.Invoke(null); } public string Create() { return _repo.CreatePageContent(); } } So in my constructor I take the pageId and pass it to my dictionary to get which subclass to load. I then get its Constructor and invoke it.\nNow I can remove the huge if block and replace it with a single line of code.\nvar page = new MyPage(pageId); On the face of it this change might look like a lot of work for not much gain as we started off with one file and now we have the original file, an interface, 77 subclasses and the MyPage class. However the original file is a lot more manageable and each sub class can be altered independently of each other.\nThis is a big step towards making this code more maintainable, there is always more that can be done but that can wait for another day.\n","date":"Feb 26, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/refactoringifstatements/","series":null,"tags":["Design","Class","Interface","Programming"],"title":"Refactoring if statements"},{"categories":null,"content":"One of the websites I have been working on has been displaying an error in the console. The error reads as follows.\nThe SSL certificate used to load resources from https://example.com will be distrusted in M70. Once distrusted, users will be prevented from loading these resources. See https://g.co/chrome/symantecpkicerts for more information. But what does this mean? Well let’s start by looking at the link provided.\nIn January 2017 it was revealed that Certificate Authorities run by Symantec which include Thawte, VeriSign, Equifax, GeoTrust, and RapidSSL had been issuing certificates that did not comply with baseline standards.\nStarting with Chrome 66, Google has decided to remove trust for these certificates. Chrome 66 is due for release around 17th April. My error mentions M70 so what does that refer to?\nChrome 70 which is due to be released in October 2018 will removed the trust for another batch of Symantec certificates.\nIf you are getting one of these errors because you are using a certificate that is going to be distrusted what will your site look like in Chrome 66 or Chrome 70?\nWell Chrome 66 is now in the dev channel so we can give it a try. Not very nice for your users is it? Now is the time to order a new SSL certificate to avoid this happening to your site.\nI first saw this error a few months ago and have been reading up about it and waiting for Chrome 66 to reach the dev channel so I could test what it did to my site. However now that I have Chrome 66 installed I spotted the intranet for the company I work for is also affected. I do not directly work on the intranet so I notified the security team that they may want to look into this.\nUnfortunately the response I received has been that Google needs to fix this before Chrome 66 is released. I am not criticising my employer or the security team, however this isn’t something Google can just “ fix “.\nThe certificates issued were issued by a CA that had issues so in order to maintain the trustworthiness of all certificates Google had little choice but to distrust them. Google and security experts need to be making more of a fuss about this and I am joining in on making a fuss by writing this blog. Scott Helme estimates that there are about 7000 websites which may be affected by the M66 and M70 distrusts.\n","date":"Feb 19, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/ssl-distrusts/","series":null,"tags":["Certificates","SSL","Security","Programming","DevOps"],"title":"Chrome distrusts SSL Certificates"},{"categories":null,"content":"A content Security Policy or CSP is a HTTP response header that defines what sources of content can be loaded on a web page. It is a way to combat Cross Site Scripting (XSS) attacks.\nWhat is a XSS attack then? When you load a webpage it also loads various other resources like images, some css style sheets, various javascript files that you want to run and probably many other things.\nHow do you know that you can trust all of these things? If you created them and they live under you control then the answer is probably yes. However these days you will probably want to use resources from across the internet, like youtube videos, google analytics, disqus comments, jquery libraries from a cdn etc and you can’t be sure exactly what they are doing.\nImagine you had a page which you could add any text into a form which would then be displayed. A malicious user could add evil javascript or get the browser to load evil code from anywhere on the internet.\nCSP to the rescue! A CSP allows the browser to only load from sources that you specify. You could specify that resources from your own site will load but the evil script will not.\nLet’s look at some examples\nContent-Security-Policy: script-src \u0026#39;self\u0026#39; This allows \u0026lt;script\u0026gt; tags to only load from the current webhost. script-src is not the only keyword you can use, let’s look at some of the others.\nscript-src – control what \u0026lt;script\u0026gt; tags will load\nstyle-src – control what css will load\nimg-src – control what images will load\nframe-src – control what frames will load\nfont-src – control what fonts will load\nobject-src – control what object tags will load\nconnect-src – control what resources a script can connect to\nmedia-src – controls what media (audio/video) will load\ndefault-src – if no specific rule exists then the default directive will run\nContent-Security-Policy: default-src https This allows any content to be loaded from any site as long as it comes from a secure (https) site\nContent-Security-Policy: default-src https://example.com This allows any content to be loaded from https://example.com only.\nHow do I use this on my site? I have added CSPs into my web.config which works great for my .Net Framework code.\n\u0026lt;system.webServer\u0026gt; \u0026lt;httpProtocol\u0026gt; \u0026lt;customHeaders\u0026gt; \u0026lt;add name=\u0026#34;Content-Security-Policy\u0026#34; value=\u0026#34;default-src https://example.com\u0026#34; /\u0026gt; \u0026lt;/customHeaders\u0026gt; \u0026lt;/httpProtocol\u0026gt; \u0026lt;/system.webServer\u0026gt; For .net core it is a bit more complex as you don’t tend to use web.config files, however check out Anthony Chu’s post , which has a solution to that problem.\nReport Only One last thing about CSPs to mention is the Report Only flag.\nContent-Security-Policy-Report-Only This does the same as the above but doesn’t enforce anything, so you can fix any problems before you break anything.\nTo view your issues just look in the developer tools in your favourite browser. Or you can configure all your reports to be collated in one place with a report-uri directive.\nContent-Security-Policy: default-src https://example.com; report-uri https://example.report-uri.com/r/d/csp/reportOnly; Scott Helme and Troy Hunt have a site called report-uri which offer a service for collating and viewing all your CSP violations so check it out if you want to know more about CSPs.\n","date":"Feb 12, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/content-security-policies/","series":null,"tags":["CSP","ReportURI","Security","Programming"],"title":"Content Security Policies"},{"categories":null,"content":"Earlier this year I signed up for pluralsight. If you want to sign up to pluralsight as well use this link http://referral.pluralsight.com/mQd8BJ4 to get money off.\nPluralsight is a website that sells training videos on a wide variety of technical topics. You sign up for a monthly or annual subscription and you can watch over 6000 courses whenever and where ever you like.\nI am not a big fan of videos as if I am sat in front of a screen I would rather be building something, however I do spend a lot of time listening to podcasts. So I have decided to convert this time to listening to pluralsight videos.\nIf we assume that by not seeing the visual portion of the videos you lose out on 75% of the learning (This is just a guesstimate I am not sure what the exact figure is or how you would calculate it). However this is still 25% more learning than if I hadn’t signed up at all.\nSince I signed up I have listened to over 27 hours of technical videos. These have covered topics like C#, Security, Xamarin, MVVM, Interfaces, Getting Involved and Clean architecture. The number one thing I have learnt is that there is a lot that I still don’t understand.\nThat said I would like to think that since starting to listen to pluralsight concepts have gone from being unknown unknowns (I don’t know about them and I don’t understand them) to known unknowns (I know about them but don’t yet fully understand them)\nTo keep track of your learning pluralsight has some tests which you can complete. I am particularly proud of my score on the Azure test which ranks me as an expert.\nWhy not check out what you can learn from pluralsight?\n","date":"Feb 5, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/pluralsight/","series":null,"tags":["Learning","Training","Pluralsight","Programming","C-Sharp"],"title":"Pluralsight"},{"categories":null,"content":"I have blogged a few times about interfaces , and how useful they are for producing good quality maintainable code. Let’s look at a problem and the solution I came up with which I am quite proud of.\nAs previously mentioned I am in the process of moving images from AWS to Azure blob storage. Now that the actual files themselves have been moved I need to change the code that references them.\nNow I could find all the code that uses the AWS API and replace it with the Azure API but I am not very good at predicting the future, we may stay on Azure for a while, we may move to AWS or Google Cloud, or we may want to go back to files sitting on a server.\nLets try and code a solution that is as flexible as possible. As you have probably guessed I am going to create an interface.\nAt first I thought about creating an interface called ICloudStorage , however this isn’t flexible enough as what happens if we go back to sticking files on a server so instead I created IStorage.\nI created three classes that implemented IStorage, AWSStorage , AzureStorage and mostly for testing at the moment FileStorage. I then created a class Storage that would call these three classes. Initially I created it like this\npublic class Storage { private IStorage _repo; public Storage(IStorage repo) { _repo = repo; } } However this would require I call it like Storage(new AzureStorage()) and I would need to know everywhere in my code which implementation I want to use. This isn’t too bad as when we change from AWS to Azure we would need to do a find and replace throughout the code and replace all AWSStorage and make them AzureStorage.\nHowever we can do better than that.\npublic class Storage { private IStorage _repo; public Storage() { Type obj = Type.GetType(ConfigurationManager.AppSettings[\u0026#34;DefaultStorageRepository\u0026#34;]); ConstructorInfo constructor = obj.GetConstructor(new Type[] { }); _repo = (IStorage)constructor.Invoke(null); } } This code will read from the web.config which implementation to use and that will decide which class to call. This means that to change from AWS to Azure we do not need to redeploy any code, all we need to do is change the web.config.\nLet’s look at the three lines and see if we can understand what is happening.\nType.GetType() looks straight forward and gets the type from the web.config\nobj.GetConstructor() This gets the constructor for the type we have just found.\nconstructor.Invoke This then invokes the constructor and it then gets cast to the interface so can be used by the _repo variable.\nThis is all fairly simple and makes sense, however it has produced some very flexible code and allows the code to be extended without recompiling.\nLet’s look at a hypothetical example. We want to add support for Google Cloud Storage. All we need to do is create a class library which implements the IStorage interface, place the compiled binary in the website and update the web.config to reference it. I haven’t tried this hypothetical example so it might be more complex than I think but in theory it should work.\nI am pretty excited at how flexible this code can be, hopefully I will use code like this more often now I understand it.\n","date":"Jan 29, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/flexible-architecture/","series":null,"tags":["Architecture","Programming","Azure"],"title":"Flexible Architecture with Interfaces"},{"categories":null,"content":"We are in the process of moving our companies websites onto the Azure platform. One of the challenges was to move image files out of the website project into blob storage. This week I have moved 150,000 of them.\nOne thing I keep banging on about is that your source code should not contain data. If it does every time you do a deployment you need to consider where these images are located and ensure you don’t overwrite or loose any. It also goes without saying that deployments of a few Mb are a lot quicker than deployments of 100s of Mb.\nAzure blob storage also gives you advantages like distributing storage across multiple datacenters which would be impossible with traditional files on a server.\nSo now that we have established that this is a good idea lets look at how we could move large amounts of data. In my case all the filenames are stored in a SQL database so the plan of action was to simply loop through the files in the database, download from current storage (either locally or other cloud storage), upload to Azure and tidy up afterwards. Due to the number of images I am going to update the database and mark when a file has been processed so I can do the move over several days.\nThis is my code\nvar source = \u0026#34;https://example.com/images/\u0026#34;; var tmp = Server.MapPath(\u0026#34;~/tmp/\u0026#34;); if (!Directory.Exists(tmp)) { Directory.CreateDirectory(tmp); } var fixturePhotos = db.Images.Where(x =\u0026gt; x.Moved == null || x.Moved == 0).Take(id); foreach (var photo in fixturePhotos) { try { string path = getFilePath(photo.FileName); if (!Directory.Exists(Server.MapPath(\u0026#34;~/tmp/\u0026#34; + path))) { Directory.CreateDirectory(Server.MapPath(\u0026#34;~/tmp/\u0026#34; + path)); } WebClient WebClient = new WebClient(); WebClient.DownloadFile(source + photo.FileName, tmp + photo.FileName); FileUploader f = new FileUploader(tmp + photo.FileName, photo.FileName); System.IO.File.Delete(tmp + photo.FileName); photo.Moved = 1; } catch { photo.Moved = 0; } } db.SaveChanges(); if (Directory.Exists(tmp)) { Directory.Delete(tmp, true); } First of all I create a tmp folder in the root of my website if it doesn’t exist to store my images temporarily.\nI then use an entity framework model to query the database that haven’t been moved, and I use the take() method to limit how many results I process. (I have been passing in 1000 at a time)\nI then use a foreach loop over all these files to perform the following actions.\nCreate additional subfolders if the filename variable stored in the database isn’t actually a filename but a filepath, note you will have to split filename and filepath which I haven’t included code for here. Download file from the original url and save into the temporary folder Upload to Azure Delete temporary file Update database giving a success or fail Once the foreach is finished I commit the database changes and delete the temporary folder. I am sure there must be other ways to do this transfer but this was quick and easy to setup and now I have a copy of all the files in Azure storage so I can test out other issues with my website.\nOne last tip about how to schedule this code. I called the above code from a MVC controller and then wrote a Azure Function to call this code on a schedule.\n","date":"Jan 22, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/moving-blobs-cloud-suppliers/","series":null,"tags":["DevOps","Programming","Azure"],"title":"Moving files into blob storage"},{"categories":null,"content":"Last week I looked at testing the UI of mobile apps , this week lets look at how we could do a similar thing for websites.\nTesting the user interface is not an excuse for a lack of unit tests . Testing the user interface takes longer so for keep creating your small unit tests that can be run after ever build. That said lets look at how you create a UI test.\nCreate a Unit Test project as normal. Now install the following nuget packages\nSelenium.WebDriver.ChromeDriverSelenium.WebDriverSelenium.WebDriver.PhantomJS.Xplatform I am going to be using Selenium to achieve my website testing and I am going to concentrate on the Chrome browser. However packages exist for other browsers so have a look at the following and I expect there are others as well.\nSelenium.WebDriver.IEDriverSelenium.Firefox.WebDriver Selenium started life as a plugin for Firefox to help create automated tests, however the latest version of Firefox is not compatible with the plugin as I write this. I have not had to install any plugins or extensions to my browsers to achieve my testing.\nEnough talk lets write a test. First we create two instance variables to store the baseURL and the driver for the browser you are using.\nprivate string baseURL = \u0026#34;https://www.example.com/\u0026#34;; private RemoteWebDriver driver; Next we need to set things up for the test to run. This creates an instance of the chrome driver, maximizes the window and sets it to wait 30 seconds before timing out.\n[TestInitialize()] public void MyTestInitialize() { driver = new ChromeDriver(); driver.Manage().Window.Maximize(); driver.Manage().Timeouts().ImplicitlyWait(TimeSpan.FromSeconds(30)); } Now comes the actual test. We navigate to a URL and then compare the title of the page loaded with a know value with a Assert statement like you would find in a unit test.\n[TestMethod] public void CheckBrowserTitle() { driver.Navigate().GoToUrl(this.baseURL); Assert.AreEqual(\u0026#34;Home Page\u0026#34;, driver.Title); } Finally we need to tidy up after ourselves.\n[TestCleanup()] public void MyTestCleanup() { driver.Quit(); } If you are used to writing tests you will know that the are usually constructed in three sections Arrange, Act and Assert. The Arrange is done in the initialize method, which makes the actual test much simpler, the first line does the Act and the last line does the Assert.\nNow we have written a simple test lets look at something more complex.\ndriver.FindElementByLinkTest(\u0026#34;click\u0026#34;) This finds any Link on the page which is click. Be careful as the string need to be exactly what appears on screen it may well be easier to specify by id or class or something that doesn’t change as often. By adding .click() on the end of this command Selenium will click on the link and you can navigate to a new page.\nWhat about submitting form data? Well you can find the element you want to fill in and add .SendKeys(\u0026ldquo;example text\u0026rdquo;) or .Submit() and this will fill in and submit form data.\nWhat about a screenshot?\nScreenshot ss = driver.GetScreenshot(); ss.SaveAsFile(\u0026#34;test.jpg\u0026#34;,System.Drawing.Imaging.ImageFormat.Jpeg); I have only just started playing around with web UI tests but you can see there is a fair bit you can do.\n","date":"Jan 15, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/website-ui-testing/","series":null,"tags":["Testing","Website","UI","Programming"],"title":"Website UI Testing"},{"categories":null,"content":"Since I started creating an android app I have been writing simple UI tests.\nI have been taking advantage of the Visual Studio App Center which allows you to test against hundreds of different devices in the Test Cloud.\nIn order to write a UI test create a UI Test App, this makes use of the nuget package Xamarin UI Test. By default you will now have a test called AppLaunches which will take a screenshot of you app after it starts.\nYou can now run this test against any device from Visual Studio assuming you have it physically plugged into your machine. However, how do you run against the Test Cloud?\nTo run against the Test Cloud you need to first install node.js. Now you can install the appcenter cli with the following command.\n**npm** install -g appcenter-cli To run the tests in the Test Cloud run the following command\n**appcenter** test run uitest --app \u0026#34;\u0026lt;username\u0026gt;/\u0026lt;appname\u0026gt;\u0026#34; --devices \u0026#34;\u0026lt;username\u0026gt;/\u0026lt;deviceset\u0026gt;\u0026#34; --app-path _pathToFile.apk_ --test-series \u0026#34;master\u0026#34; --locale \u0026#34;en_US\u0026#34; --build-dir _pathToUITestBuildDir_ where is your appcenter username, is the name of your app in appcenter and is the group of apps you have created in app center to test against.\nLook at device sets in the test section of the appcenter and click the new device set button. You can then search for any device you like and add it to a set, as I write this there are 244 devices you can test against.\nIt is currently not possible within app center to run tests against a new build, however if you build your app in VSTS as well you can create build or release step that runs it. As the Visual Studio app center is still under development I wouldn’t be surprised if it is added at some point.\nIn VSTS look for Mobile Center Test in the definitions and you can specify the same variables as specified in the command line above.\nNow how do you actually write a useful UI test? I mentioned above you get a default test which contains the following code, this takes a screenshot of your app. However you don’t have to include this when using the Test Cloud as screenshots are included for free.\napp.Screenshot(\u0026#34;First screen.\u0026#34;); Lets look at what else is included in app\napp.Repl(); This starts an interactive REPL (Read-Eval-Print-Loop) which lets you explore what is on screen in your app and pauses execution. I don’t include this in my tests, however I do make use of it to explore what is on screen and what tests I might make use of.\napp.Tap(c =\u0026gt; c.Marked(\u0026#34;Button\u0026#34;)); This taps an element on screen called Button. There is also a method called TapCoordinates which would allow you to click anywhere you like.\napp.WaitForElement(c =\u0026gt; c.Marked(\u0026#34;View\u0026#34;)); After clicking a button you are probably going to want to wait for the app to load extra data or a new screen. The WaitForElement waits for an element to appear on screen. There are also methods that wait a period of time or wait until an element no longer exists.\nThese are the main methods I have used so far, however there is an extensive list including methods for scrolling, swiping, pinching and adjusting the volume buttons. So you should be able to test all manner of app functionality and if you make use of the Test Clouds will know which devices are causing problems.\n","date":"Jan 8, 2018","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2018/01/01-newproject-vs.png?resize=300%2C182\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2018/mobile-app-ui-testing/","series":null,"tags":["Android","Azure","App"],"title":"Mobile App UI Testing"},{"categories":null,"content":"It is 2018 so it must be time to think about what my plans and goals are for the new year.\nBuy a house This a huge goal for me. For many years I was content with renting and thought owning my own home wasn’t for me, but then I went and started a family and my career started going in the right direction and my thoughts about this have changed.\nIt is going to take a lot of work to make this goal happen in 2018. First I need to save enough money for the deposit, which is far from easy with a wife and two children. Second I need to get our current place sorted ready to physically move which again won’t be easy. Third I need to make a decision which will affect where I live for the next 25 years. At the moment I am torn between the size of property and its physical location, I need to balance both of these and meet the requirements from the rest of the family.\nI do think this is an achievable goal so wish me luck.\nProgress in my career I need to be careful how I phrase this in case past, current or future employers are reading. I have been working hard at my current role for over a year and I am starting to wonder what is next for me. I am not entirely sure what form this is going to take yet, I have a few ideas on the go and hopefully I can reveal more soon.\nAnother one that is very achievable, as long as I have it in the back of my mind as the year goes on progress should be made.\nSee more of family I recently missed a family celebration due to the expansion of my own family and it reminded me that there are aunts, uncles and cousins that I haven’t seen in years and it would be nice to reconnect. Also as my own boys get older and are learning to interact it would be great if they could get to know my own family better.\nI don’t have a clear idea how to achieve this one as it takes two but I am going to make more of an effort.\nCelebrate 5 years of marriage Wow how has it been five years since we got married? At some point in 2018 I will take time out from the usual and spend it with the wife away from the boys. I won’t give away more details as I want that to be a surprise.\nCelebrate the birth of my two sons I don’t want to get my boys christened as I would like them to make up their own minds when they are old enough, however I do want to celebrate all that they are with a thanksgiving service. Not sure when we are going to sort this, however it is my hope that it will be soon.\nFamily holiday Not much of a goal but in the summer I will take time out to spend a week away with the family. In previous years we have done scotland, wales and northumberland. This year I am thinking of somewhere not too far away as Edward is still small, but maybe near where I grew up.\nLightning Talk A lightning talk is a very short talk or presentation given at a conference or user group. Friends of mine are having lots of success talking about what they know about and I think it might be time for me to have a go. I am not a natural public speaker so this is a step out of my comfort zone, however I am going to start small and see what happens. At the moment I don’t know what my subject or topic is going to be, my suspicion is that it will be devops related as that interest me more but watch this space.\n","date":"Jan 1, 2018","img":"","permalink":"https://www.funkysi1701.com/posts/2018/lets-see-2018-can/","series":null,"tags":["Career","Family","Marriage","Programming","Goals"],"title":"Lets see what 2018 can do!"},{"categories":null,"content":"As 2017 starts to draw to a close let’s look at some of the highlights from the past year.\nEdward In November I became a father again and celebrated the arrival of Edward into the world. Before the birth I was worried how well my first son James would adapt to the new arrival but I am happy to say I didn’t need to, he mostly ignores the new arrival.\nStar Trek In September the latest Star Trek TV series Discovery arrived on Netflix. I have been very positive so far about the show, I am still positive however there are things that annoy me about the new show, however I am keen to see the next episode each week so they are definitely doing something right.\nOffice move In May I helped my employer move to brand new offices . This was a lot of hard work, but everything went as planned and our new home is great.\nSide projects I blogged in February about having a side project but it wasn’t until later in the year that this solidified into what I am working on now. It started as a way to understand interfaces this then led to learning Xamarin and creating an android app to help promote this blog. Troy Hunt expanded his HIBP API to include passwords that have been in data breaches, I then started working on pwned passwords which is a simple android frontend for this.\nLearning R In Feb I also experimented with a new programming language R . It started with a problem about converting exchange rates and ended with a SQL stored procedure that executed some R code, I still want to learn lots more but it was a good example of the sorts of thing you can build with R.\nDNS programmatically One of my most successful blogs was about how you can create dns records within your application eg sites that have yourname.domain in the url. It was a great chance to look at how dns works and learn more about what azure can do to manage this. I want to expand on this and build something.\nSo that was 2017, for some ideas about what 2018 might have in store for me look out for my next blog post.\n","date":"Dec 28, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/looking-back-at-2017/","series":null,"tags":["StarTrek","Goals","Programming","Azure","Baby","C-Sharp"],"title":"Looking back at 2017"},{"categories":null,"content":"Azure Table storage is cheap way to store data, however it has some drawbacks that you should be aware of.\nAzure Table storage is a simple way to store NoSQL data with key/attribute pairs. I am very familiar with storing data in SQL databases and would still choose SQL over Table storage, however Table storage is significantly cheaper so could be worth investigating depending on your project.\nTroy Hunt makes use of Table storage for his Have I been pwned? website so there are projects out there that make use of it to great affect.\nTo work with Table storage you need to use a nuget package WindowsAzure.Storage\nInstall-Package WindowsAzure.Storage\nTo load data from a Table in Azure Table storage I use the following code\nCloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString); CloudTableClient tableClient = storageAccount.CreateCloudTableClient(); CloudTable table = tableClient.GetTableReference(\u0026#34;tablename\u0026#34;); TableOperation retrieveOperation = TableOperation.Retrieve\u0026lt;Entity\u0026gt;(PartitionKey, Rowkey); table.CreateIfNotExists(); TableResult retrievedResult = table.Execute(retrieveOperation); Example eg = new Example(); if (retrievedResult.Result != null) { eg.ID = ((Entity)retrievedResult.Result).Id; eg.Date = ((Entity)retrievedResult.Result).Date; } return eg; You need to create a class (called Entity in the example above) derived from TableEntity which defines the Partitionkey and Rowkey, plus and other columns you want to store in table storage. The row key and partition key uniquely identify the data in the table, think of this as the primary key of the table if you are used to SQL. This class must also contain a parameterless constructor.\nThis is the only way to retrieve data, using the partitionkey and rowkey. If you want to retrieve a specific piece of data you would need to retrieve all rows and then search them for what you need. For me this is not a big problem as I only have 150 rows but if you have millions of rows you may need to think carefully how to use this.\nTo save data I use a very similar piece of code\nCloudStorageAccount storageAccount = CloudStorageAccount.Parse(path); CloudTableClient tableClient = storageAccount.CreateCloudTableClient(); CloudTable table = tableClient.GetTableReference(\u0026#34;tablename\u0026#34;); Entity post = new Entity(ID, DateTime.Now); TableOperation insertOperation = TableOperation.InsertOrReplace(post); table.CreateIfNotExists(); table.Execute(insertOperation); I create a Entity object and then pass this as a parameter into the InsertOrReplace method.\nTo delete data it is also very similar, you create an entity object and pass this as a parameter to the Delete method.\nWhen debugging my table storage code I found the Azure Storage Explorer very useful for seeing what data actually existed in the table and what might be throwing an error, usually something wrong with my Entity.\nI mentioned earlier that Table Storage was cheaper than SQL Azure. Well for my simple playing about with things I have found my monthly charge of £10+ has been reduced to £1+ If I were to build anything that is more than just me learning about how it works I would probably continue to use SQL but for the cost of learning new tech it is well worth giving table storage a try.\n","date":"Dec 17, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/azure-table-storage/","series":null,"tags":["Azure","SQL","Table Storage","Database","Programming"],"title":"Getting started with Azure Table Storage"},{"categories":null,"content":"Source control is bread and butter for web developers, however not so much for SQL developers and other business people. One of my goals for the coming year is to get the whole of my team using source control processes.\nOne of the arguments against using source control is there will always be a few exceptions where it won’t be used. Lets look at a few scenarios to help make the case for source control.\nScenario One\nWeb Dev 1 makes a change in production to fix an issue and doesn’t use source control. Web Dev 2 is working on a new feature and makes use of source control. He is given the sign-off to deploy his new feature and in the process undoes Web Dev 1’s fix.\nManagement blames Web Dev 2 as they “broke” stuff during the deployment. Web Dev 2 doesn’t understand what happened and Web Dev 1 is oblivious to the entire scenario despite in reality being part of the problem.\nScenario Two\nA SQL Dev works all night to fix a problem. He deploy to production and doesn’t notice a missed where clause due to lack of sleep.\n—\nLet’s look at these two scenarios done with the whole team buying into source control.\nScenario One\nWeb Dev 1 makes a change in production to fix an issue and also commits to source control. This takes him an extra 5 minutes (if that!) Web Dev 2 is working on a new feature and makes use of source control. He is given the sign-off to deploy his new feature and in the process merges Web Dev 1’s fix before he deploys to production.\nManagement is happy with Web Dev 1, nothing is broken and new functionality has made the website better not worse.\nScenario Two\nA SQL Dev works all night to fix a problem. He deploy to production and doesn’t notice a missed where clause due to lack of sleep, he also commits to source control this takes him an extra 5 minutes (if that!).\nIn the morning a colleague looks through the commits made during the night and queries the missing where clause. A fix is made in source control and deployed, management and the client are unaware that a problem was created and fixed.\n—\nThese are simple scenarios but I cannot think of any situation where the outcome would be better to not use source control. These examples assume no continuous deployment, adding this to the system before every team has bought into source control would cause bugs and issues all over the place.\nHowever with a team that is 100% behind source control continuous deployment can achieve an amazing productivity boost. After every commit code could be automatically tested, reviewed by other members of the team and deployed to test environments for further analysis. Only tested code that has been reviewed can get anywhere near production environments.\nHaving said all this there are ways especially on the SQL side to mitigate loosing changes while you work on training individuals and convincing management. Before any code is deployed to production do a schema compare. If you only see changes you have made you can proceed, if you don’t shout at your team.\nAdvantages\nCode Reviews\nTeam awareness of what is being changed\nHistory of changes\nSmoother Deployments\nAutomation\nBetter Teamwork\nDiff changes between different versions\nMany more\nDisadvantages\nTime to learn tools\nRemember to commit changes\nTechnologies that use binary files harder to version control\n","date":"Nov 13, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/get-zero-code-changes-outside-source-control/","series":null,"tags":["Git","SQL","Visual Studio","SourceCode","Programming"],"title":"How to get Zero Code Changes outside of Source Control"},{"categories":null,"content":"Writing SQL queries is typically done with SQL Management Studio (SSMS). However this tool is a bit of a beast so let’s look at how you could use Visual Studio Code instead.\nVisual Studio Code is a free text editor but it is so much more than just a text editor. VS Code can be downloaded from https://code.visualstudio.com/Download To work with SQL Server download the mssql extension. Press CTRL+SHIFT+P and then Select Install Extension and type mssql.\nIntellisense in Visual Studio Code is brilliant, better than SSMS. Lets look at how to get it all set up.\nCreate a new file and set the language type to SQL (Press CTRL+K,M )\nOpen the command palette, CTRL+SHIFT+P and type SQL to show the mssql commands. Select the Connect command.\nThen select Create Connection Profile , this creates a profile to connect with your SQL Server. Follow the prompts to get it all setup.\nLook in the bottom right corner of the status bar and you should see you are connected.\nNow if you type sql you will see a long list of SQL code snippets that you could use.\nChoose a snippet to create, and edit it as required. When you are happy press **CTRL+SHIFT+E ** to execute.\nThis is basically all there is to it. However this is an incredibly powerful way of working, the intellisense instantly tells you what database objects you can use in your query and there is a wealth of different snippets you can use.\nWhen returning data you get a similar view to SSMS but you can save as Excel, CSV or JSON.\nSSMS is a very graphical way of doing things, you can double click a table and see its columns or indexes. VS Code relies on TSQL commands but you have access to exactly the same information.\nFor more information about VS Code and the mssql extension check out https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-develop-use-vscode ","date":"Nov 6, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/sql-with-visual-studio-code/","series":null,"tags":["Database","SQL","Visual Studio","Programming"],"title":"SQL with Visual Studio Code"},{"categories":null,"content":"A while back I blogged about learning about interfaces as I didn’t really understand the value of them. I do now.\nI created an application that used interfaces so I could learn how it worked. I created a Logger Interface and created multiple classes that implemented that interface so I could swap out the different implementations easily. I created a SQL Logger and a File Logger and my code could be written and be completely unaware of which implementation it was using.\nThis application uses SQL Azure and so I have a monthly bill to pay. Wouldn’t it be cool if I could reduce this bill? How about using the cheaper table storage instead?\nEasy!\nCreate a new class that implements my interface and all I need to do is write the three methods defined in my interface and I can swap from SQL Azure to table storage.\nAnother benefit to interfaces is testing. Say I have an interface called inotification for sending notifications, I can have several implementations of this email, twitter, slack etc\nNone of these implementations should be used in unit tests, as you don’t want a tweet being sent every time you run your tests. Why not create an implementation that simply returns something for each method call and doesn’t actually do anything. I can then run my tests with my fake implementation which tests my code logic but not the implementation I have chosen (this can be tested later on with integration tests or user testing if required).\nThis is pretty much all I have to say about interfaces. I just like how I can swap different implementations.\nIt does take a bit of work to get the interface setup. I found that when writing the second implementation the interface would need to change slightly, mostly as it was badly designed to begin with. I think for beginners there may be some value to writing multiple implementations of an interface so you can be sure your interface is good, however I am sure with experience this will not be required.\n","date":"Oct 31, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/interfaces-are-cool/","series":null,"tags":["BadDesign","Interface","Programming"],"title":"Interfaces are cool!"},{"categories":null,"content":"DNS is the backbone of the internet. It converts domain names into IP addresses. But how can we do clever things with it?\nMaking DNS changes has always been a pain. You have to remember what company you registered your domain name with, then locate the login details for their website, login and fill in some web form and wait for the changes to replicate across the internet.\nThere are various websites that assign you a custom subdomain name like name.domain.com, obviously they must do this programmatically, they don’t have a guy following the above process.\nLets see how we could do this.\nMy first thought was to make use of the DNSimple API. https://developer.dnsimple.com/v2/ The API allows all sort of DNS changes to be made, however this is a paid service so let’s look at other options.\nAzure now has a DNS section. You can go to the portal, add a DNS zone, and then add as many A, CNAME and MX records as you need. This is of course no better than doing it manually.\nAzure offers a SDK and an example Visual Studio project. Lets look at how it works.\nFirst you need to setup some credentials to use your Azure subscription. The easiest way to do this is with powershell.\nLogin–AzureRmAccount $sp = New-AzureRmADServicePrincipal –DisplayName exampleapp –Password \u0026#34;{provide-password}\u0026#34; Sleep 20 New-AzureRmRoleAssignment –RoleDefinitionName Contributor –ServicePrincipalName $sp.ApplicationId This will create a AD Service Principal which has access to your Azure subscription.\nYou need to get the following IDs from the Azure portal.\nsubscriptionId for your subscription\ntenantId or the Azure AD Directory ID\napplicationId of the service principal created above\npassword you entered into the powershell script above\nvar serviceCreds = await ApplicationTokenProvider.LoginSilentAsync(tenantId, clientId, secret); var dnsClient = new DnsManagementClient(serviceCreds); dnsClient.SubscriptionId = subscriptionId; var recordSetParams = new RecordSet(); recordSetParams.TTL = 3600; recordSetParams.ARecords = new List\u0026lt;ARecord\u0026gt;(); recordSetParams.ARecords.Add(new ARecord(\u0026#34;1.2.3.4\u0026#34;)); var recordSet = await dnsClient.RecordSets.CreateOrUpdateAsync(resourceGroupName, \u0026#34;funkysi1701.com\u0026#34;, \u0026#34;test\u0026#34;, RecordType.A, recordSetParams); The above code connects to Azure and creates an A record for test.funkysi1701.com that points at the ip address 1.2.3.4.\nOther DNS records can be created in a similar way.\nThe above example creates a new RecordSet but you can use the following to delete or get existing records.\nvar recordSet = dnsClient.RecordSets.Get(resourceGroupName, \u0026#34;funkysi1701.com\u0026#34;, \u0026#34;test\u0026#34;, RecordType.A); dnsClient.RecordSets.Delete(resourceGroupName, \u0026#34;funkysi1701.com\u0026#34;, \u0026#34;test\u0026#34;, RecordType.A); Now I have found out that I can write a console app to edit my DNS records I need to change the nameservers for all my domains so I can take advantage of this new feature.\n","date":"Oct 16, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/creating-dns-records-programmatically/","series":null,"tags":["Azure","DNS","Programming"],"title":"Creating DNS records programmatically"},{"categories":null,"content":"I tried to resist but I am going to have to write about the new Star Trek series Discovery. Warning this post is going to include SPOILERS. If you read on you have been warned.\nI watched the first two episodes of the brand new Star Trek TV show. As I am an international viewer I used Netflix, if you are from the US you need to use CBS All Access. I have heard a lot of complaining that the show is not free to watch. CBS is making Star Trek Discovery to make money, if they don’t make money they will stop making it. Its as simple as that.\nNetflix costs £5.99 per month and CBS All Access costs $5.99 per month. I don’t believe this is a lot of money. The 5.99 above allows you to watch any of the 726 Star Trek episodes at any time of day whenever you want as many times as you want, plus you have access to all the other movies or TV shows available. I don’t think this is too much to ask.\nBack to Discovery. I loved it! It felt like Star Trek. I had feared it might only share the name and would be an action filled TV show that had little in common with what he had seen before. I was wrong this is definitely a show that can proudly call itself Star Trek.\nThe Klingons\nWhen it was first announced that the Klingons would feature in the new show I was a bit “meh”. We had done lots with the Klingons before and they had never been my favourite alien race. The look of the Klingons was also going to be changed, I have to admit this didn’t bother me. Klingons have had their look updated before. In 1966 Klingons had dark faces and smooth foreheads, in 1979 the forehead ridges were added as the first Star Trek movie was made, and now Discovery has removed all hair from Klingons. I can explain it away as productional changes, I don’t need an on screen explanation like we had on Enterprise (or even DS9).\nWhat I have seen so far is a menacing alien race that fits with what we have seen before with lots of references to honor, Kahless and speaking Klingon. I must admit reading all the subtitles is getting a bit tiresome, but that is a minor issue.\nConflict\nHistorically Star Trek has not featured conflict between the main starfleet characters due to the idea that humanity has evolved beyond this. DS9 got round this by having Odo and Kira who are not starfleet characters so can have a little bit of conflict. Discovery has completely abandoned this idea.\nIn the pilot episode, the Michael Burnham commits mutiny on her captain, even attacking her with a vulcan nerve pinch. In the third episode we finally meet the discovery crew. Gone are the TNG days where the ships crew are like a family, I am not sure I can think of a single character that would call another character “friend”. We are in a time of war so this would be expected, however I do hope we see deepening friendships form between characters.\nI am OK with the change to feature more conflict. I must admit the darkness of the third episode, did stop to make me think a bit, however by the fourth episode I was won over.\nCast\nThe story is concentrated around Michael Burnham, so I do worry that other characters won’t get a look in. However from what I have seen Michael Burnham is a great character. She is a strong female character, with a intriguing back story relating to Sarek and possibly also Spock.\nSaru the alien character on the show, played by the very tall Doug Jones is great. From the trailers he was all about sensing death, but there is far more to him than that. I am looking forward to learn more about him and his threat ganglia.\nAs the show has gone on we have started to learn about the other characters like Captain Lorca, Tilly and Paul Stamets.\nTitle Sequence\nI don’t like the opening title sequence, it feels very cheap like a draft version which hasn’t been finished yet. I was looking forward to a title sequence that would show off the USS Discovery, maybe like TNG where it warps around our own solar system. What we see is some of the tech that features in Star Trek which is nice but I want more. The theme tune while great that it features parts from the classic theme doesn’t stand on its own. I can hum all the other TV shows themes, and now after 4 episodes would struggle to do that.\nHowever I expect that after a while I will grow to like this more.\nOverall\nI like Discovery and I will keep watching it. I want to find out what happens to the characters. There are no annoying characters like other shows have had. There is character development, none of the characters are going to finish where they started. I want more friendship between characters, but I expect that will come, I want more exploring and doing Star Trek stuff, but most of all I want more episodes, roll on next weeks show.\nOther things I noticed\nSound effects on the bridge are awesome, it makes me feel at home The USS Discovery doesn’t feature in the first two episodes despite being the title of the show. Others have noted that Captain Georgiou has books on her shelf which feature classic episode titles ","date":"Oct 10, 2017","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/05/DAD0hTKUAAAUkTP.jpg?resize=662%2C366\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2017/discovery-my-thoughts-so-far/","series":null,"tags":["StarTrek","Discovery"],"title":"Discovery – My thoughts so far"},{"categories":null,"content":"In November 2015 it was announced that a new Star Trek series was going to be launched. It has been a long wait with multiple delays but Star Trek Discovery is finally here.\nStar Trek Discovery launches in the US on 24th September and in the UK on 25th September.\nLets have a look back how we got here.\nStar Trek was created by Gene Roddenberry in the 1960s. He imagined a show where we would explore the galaxy while also exploring the human condition. He described it as a wagon train to the stars. However it was cancelled after 79 episodes and three seasons due to budget cuts and low viewers.\nAfter cancellation its popularity boomed in reruns. In the mid 1970s an animated series was created. But demand for Star Trek continued until a new Star Trek show was commissioned called Phase Two.\nIn 1977 Star Wars came out, and the phase two idea was cancelled and turned into a movie. In 1979 we got Star Trek The Motion Picture. It went massively over budget and failed to really capture the Star Trek spirit.\nA much cheaper sequel was created in 1982 Star Trek II: The Wrath of Khan with Roddenberry taking more of a back seat. A film widely considered to be one of the best trek films. Four more feature films starring the original cast were made.\nHowever in 1987 Gene Roddenberry teamed up with Rick Berman and The Next Generation was created with a brand new cast. The series ran for seven seasons set roughly a century after the adventures of Kirk and Spock.\nPopularity of Star Trek soured and this led Rick Berman to help create the spin off series Deep Space 9, Voyager and lastly Enterprise along with four feature films continuing the adventures of the TNG cast. When Enterprise was cancelled in 2005, Star Trek appeared to be dead after 18 years on TV.\nHowever in 2009 movie creator JJ Abrams recast the classic crew for the big screen. The film was big on action, but took place in an alternative timeline allowing for familiar characters to have new adventures without interfering with events depicted in the TV shows.\nTwo sequels to this film have been produced and are bringing Star Trek back into popularity. This has given rise to Discovery. Star Trek is returning to the TV screen and it is going to be great.\n","date":"Sep 22, 2017","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/09/gqojtfz1dhmxoiri7g8p.png?resize=662%2C372\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2017/star-trek-is-back-with-discovery/","series":null,"tags":["StarTrek","Discovery"],"title":"Star Trek is back with Discovery"},{"categories":null,"content":"I think Azure is great, but there is loads to it so I can never know about all of its features. There is a video series hosted by Scott Hanselman called Azure Fridays which I have started to watch in an effort to keep more up to date about some of its cool features.\nI watched this video recently and it is all about application insights and new ways you can debug your web applications by creating snapshots. I am a big fan of application insights so adding extra ways to debug my apps is a big win for me. Once I get this feature working in my code I will no doubt blog about it.\n","date":"Sep 18, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/azure-friday/","series":null,"tags":["Azure","Programming","ApplicationInsights"],"title":"Azure Friday"},{"categories":null,"content":"I recently blogged about using Azure Web Jobs , Azure Function is another way of doing the same thing, lets look at how they work.\n(Sorry its been a while since I blogged but I suspect an erratic schedule will continue for the next few months.)\nTo create an Azure Function go to the Azure portal and click add new and search for \u0026ldquo;Function App\u0026rdquo;\nGive your app a name and select the usual resource group and location settings.\nNow when you open Function Apps you should see your new app.\nI want my Function App to run on a schedule so I clicked the + next to functions and selected TimerTrigger. I am a c# programmer so I selected this option as well.\nGive your function a name and specify using the usual cron notation how often it should run. I want mine to run at 9.30pm each night so use 0 30 21 * * *\nNow comes the code bit. By default you get a window with the following code in it\nusing System; public static void Run(TimerInfo myTimer, TraceWriter log) { log.Info($\u0026#34;C# Timer trigger function executed at: {DateTime.Now}\u0026#34;); } It is entirely up to you what you get your function to do. In my case I just wanted to call a URL on a schedule so I created some code that used httpclient.\nusing System; using System.Net.Http; public static async Task Run(TimerInfo myTimer, TraceWriter log) { log.Info($\u0026#34;Buffer 0 function executed at: {DateTime.Now}\u0026#34;); HttpClient client = new HttpClient(); var result = await client.GetAsync(\u0026#34;URL\u0026#34;); string resultContent = await result.Content.ReadAsStringAsync(); log.Info(resultContent); } Once you have created your app and it has run you can use the monitor section to view success and failures.\nThere is loads more you can do with Azure Function but this is a good place to start.\n","date":"Sep 12, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/azure-functions/","series":null,"tags":["Azure","Programming","DevOps"],"title":"Azure Functions"},{"categories":null,"content":"Pwned Pass is now available from the Google Play Store .\nPwned Pass is a simple android app that allows you to type in a password and tells you if it has been used in a data breach.\nTroy Hunt of Have I Been Pwned? recently added a new API to his website which allows you to search his extensive database of pwned passwords, 306 million of them. I have simply created a Android frontend to this API.\nThe API itself takes a SHA1 hash of the password and either returns a HTTP 200 if the password is found or a HTTP 404 if the password does not exist in the HIBP database. For more details of how Troy Hunt created this check out his blog post .\nMy app simply generates a SHA1 hash of anything that is typed in and then passes this to Troy Hunt’s API. I then get the HTTP return code so I know if the password exists or not.\nIt should be noted that: Do not send any password you actively use to a third-party service – even this one! I don’t log anything that you type into my app and all I am then doing is passing a SHA1 hash over SSL to HIBP. However you shouldn’t trust my word alone.\nThe app itself is written in Visual Studio with Xamarin Forms in a similar fashion to the app I talked about last week .\nAs I am using Xamarin Forms there is the potential that I may develop iPhone or UWP versions of this code in the future. With that in mind I have made use of interfaces for the android specific parts of the code.\nI also make use of the modernhttpclient nuget package due to problems I encountered with httpclient and SSL. This is due to limitations of what libraries are available in mono and what has been implemented, I suspect there are better ways to solve this but that is all part of the fun.\nPlease do have a look at Pwned Pass and let me know what you think. Especially if it doesn’t work or throws errors. I would like to spend time making this app as good as I can make it.\n","date":"Aug 14, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/pwned-pass-available-from-the-play-store/","series":null,"tags":["Android","Mobile","Programming","App","PwnedPass"],"title":"Pwned Pass – Available from the Play Store"},{"categories":null,"content":"For the past week or so I have been playing around with Xamarin and creating an android app.\nWell I now have an app in the Google Play Store. Check out https://play.google.com/store/apps/dev?id=6148298088834956775 . Before you rush and download the app I must warn you that it doesn’t do much yet. It displays some content that is on my website and there are a few links to allow sharing of content. I have some ideas to display content from my blog and allow sharing. I also have some other ideas for apps that might actually be useful to people that are not me. If you have ideas or feature requests do let me know.\nOk how did I go about creating this app and getting it in the app store?\nXamarin is now part of Visual Studio so step one is install all the Xamarin features to Visual Studio and build an app.\nNext I wanted to monitor my app. Now I know Application Insights doesn’t support apps so what tools are out there? HockeyApp is something I had heard of but they are in the process of being replaced with Visual Studio Mobile Centre .\nIt was relatively easy to hook up my app to Visual Studio Mobile Centre. First install the required nuget packages. Then add using statements and the following line to your MainActivity.cs file (these instructions are available on the Mobile Centre)\nusing Microsoft.Azure.Mobile; using Microsoft.Azure.Mobile.Analytics; using Microsoft.Azure.Mobile.Crashes; using Microsoft.Azure.Mobile.Distribute; using Microsoft.Azure.Mobile.Push; MobileCenter.Start(\u0026#34;[Unique ID]\u0026#34;,typeof(Analytics), typeof(Crashes), typeof(Distribute), typeof(Push)); Now you can connect the Mobile Centre to your source code (VSTS in my case) and get it to run a build for every commit.\nOne complexity of the build is that you need to supply a keystore file (basically a certificate to digitally sign your app). I found the best way to do this was to use Visual Studio to create the file.\nIn VS2017 there is a option called Archive Manager under the tools menu. In here click the distribute button and select Ad-hoc. In the signing identity section you can create a keystore file. Enter a few details and a keystore file will be created in AppData\\Local\\Xamarin\\Mono for Android\\Keystore[keystore name][keystore name.keystore]\nOnce you have added the keystore file to your build you can enable the distribute option. Now you will get an email after every build with a link to install your app.\nEvery time your app crashes the details will be logged in the crashes section for you to explore and fix the issues.\nThe Analytics section allows you to explore how your app is being used. You can also add Analytics.TrackEvent(\u0026ldquo;Feature X\u0026rdquo;) to measure the usage of different features.\nThere are more things you can do which I will explore more at another time along with how to get your app into the Google Play Store.\n","date":"Aug 7, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/android-app-development-and-the-visual-studio-mobile-centre/","series":null,"tags":["App","Azure","Programming","Android"],"title":"Android App Development and the Visual Studio Mobile Centre"},{"categories":null,"content":"For a while I have found myself writing the same bits of code for different web projects. This annoys me as it goes against the DRY principle (don’t repeat yourself).\nOne possible solution is to write your own nuget packages. You can then add this piece of code to any project you work on.\nnuget.org is the public nuget feed where any developer can download nuget packages. You could publish your nuget package here, but your might want to restrict access so better to create a private nuget feed.\nLets look at how we create a nuget feed in Visual Studio. First thing you need to do is install the Package Management extension to Visual Studio Team Services (its free for less than 5 users), this will add a packages section under the build menu.\nBefore you can start using this new feature you need to add a Package Management License in the users hub.\nOnce that is done you can create a feed. You need to give your feed a name, decide if only members of the current project or everyone in your account should have access to read and contribute to.\nNow you have a feed you could use the nuget package command to create a nupkg file and then nuget push command to add it to your feed. A better way is to get Visual Studio Team Services to do all the hard work.\nCreate a new project in Visual Studio Team Service to house your nuget package. In the build section add an empty build definition. Choose a build agent, I am using the Hosted VS2017. Then add the following steps nuget restore, Visual Studio Build, nuget pack and nuget push.\nnuget restore – this step is only needed if your code depends on other packages. If it depends on other packages that are only in your feed you must specify your feed in the feeds and authentication section. Visual Studio Build – this builds your code like you would in Visual Studio. The only config I made to this step was to specify Release in configuration. nuget pack – this creates the nupkg file from your built project. In configuration to package specify the same as you specified in the previous step (in my case Release) nuget push – this publishes to your feed, so of course you need to specify your feed. One last thing to configure is to enable the continuous integration option in triggers. This means whenever you push code all these steps will run and you have a new version of your nuget package.\nIn Visual Studio you need to create a *.nuspec file, this contains all the meta data about your nuget package. Let look at an example.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;package \u0026gt; \u0026lt;metadata\u0026gt; \u0026lt;id\u0026gt;$id$\u0026lt;/id\u0026gt; \u0026lt;version\u0026gt;1.0.2\u0026lt;/version\u0026gt; \u0026lt;title\u0026gt;Nuget\u0026lt;/title\u0026gt; \u0026lt;authors\u0026gt;Simon Foster\u0026lt;/authors\u0026gt; \u0026lt;owners\u0026gt;Simon Foster\u0026lt;/owners\u0026gt; \u0026lt;requireLicenseAcceptance\u0026gt;false\u0026lt;/requireLicenseAcceptance\u0026gt; \u0026lt;description\u0026gt;An example of a nuget package.\u0026lt;/description\u0026gt; \u0026lt;releaseNotes\u0026gt;Release Notes\u0026lt;/releaseNotes\u0026gt; \u0026lt;copyright\u0026gt;Copyright 2017\u0026lt;/copyright\u0026gt; \u0026lt;projectUrl\u0026gt;https://[yourVSaccount].visualstudio.com/nuget/\u0026lt;/projectUrl\u0026gt; \u0026lt;/metadata\u0026gt; \u0026lt;/package\u0026gt; One last thing to mention is version numbers. You can either change the version number in your *.nuspec file everytime you push changes. This will create stable packages like 1.0, 1.1, 1.2 etc\nHowever you can use the automatic version number setting in the nuget pack build step. However I have found this only ever creates pre-release packages and I haven’t found a way to upgrade a package from pre-release to stable.\nThis is a really neat way to reuse your code in multiple projects. I have only been looking at this for a few days and I have already extracted code to do with emails, creating excel downloads and database related methods. I suspect that doing this will also have a side benefit of forcing me to create code with fewer dependencies so more code can be turned into a nuget package.\n","date":"Jul 31, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/creating-nuget-packages/","series":null,"tags":["Nuget","C-Sharp","Programming","Visual Studio"],"title":"Creating your own nuget packages with VSTS"},{"categories":null,"content":"For a while the Async and Await commands in c# have confused me.\nLike most things the best way to learn about something is to use it in a real world example. I am currently adding an email alert feature to a website. This is an ideal example of something that would benefit from Asynchronous programming. There is no need for the webpage to wait to send 1000s of emails, lets just send a call to get started and allow the browser to carry on as normal.\nThis is my first try at using async and await so feel free to suggest best practises in the comments.\nLets start with a Send method in my EmailController.\npublic ActionResult Send(int id, int pageId, int userID) { if (!Authorize.checkPageIsAuthorised(userID, (Authorize.PageIds)pageId)) { return Redirect(\u0026#34;/login\u0026#34;); } else { Task\u0026lt;string\u0026gt; t = SendNotifications(id,userID); return Redirect(Request.UrlReferrer.ToString()); } } This simply checks to see if you have permission to the page. If not redirects to the login page otherwise it makes a method call and redirects back to the page it came from.\nLets have a look at that method call in more detail.\nTask t = SendNotification(id, userid);\nSendNotification doesn’t return a normal string it returns a Task, so lets look at how we are creating this.\npublic async Task\u0026lt;string\u0026gt; SendNotifications(int id,string type,int userid) { //logic ommitted await ef.SendEmail(model, emailHtmlBody); return \u0026#34;OK\u0026#34;; } The return type is set to Task but it has the aysnc keyword appended to it. It also makes a call with the await keyword.\npublic async Task SendEmail(EmailModel model,string emailHtmlBody) { //logic removed await smtp.SendMailAsync(message); } So that is it. My first bit of code that uses Async and Await. My controller calls a method asynchronously which then calls another method asynchronously which sends emails asynchronously.\nAsync – This enables the Await keyword to be used in the method\nAwait – This is where things get asynchronous. The await keyword allows the code to wait asynchronously for the long running code to complete.\n","date":"Jul 24, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/async-and-await/","series":null,"tags":["Async","Await","Programming","C-Sharp"],"title":"Async and Await"},{"categories":null,"content":" I recently watched Troy Hunt’s What Every Developer Must Know about HTTPS course on Pluralsight. Its very good and really make you think about SSL certificates and how to correctly implement them.\nOne thing in particular Troy mentions is the website SSL Labs . This website allows you to test a websites implementation of SSL. A grade of A to F is assigned with A being the best and F being the worst.\nTroy Hunt has a blog post where he discusses how Australian Banks score. Lets look at a few UK banks.\nBank SSL Certificate Grade Home Page Under SSL HSBC www.hsbc.co.uk B Y Nationwide onlinebanking.nationwide.co.uk C N NatWest www.nwolb.com C N Barclaycard www.barclaycard.co.uk A- Y Barclays bank.barclays.co.uk A- N Lloyds Bank www.lloydsbank.com A N Royal Bank of Scotland www.rbsdigital.com C N Standard Chartered www.sc.com C Y Virgin Money uk.virginmoney.com A+ Y Santander retail.santander.co.uk A- N On the whole the ratings are all quite good with all being in the range A-C. However I have also indicated if they have SSL on the home page. Only 4 out 10 website listed above have the home page load under SSL.\nWhy does this matter as long as the login is under SSL? Any page that loads over http is potentially at risk from a man in the middle attack. A fake malicious home page could contain links to any page and trick users into entering personal information.\nIf you want to test a bank or other website not listed here. Go to https://www.ssllabs.com/ssltest/index.html and type the address that is on the SSL certificate in to the search. The good news is that this site scores a A.\nTroy mentions that there is rapid growth in the adoption of SSL, there is also rapid growth in improving ratings. One of these banks went from a C to an A during the course of writing this blog.\n","date":"Jul 17, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/how-good-is-your-ssl/","series":null,"tags":["SSL","Security"],"title":"How good is your SSL?"},{"categories":null,"content":"Whenever I write a new test I have to think how best to do it. Hopefully I can summarise a few tips here to help get started.\nArrange Act Assert The first thing I think about when writing a test is Arrange, Act, Assert. Arrange, Act, Assert is a pattern for writing the tests.\nArrange – This gets things in order ready to execute the test.\nAct – This executes the method you want to test.\nAssert – This compares the value produced in the Act step with a known value typically with a method similar to the following\nAssert.AreEqual(expected value, actual value)\nSay for example you wanted to test a method called ReturnsTrue() which does nothing but returns a value of true. This method is in a class called ReturnsTrueClass\nThe Arrange step in this example would be.\nReturnsTrueClass t = new ReturnsTrueClass(); The Act step in this example would be.\nvar result = t.ReturnTrue(); The Assert step in this example would be.\nAssert.AreEqual(true, result); This is a stupidly simple example but hopefully you get the idea of how you can build all your tests with these three steps.\nRecently I saw a tweet complaining that someone has mixed up expected and actual in the Assert statement.\nThere is a minor but special hell reserved for those who mix up the expected and actual parameters in Assert.Equals\n\u0026mdash; Keith Williams (@zogface) July 5, 2017 At first glance this probably isn’t the worst mistake to make as if your tests are all passing actual and expected are the same.\nHowever tests will fail, that is the whole point of them, you can then fix bits of code. If you have mixed up actual and expected it adds extra time to debugging and figuring out what values are produced from your code and what you are expecting it to produce. It may be your test uses a mocking framework and somewhere in there, there is an issue, with mixed up expected/actual you may assume a problem in your code rather than the test.\nAlso, how do you make such an error? When I type Assert.AreEquals() in Visual Studio, Visual Studio tells me what each parameter does, it takes a matter of seconds to do this, just by hovering over the code.\nOne last tip to say about tests. Write your tests to test the behaviour of your application.\n","date":"Jul 10, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/writing-your-first-test/","series":null,"tags":["Act","Arrange","Assert","Programming","Testing"],"title":"Writing your first test"},{"categories":null,"content":"I am a fan of Azure but today I have been looking at AWS. Specifically how to upload and download files.\nAWS S3 stores files in Buckets. I already had an AWS S3 account setup with a Bucket. I am going to assume you have got a bucket setup and concentrate on the code to get files in and out.\nFirst step is to use nuget to install the AWS packages. In nuget the packages you want are called AWSSDK.Core and AWSSDK.S3.\nThe using statements you want to use are called Amazon.S3 and Amazon.S3.Transfer, not sure why this doesn’t match nuget, this difference caught me out a couple of times.\nNow to the code that uploads files\nAmazonS3Client AWSclient = new AmazonS3Client(accessKeyID, secretAccessKeyID, Amazon.RegionEndpoint.EUWest1); TransferUtility fileTransferUtility = new TransferUtility(AWSclient); using (FileStream streamWriter = new FileStream(path, FileMode.Open)) { TransferUtilityUploadRequest fileTransferUtilityRequest = new TransferUtilityUploadRequest { BucketName = \u0026#34;flawlessimages\u0026#34;, InputStream = streamWriter, Key = fileName }; fileTransferUtility.Upload(fileTransferUtilityRequest); } Lets break it down and look at what it does.\nAmazonS3Client AWSclient = new AmazonS3Client(accessKeyID, secretAccessKeyID, Amazon.RegionEndpoint.EUWest1); This creates an instance of AmazonS3Client, we are passing the Access Key and Secret Access Key both of which can be found from your Amazon S3 account My Security Credentials section. Amazon.RegionEndpoint.EUWest1 specifies the amazon data centres that your bucket is located in.\nTransferUtility fileTransferUtility = new TransferUtility(AWSclient); This creates an instance of TransfterUtility using the AmazonS3Client instance we created in the previous step.\nusing (FileStream streamWriter = new FileStream(path, FileMode.Open)) { This opens up a filestream from a files path and specifies that the file should be opened.\nTransferUtilityUploadRequest fileTransferUtilityRequest = new TransferUtilityUploadRequest { BucketName = \u0026#34;flawlessimages\u0026#34;, InputStream = streamWriter, Key = fileName }; fileTransferUtility.Upload(fileTransferUtilityRequest); This last step specifies which bucket to upload to, what input stream to upload and the Key to use. Key is just AWS way of referring to files, more commonly referred to as the filename.\nThis is all you need to do to upload a file to your Bucket. The file will be located at https://s3-eu-west-1.amazonaws.com/[bucketname]/[filename] , however by default it will not be downloadable until you set Read permission to everyone, once you do that anyone who has the link will be able to download your file.\nThis is the same permission level as any file you have on your webserver, however AWS has a better way.\nusing (s3Client = new AmazonS3Client(accessKeyID, secretAccessKeyID, Amazon.RegionEndpoint.USEast1)) { GetPreSignedUrlRequest request1 = new GetPreSignedUrlRequest { BucketName = bucketName, Key = filename, Expires = DateTime.Now.AddMinutes(5) }; urlString = s3Client.GetPreSignedURL(request1); } Here we are generating a url to download the file, but we are specifying that it is only valid for 5 minutes. This means that if you share the url it will only work for 5 minutes, after that AWS will give an access denied message.\nThis is much better security than you have on a typical web server, and easy to implement, every time a user clicks on a download link you generate a new presigned url and send the download to the browser, as long as this process doesn’t take longer than 5 minutes the user will never know.\n","date":"Jul 3, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/uploading-files-to-aws/","series":null,"tags":["AWS","C-Sharp","Programming"],"title":"Uploading Files to AWS"},{"categories":null,"content":"I keep hearing about Azure WebJobs but I have never used them. Time to change this.\nWebJobs are a feature of Azure App Service that can run a script at a specific time. In my case I would like to hit a specific url of my website at the same time every day.\nTo the right you can see an example of the WebJobs form on the Azure portal that you need to fill in.\nYou need to supply a name for your webjob.\nYou need to upload the script that will run in my case I used a powershell script. My script consisted of which basically just loads the url specified.\n$progressPreference = \u0026#34;silentlyContinue\u0026#34;; $result = Invoke-WebRequest -Uri (\u0026#34;https://www.google.com\u0026#34;) -Method Get -UseBasicParsing; Type refers to if your job will be triggered or run continuously, I want it to be triggered.\nTriggers refers to if you want it to be scheduled or manual, something that you can run on an ad hoc basis. I of course want scheduled.\nIf you are familiar with the linux CRON then the next box will make sense to you for everyone else I will try and make sense of it. The box consists of 6 numbers which can either have a value or a *. The numbers correspond to the following {second} {minute} {hour} {day} {month} {day of the week}.\nA hourly job would be expressed as 0 0 * * * *, ie every day of week, every month, every day, every hour and only when minute and second equals zero. For more help with this check out the MSDN docs about it. I want to use 0 30 21 * * * to run daily at 9.30pm.\nThat’s it everything setup, now time to wait and see if it works.\nOh no!\nIt failed to run at the specified time.\nThe reason for this is the scheduler requires the feature Always On to be turned on which is not available in the free App Service. Before you reach for your wallets, I found a solution on this blog post that allows them to run on the free tier.\nThe thinking behind this solution is you need to keep the website alive throughout the day so Tom has created a script that does this. His script can be found on his blog or on his github page .\nSet this script up to run every 5 minutes (0 */5 * * * *) like the example above.\nThe nextthing you need to do is create a Custom connection string in the Application Settings blade called SecretThing. Tom’s script references this to access the website and keep it alive. The password you need to put in SecretThing can be found in you publish profile (downloaded from the Overview blade in the Azure portal). For more details and a better explanation check out Tom’s blog .\nOne last thing to mention about WebJobs is that you can see details about when they have run at https://[YourWebAppName].scm.azurewebsites.net/azurejobs/#/jobs and this can be a great place to help debug your scripts.\n","date":"Jun 26, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/using-azure-webjobs-to-automate-stuff/","series":null,"tags":["Automate","Azure","Cron","Programming"],"title":"Using Azure WebJobs to Automate Stuff"},{"categories":null,"content":"A while ago I blogged about promoting my blog with Buffer. At the time I made use of the nuget package BufferAPI but lets look at some improvements I can make.\nThe BufferAPI package worked great from my console app, but when I tried to use it from a Controller in an MVC app I never got it to work. Lets look at the API docs and see if I can rewrite it.\nThere are two main types of API calls GET which gets data from the server and POST which posts data to the server. These come from the types of HTTP requests.\nI quickly figured out how to use the GET API call to authenticate using https://api.bufferapp.com/1/profiles.json?access_token=XXXX However POST was defeating me. That was until I remembered Fiddler .\nI had heard Troy Hunt (and others) talk of using Fiddler to examine what data is being passed among websites. Troy uses it to do a man in the middle test to see what information can be stolen.\nIt is really easy to setup, install Fiddler, click yes to a few security warnings and you can see what information is being passed from your code to remote APIs.\nOnce I had Fiddler installed I could compare what information is being passed between a successful API call using the BufferAPI nuget package and an unsuccessful API call using my code.\nFiddler also showed that passing my authentication token in a POST request is much better. Despite both GET and POST being encrypted when using HTTPS, anything at either end that logs URLs will have a log of your username and password.\nIf you have not tried Fiddler, give it a try especially if you are doing things with API calls.\n","date":"Jun 19, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/fiddler-and-apis/","series":null,"tags":["",""],"title":"Fiddler and APIs"},{"categories":null,"content":"Last week I talked about Power BI , what it is and some of the different services you can use with it. This week lets add some of that data to a simple web page.\nFor this example I am going to add the google analytics data from this website to this page.\nFirst login to your Power BI https://app.powerbi.com/ Click the get data link at the bottom left.\nClick My Organisation to bring up the app search box.\nClick the Apps tab and search for “google” in the search box, you should then see Google Analytics, click into this and then click the get it now button.\nLog into your google account. If you have multiple google accounts I found it worked best to sign out of all of them or run this in an incognito window.\nOnce you are signed in you should see a list of the different google analytics data you have, select the one you want to use and click import.\nPower BI will then go away and start loading the data.\nOnce loaded go to Reports and select the Google Analytics that has been loaded. If you have more than one, it is a good idea to rename each one eg Corporate Site Google Analytics, Blog Google Analytics so you won’t get mixed up.\nIn the file menu select Publish to web and agree that you are OK for this to be made public.\nYou will then be given a piece of HTML code that starts with \u0026lt;iframe copy this onto your web page. Reload your webpage and you should see something similar to mine below.\nIt should be noted that while you can but the webpage containing the iframe behind a login page, the data could still be accessed if you knew the url contained within the iframe, this is why the link can be emailed and continue to work.\n","date":"Jun 12, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/how-do-i-add-power-bi-data-to-a-webpage/","series":null,"tags":["PowerBI",""],"title":"How do I add Power BI data to a webpage?"},{"categories":null,"content":"The term Business Intelligence (BI) refers to technologies, applications and practices for the collection, integration, analysis, and presentation of business information. The purpose of Business Intelligence is to support better business decision making. Essentially, Business Intelligence systems are data-driven Decision Support Systems (DSS). Business Intelligence is sometimes used interchangeably with briefing books, report and query tools and executive information systems.\nWhat is Power BI? Power BI is a business analytics service provided by Microsoft. It provides interactive visualisations with self-service business intelligence capabilities, where end users can create reports and dashboards by themselves, without having to depend on any information technology staff or database administrator.\nMicrosoft describe it as follows: Power BI is a suite of business analytics tools that deliver insights throughout your organisation. Connect to hundreds of data sources, simplify data prep, and drive ad hoc analysis. Produce beautiful reports, then publish them for your organisation to consume on the web and across mobile devices. Everyone can create personalised dashboards with a unique, 360-degree view of their business. And scale across the enterprise, with governance and security built-in.\nWhat can you do with it? One of the first thing you can do with Power Bi is connect some of the apps that you use.\nTo sign up to use Power BI go to https://powerbi.microsoft.com/en-us/ Once you are signed in click the get data link in the bottom left corner and browse through the apps you can use.\nIf your website uses google analytics you can connect it up to Power BI and see some cool analytics. The below map was generated from some google analytics data, you can see that this website has had users across the globe, with UK and USA getting a lot of hits. Another thing you can do is connect Azure Application Insights. This provides some similar data to google analytics about who is visiting your site.\nIf your code is hosted in Visual Studio Team Services you can get some cool analytics from your commit history. There is a huge long list of apps you can connect to power BI so you can start getting all sorts of analysis very easily. I am hopefully going to talk another time about getting custom data into Power BI.\n","date":"Jun 5, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/businessintelligence/","series":null,"tags":["ApplicationInsights","Programming","BusinessIntelligence","SQL"],"title":"What the heck is Business Intelligence?"},{"categories":null,"content":"Have you heard of the website https://haveibeenpwned.com ?\nWell you should have. Have I Been Pwned is a website created by security expert Troy Hunt that keeps track of data breaches and allows you to search and find ones that affect you. As I write this Troy Hunt has tracked 3,752,984,562 pwned accounts from 216 pwned websites.\nWhat does “pwned” mean? pwn means to compromise or control, specifically another computer (server or PC), website, gateway device, or application. (as defined on wikipedia)\nOriginally, pwn and its variants were pronounced /ˈoʊn/ in the same way as the verb own, the tail of the p being “silent”.\nIn terms of this site Troy Hunt defines it as:\nA \u0026ldquo;breach\u0026rdquo; is an incident where a hacker illegally obtains data from a vulnerable system, usually by exploiting weaknesses in the software. All the data in the site comes from website breaches which have been made publicly available.\nWhat can I do? As an individual you can search for your email address in Have I Been Pwned, I am in 7 data breaches.\nAs a person responsible for an email domain you can search and find which of your users are in a data breach.\nYou can also sign up to get notified of any future data breaches you might get caught up in.\nIf you are in a data breach change your password. If you use the same password across multiple sites or services change them as well. Consider using a password manager so you can have unique complex passwords for every services you use and not have to worry about forgetting them.\nIf you look after a website or service then follow Troy Hunt. Think about security, is your site vulnerable to SQL injection, do you store passwords with reversible encryption?\nHow worried should I be? Data breaches are happening more and more often. Its not showing any sign of slowing down, in fact I expect there to be lots more in the years to come. However there are things you can do to mitigate the damage of being in a breach.\nUse a password manager Don’t share passwords between sites Regularly change your passwords Think of passwords as pass phrases and include spaces between the different words Consider what information a company has about you. How worried would you be if this became public knowledge? Consider if you want this online, weigh up the benefits etc\n","date":"May 29, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/have-i-been-pwned/","series":null,"tags":["Pwned","Data Breach","Programming","Security","Troy Hunt"],"title":"Have I Been Pwned?"},{"categories":null,"content":" I am really excited a new Star Trek Discovery trailer has just been released, if you haven’t seen it yet, watch it now.\nThe rumour mill has been going crazy about Star Trek Discovery but now we have a trailer with actual footage to analyse. In my opinion all the concerns about changes in the production team have gone out the window, this looks to be a polished Star Trek show that can sit alongside our favourites.\nThe Ship When images first surfaced of the ship I wasn’t keen, however all the shots of the ship look great. I assume this must be the USS Discovery, although it is hard to tell anything from a two minute advert.\nThe Bridge The Bridge looks recognisable as a Star Trek bridge, but also looks fresh and new. I think it feels very JJ Verse. I like it.\nThe Uniforms The uniforms look like a mid step between what we saw on Enterprise and what we know as the Classic Trek look. I can’t see any Blue, Red, Gold to signify departments, but it looked like Gold and Silver could well signify something. Really hard to tell at this stage, but I like what I can see.\nCharacters The show feels like it is going to be gritty and real. With characters you can really get behind. I liked what I saw of the captain and lead character. The alien character intrigued me, hopefully his line about death will be explored. It feels like there is a strong link between Vulcans and Sarek with the shows lead (played by Sonequa Martin-Green) Not sure how I feel about this yet, depends on what they do with it. I don’t want them to just redo what they have done with Spock, but again lets wait and see.\nKlingons OK I am not sure I like the strong emphasis on the Klingons. I think we have spent a lot of time learning about Klingons, so when I heard the casting news about Klingons I was not excited. The Klingons also have a new look, which doesn’t bother me that much as the Klingons have changed their look before. But lets wait and see what happens.\nStar Trek is back After over a decade with no Star Trek on TV, it is finally coming back. I am beyond excited about this. There are lots of things I want from this new show but this advert has definitely rekindled my excitement and hope that we are going to get something good.\n","date":"May 22, 2017","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/05/DAD0hTKUAAAUkTP.jpg?w=1199\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2017/star-trek-discovery/","series":null,"tags":["StarTrek","Discovery"],"title":"Star Trek Discovery – thoughts on new trailer"},{"categories":null,"content":"No blog post this week. However why not check out Troy Hunts blog article about Windows Update .\nI am probably fairly unique in that I like installing updates whether it be on my phone, my PC or for one of my favourite programs. One of my tasks when I used to work in a sysadmin role was ensuring all windows updates got installed on all servers and client machines. It was a never ending task, as soon as you got almost all updates done, it would be update Tuesday and a new set of updates would be released.\nI can think of a couple of times an update got installed that caused a problem and needed to be rolled back. However I have no way of measuring how often it fixed a problem or improved the security of the OS or other software. If you compare the annoyance of waiting for updates to install against having a computer (or worse a server) be hacked or otherwise compromised I think it is fairly clear what you need to do.\nAs someone who works in IT I feel it is my duty to share the importance of keeping your devices updated. Don’t turn automatic updates off or disable it. It is there to help you and keep your computer safe and secure.\n","date":"May 15, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/windows-update-dont-turn-off/","series":null,"tags":["Windows","Update","ITAdmin"],"title":"Windows Update – don’t turn it off!"},{"categories":null,"content":"High Charts is a javascript library that allows graphs and charts to easily be added to web pages.\nA chart like this can easily be added with a few lines of html and javascript.\n\u0026lt;script src=\u0026#34;https://code.highcharts.com/highcharts.js\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;https://code.highcharts.com/modules/exporting.js\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;container\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; This adds the highcharts library and a container for the chart to be displayed in.\nHighcharts.chart(\u0026#39;container\u0026#39;, { title: { text: \u0026#39;Solar Employment Growth by Sector, 2010-2016\u0026#39; }, subtitle: { text: \u0026#39;Source: thesolarfoundation.com\u0026#39; }, yAxis: { title: { text: \u0026#39;Number of Employees\u0026#39; } }, legend: { layout: \u0026#39;vertical\u0026#39;, align: \u0026#39;right\u0026#39;, verticalAlign: \u0026#39;middle\u0026#39; }, plotOptions: { series: { pointStart: 2010 } }, series: [{ name: \u0026#39;Installation\u0026#39;, data: [43934, 52503, 57177, 69658, 97031, 119931, 137133, 154175] }, { name: \u0026#39;Manufacturing\u0026#39;, data: [24916, 24064, 29742, 29851, 32490, 30282, 38121, 40434] }, { name: \u0026#39;Sales \u0026amp; Distribution\u0026#39;, data: [11744, 17722, 16005, 19771, 20185, 24377, 32147, 39387] }, { name: \u0026#39;Project Development\u0026#39;, data: [null, null, 7988, 12169, 15112, 22452, 34400, 34227] }, { name: \u0026#39;Other\u0026#39;, data: [12908, 5948, 8105, 11248, 8989, 11816, 18274, 18111] }] }); This adds the data and sets up various options for the charts. More details about the different charts and options you can set can be found at https://www.highcharts.com I am quite a fan of the different gauges that you can put on your site, as you can add a bit of animation and make the needles bounce around. https://www.highcharts.com/demo/gauge-solid As most of the data I deal with lives in a SQL Server database, I have been spending quite a bit of time writing Stored Procedures and functions to mould the data so I can easily pass data from the database to javascript.\nHighcharts are very flexible with lots of different options that allow you to display charts of almost any data you have, these charts are also exportable as PNG, JPEG, CSVs etc\n","date":"May 8, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/highcharts/","series":null,"tags":["HighCharts","Graphs","Programming","Javascript","SQL"],"title":"HighCharts"},{"categories":null,"content":" For the past few weeks my software developing has been taking a back seat as I planned and coordinated the IT requirements of an office move.\nThe company I work for has been working out of a converted barn, but as the company has grown we have outgrown the building and for quite a while it has been a real squeeze to get everyone in. We now have a shiny new offices with plenty of room for growth.\nIt was at the start of the year when I first started getting involved with the new office. At that time building work was about to begin. The office had concrete floors so in order to get network and power points in the floor, channels would have to be cut into the concrete for the floor boxes and cable runs.\nAt the old office our servers were in a cabinet in the corner of the room which meant they could be quite loud especially when the fans were going full pelt. Our American owners were supplying brand new IT equipment and we would have a dedicated server room.\nSoon a weekly conference call was setup so we could coordinate with the IT people in America and the various different contractors that would help deliver our new office.\nOne thing I was particularly proud of was YDS. York Data Services (YDS) is an ISP I have worked with in the past at a previous job and I was able to continue my relationship with them and they were contacted and became our primary internet supplier.\nUnfortunately most ISPs have to deal with BTOpenreach who have a bit of a monopoly on getting phone lines or leased lines installed. The initial estimate we were given was two weeks after we were due to move in. It was looking like we would move in and two weeks later we would get an internet connection. We were in the process of getting quotes for a temporary internet connection when BT contacted us and could get us connected up. Soon afterwards a huge cabinet and two pallets full of equipment was delivered. Two days later a team was dispatched to rack everything up. On the rack we had the following equipment: 4 x UPS, 4 x Network Switches, 2 x Routers (for 2 x Leased Lines from different providers, the secondary connection has not been installed yet), 2 x Palo Alto Firewall devices, 2 x New Servers.\nJust prior to our move we had a rack full of equipment but no patch cables, nothing connected up and I was beginning to get concerned that we would not be ready in time.\nThankfully a team was dispatched to help out over the weekend of the move, with the patch cables being delivered only hours beforehand. I worked closely with them and we worked late into the night. By the end of the Friday night our domain controller had been moved and its IP address updated. DHCP scopes had been setup for the new network. Once that was done we could get the WiFi points connected up (These had already been fitted to the ceiling).\nThe following day we moved our existing servers and updated their IP addresses and got everything patched up. Furniture started arriving so desks could start being setup and phones connected up. Everything was slotting into place, and for the first time our new office was starting to look like an office.\nBy the time it got to Monday the only problem was the phone number had not transferred as requested (something else to blame on BT) All staff could either connect via an Ethernet cable or connect to our brand new WiFi network and had access to all the IT services they had at the old office. Another minor issue was the NAS we used for backups had to be reset as it couldn’t communicate with the domain so no one could login, luckily this didn’t affect the data on it.\nFrom my perspective the move was a massive success considering how complex and how many different people had been involved.\n","date":"May 1, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/office-move/","series":null,"tags":["ITAdmin","Routers","Servers","OfficeMove"],"title":"Office Move"},{"categories":null,"content":"I created my GitHub account in August 2010, lets look at what I have done with it.\nIn 2010 I started by committing the code for an old php website I had created. Then in December I committed some other php sites.\nBetween 12 December 2010 and 29 September 2011 I must have created some automated process which is why there is a commit every day. Looking at the diff it appears to be related to tracking visitors to a site. I assume back then I hadn’t heard of the .gitignore file!\nNothing for a few years until 2014, when I started compiling a code samples collection, this is bits of code that I want to show off, I have added to this since 2015 so I really should go back to this as my skills have developed a bit since then.\nIn 2015 I really started to think of myself as a developer, I added repositories for Raspberry Pi, and a few C# ideas I had. I ended the year taking part in Advent of Code\nWhat did I do last year? Well I started going to York Code Dojo and this meant lots of forks from their code examples and also my first pull request. I also did a fair bit of looking at other repositories trying to find a good open source project to contribute to, still not found one for me yet. Lastly I ended 2016 with a bit of Advent of Code.\nNot sure how enthralling a blog post this is, but fascinating to see the different commit histories.\n","date":"Apr 24, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/github-history/","series":null,"tags":["Git","Github","Programming","Code"],"title":"GitHub history"},{"categories":null,"content":"I would like to automate the promotion of this blog. Currently to promote this blog on social media I use a few different services. Buffer (buffer.com ) is a service that allows you to schedule updates to the main social media channels (Facebook, Twitter, LinkedIn and Google+)\nIFTTT or If This Then That (ifttt.com ) is a service that allows you to connect different online services. You can send an email when a specific event occurs in your calendar for example.\nI have been using a combination of these services to share to social media some of my past blog posts. I then add to my calendar details of my blog posts. Then I use IFTTT to add the event to Buffer, and then buffer tweets on a schedule.\nThis works great however it is a manual process to add my posts to my calendar. I have been using a spreadsheet to help me generate an ics calendar file which I import into my google calendar. There must be a better way of doing this.\nI have written some code that reads the RSS feed of my blog and then shares that to Buffer using the Buffer API. The code I am creating is far from finished however I am trying to use the concepts of clean code to make it as flexible as possible.\nI have an interface called ISocial which my buffer code implements, but it would be easy to add a class that implements the same interface but uses the twitter or facebook APIs. My code reads from a specific WordPress RSS feed, but it should be easy enough to adapt to read from a SQL database or any other data source.\nI am currently unsure what kind of interface to use for this application. I could create a web page that controls the functionality, or maybe just a windows application, or maybe both of these are complicating things and all I need is just a console application that could be added as a scheduled task.\n","date":"Apr 17, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/automation-promotion/","series":null,"tags":["Buffer","Blogging","XML","Programming","RSS"],"title":"Automation of the Promotion of my Blog"},{"categories":null,"content":"I recently came home from a busy day of work to my wife blaming me for allowing our 18 month old son to reach some paint and get it all over the carpet. She was concentrating on the fact that he could reach the paint not on the fact that she had brilliantly got the paint out of the carpet.\nMy instinct on being blamed is to deny it, or start throwing blame back at her. It is possible I put the paint in his reach, it is also possible it was someone else.\nIt is very easy to blame someone else. For example, looking through the git history to find out who changed a specific file is only a few clicks.\nBut is it ever productive to blame someone? Is it not better to focus our energy on fixing the issue at hand?\nIn the world of business it is easy to go from blaming other people until you have a blame culture. When you have a blame culture everyone starts looking out for themselves so it’s not them that gets the blame and productivity will suffer.\nI think it is more important to put in place​ processes to minimise issues happening again. If a deployment causes downtime, don’t ask who’s fault is it and sack them. Instead what can we do to reduce the chance of it happening again?\nAs someone who has an interest in DevOps, I often break things and don’t want to get blamed for that, I do want to improve my processes so I am not always breaking the same things.\nWhat do you think about Blame? Is what can we do differently always better than who did this?\n","date":"Apr 10, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/blame/","series":null,"tags":["Blame","Culture","DevOps","Programming","SoftSkills"],"title":"Blame"},{"categories":null,"content":"I am trying to understand interfaces and when to use them in my code.\nAn interface defines a contract and any class that implements that interface agrees to fulfil that contract.\nLets look at an example as this tends to be how I learn best.\nMost applications require some sort of data to work from so lets start by defining IData which can load data.\npublic interface IData { Blog LoadData(string Connectionstring); } My interface defines one method LoadData and outputs an object called Blog (I will explain why in a minute)\nA common data source is a SQL database so we could define a SQL class that implements IData.\npublic class SQL : IData { public Blog LoadData(string Connectionstring) { Blog blog = new Blog(); using (SqlConnection con = new SqlConnection(Connectionstring)) { con.Open(); //etc We could also get data from an RSS feed of a blog (hence why I called the object Blog earlier)\npublic class XML : IData { public Blog LoadData(string Connectionstring) { XmlDocument myXmlDocument = new XmlDocument(); myXmlDocument.Load(Connectionstring); Blog blog = new Blog(); foreach (XmlNode RootNode in myXmlDocument.ChildNodes) { //etc Both classes implement IData and have a method called LoadData which has a string parameter and outputs a blog object. The string parameter is either a connection string to a SQL database or the URL of the rss feed. Not sure if there is a better way of doing this bit, maybe the name of the string needs making more generic.\nNow we have some classes that implement an interface what can we do with them. Lets write a class called GetData which gets data but doesn’t care if it comes form the rss feed or a SQL database.\npublic class GetData { private IData _repo; public GetData(IData repo) { _repo = repo; } public Blog LoadData(string Connectionstring) { var original = _repo.LoadData(Connectionstring); return original; } } When we call GetData we can either pass in XML or SQL as the class is not tied to either implementation. We could even write other classes that implement IData for testing purposes.\nMy full code can be found on github .\nThe advantages of writing code in this way include code that is easier to extend, easier to test and easier to maintain. This is only the start of my understanding and I am sure this is going to be a topic I come back to in the next few weeks.\n","date":"Mar 27, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/interfaces/","series":null,"tags":["C-Sharp","Interface","Programming"],"title":"Interfaces"},{"categories":null,"content":"This week I have been looking at improving my understanding of a few programming terms, like SOLID and I will try and define them so I can look back here when I get confused.\nMVC I have previously blogged about MVC , but my understanding was not 100% correct so I will refine this here.\nModel – Now this is where my understanding was not quite correct. I thought the model was the actual source data, eg an XML file, SQL database etc. The model is the business logic so this is a processed version of the source data. MVC does not care where data is stored it can be flat files, SQL, XML or anything really.\nView – This displays data to the user and typically is the HTML/CSS markup. Only display related logic would get included in the view.\nController – This is another place I had got a bit confused. I had thought all the logic lived here. This is incorrect controllers are only concerned with getting data between Model and View.\nSOLID SOLID are five principles of object oriented programming and design.\nS is for Single Responsibility Principle. A class or piece of code should be responsible for doing one thing.\nO is for Open/Closed Principle. Code should be open for extensions but closed for modifications. Often this refers to the way you can implement an interface and add extra functionality.\nL is for Liskov Substitution Principle. Objects in a program can be replaced with subtypes of that object with out changing functionality.\nI is for Interface Segregation Principle. Large interfaces should be split down into small interfaces so that clients only know about methods that are of interest.\nD is for Dependency Inversion Principle. High and Low level modules should depend on abstractions.\nI need to look into SOLID some more but here are some examples .\nDRY DRY is simply Don’t Repeat Yourself. It is often easy to spot when a function call will help you not have to use the same bit of code in different places. I am often spotting places where I can put this into action.\n","date":"Mar 20, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/solid-programming-terms/","series":null,"tags":["Design Patterns","DRY","MVC","Programming","SOLID"],"title":"SOLID and other programming terms"},{"categories":null,"content":"A few weeks back I attended a talk at Agile Yorkshire about Test Driven Development or TDD by Dr Oliver Shaw. I was impressed at how easy Oliver made it look, so as I have never tried it I thought I should give it a try.\nTest Driven Development or TDD is a way of development which starts with writing a Unit Test. First you write a failing test, then you write the code to make it pass, then you refactor your code. This can be remembered by thinking of Red, Green, Refactor. Red being the failing test, Green being getting the test to pass, and Refactor being the refactoring.\nDuring the demonstration Oliver used a language called scala and a system that automatically reran all the tests after every change. I code with Visual Studio in C# is there a way I can get my tests to run automatically as well?\nA bit of googling and configuring I can answer this as Yes.\nThe nuget package called Giles is a watcher which will rerun your tests similar to how Oliver did it with his scalar environment. Fans of Buffy the Vampire Slayer will get the joke of why a watcher is called Giles. I couldn’t get this to work with MSTest but works fine with NUnit. There is a powershell script giles.ps1 which you need to run and will update every so often with how many tests have passed or failed. However you may not see this if you are coding in Visual Studio but there is a way to get a notification.\nIf you install the application Growl you can get notifications from Giles which pop up and then disappear. So whatever you have on screen you can find out almost instantly if you have broken tests.\nAnother thing that I wanted to configure is a way of viewing code coverage and which methods are tested and which aren’t. If you are familiar with VSTS after a build it gives you a percentage score for test coverage. I don’t find this overly useful as it doesn’t tell you what is covered and what isn’t. Also what if you want to use Github, how do you calculate the code coverage then?\nThe nuget packages OpenCover and ReportGenerator allow a html report of code coverage to be produced. I created a batch script that can be run whenever you require this information.\n[path]\\OpenCover.Console.exe -target:\u0026#34;[path]\\nunit3-console.exe\u0026#34; -targetargs:\u0026#34;[path]\\Test.dll\u0026#34; -output:\u0026#34;[path]\\coverage.xml\u0026#34; -register:user [path]\\ReportGenerator.exe \u0026#34;-reports:[path]\\coverage.xml\u0026#34; \u0026#34;-targetdir:[path]\u0026#34; The commands are fairly straightforward, the only tricky bit is sorting out all the filepaths to the different programs.\nNow that I have all this plumbing setup time to give TDD a try and see what I can build.\n","date":"Mar 13, 2017","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/03/tdd_flow.gif?resize=287%2C300\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2017/test-driven-development-tdd/","series":null,"tags":["Test Driven Development","Testing","Programming","Visual Studio"],"title":"Test Driven Development or TDD"},{"categories":null,"content":"As a developer using source control and git is bread and butter of what we do. Github is probably the most popular and widely known hosting service for source control but I have also used Bitbucket and Visual Studio Team Services. Lets have a look at each one and what they offer. Note while I have included prices I have only tried out the free versions.\nGithub URL: https://github.com/ Private Repositories: Not Available for free Public Repositories: Unlimited Team Size: Unlimited Prices: $7 per month for unlimited private repositories, $25 per month for 5 users then $9 per month per user This is probably the most widely used service for hosting code. Over 13 million repositories of code. This is an ideal solution if you want your code to be publicly viewable, but take care not to publish passwords, private keys or your companies trade secrets. Every developer should have a Github account for displaying bits of code they are proud of.\nThere are a number of externally built APIs that link into Github for doing extra features, like building, code coverage etc\nBitbucket URL: https://bitbucket.org/ Private Repositories: Unlimited Public Repositories: Unlimited Team Size: Less than 5 Prices: $10 per month for 10 Users, $100 per month for 100 Users, $200 per month for Unlimited Users At my last job we used Bitbucket extensively for all our projects. All the code was private so only the team could access it, however before I left we were approaching the 5 user limit (but looking at these prices cost seems very reasonable)\nVisual Studio Team Services URL: https://www.visualstudio.com/team-services/ Private Repositories: Unlimited Public Repositories: Not Available Team Size: Less than 5 Prices: $30 per month for 10 users, and other features paid for via Azure Invoices Visual Studio Team Services or VSTS is Microsoft’s version control solution and I have only just started using it, however what I have seen I like. There are lots of options for building your code so VSTS is more than just hosting your code it is verging on a Continuous Integration/Delivery solution. Being a Microsoft product there are numerous links to Azure and it is easy to deploy stuff to that platform.\nAll three have options for tracking issues but VSTS have options to add Stakeholder users which would allow none developers to add and keep track of issues and progress with them.\nIf I want to run tests, look at code coverage VSTS is probably the solution I would go for, if I want something that is public I would go for Github. What do you think which of these is your favourite?\n","date":"Mar 6, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/github-vs-bitbucket-vs-vsts/","series":null,"tags":["Git","Bitbucket","DevOps","Programming","VSTS","SourceControl"],"title":"Github Vs Bitbucket Vs Visual Studio Team Services"},{"categories":null,"content":" Today I spent some time learning the R language.\nThe problem I was trying to solve was to convert local prices of some items into Euros. I had been using a fixed exchange rate for all data, but as exchange rates fluctuate so much this is incorrect.\nMy first though was to find a free API that I could query to get the values I wanted. The first API I found didn’t cover all the currencies, the next one I found I burnt through the free allowance in one pass.\nA colleague of mine mentioned using R to solve this, he sent me some links and I set out to write my first piece of R code.\nMy finished code can be found on github and I will attempt to explain some of it.\nR defines functions fairly simply\nnameoffunction \u0026lt;- function(arg1, arg2) { arg1 * arg2 } I have created a function that takes 2 parameters date and currency. I know I have about 10 different currencies that I want to get currencies for and I want to loop through each day so I will need to pass in a date.\nThe source of my exchange rate information is the www.xe.com website, its historical exchange rate page passes currency and date into the query string so I should be able to build up a string containing all the different elements.\nAll programming language can concatenate strings and R is no different. R uses paste()\nvar \u0026lt;- paste(\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;) However R has an annoying feature in this function. I would expect that var in the above example would contain \u0026ldquo;HelloWorld\u0026rdquo;, it doesn’t it contains \u0026ldquo;Hello World\u0026rdquo;. Why it automatically adds a space I don’t know?\nvar \u0026lt;- paste(\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;, sep=\u0026#34;\u0026#34;) I am not entirely sure what all of the code does but I can take a good guess.\nread_html() I would guess loads a html page, html_nodes() finds all the html tags of a certain type on the page, in my case \u0026lt;table\u0026gt;, html_table() reads the first table it finds.\ntable1[2] selects the second column, and head() selects a specific number of rows. I want the first row and second column so I combine these two as head(table1[2],1)\nNow that I have found my exchange rate what do I do with it? R can read and write to SQL Server so why not store this info in a SQL lookup table. I can then use this data in a stored procedure when I process my data.\nTo query sql you can use sqlQuery(), it has two parameters, a sql connection and a TSQL command (eg a SELECT, INSERT, UPDATE statement)\nI use a while loop to loop through every day between 1st October 2016 and today and look up the exchange rate for each currency.\nFor now I am manually running this R script, however there are ways to run R directly from SQL Server which I may well investigate. I could then have a SQL job to run this on a schedule, maybe once a day to get the latest exchange rates. I also would like to do something a bit cleverer like only getting exchange rates for the days that I need them by querying existing database tables.\n","date":"Feb 27, 2017","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2017/02/Exchange-Rate-Calculator.jpg?resize=300%2C202\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2017/learning-r/","series":null,"tags":["ExchangeRates","R","Programming"],"title":"Learning R"},{"categories":null,"content":"Hello?\nYou have probably like me almost forgotten about this blog. Life has got in the way over the last few months, but lets see if I can restart my blogging habit.\nIn 2017 I want to start a side project for a few reasons. I want to improve my coding skills and look at things I wouldn’t normally during my day job.\nI have been trying to decide what to build. The actual project doesn’t matter too much, it is the techniques I use during build that matters the most. I would like something that I can start off as a windows app and then extend onto the web or mobile apps.\nI started off thinking about a Rubik’s cube app, however my initial coding has lead me to believe it is probably too complex for my first app.\nLast year I spent an evening looking at Connect 4 during a York Code Dojo session. I think this should be complex enough that I can code some logic to efficiently solve the game, however not too complex I get defeated before building too much.\nSo far I have built a console app that displays via Console.Write() commands, a grid to represent the game, with 0 meaning empty, 1 red and 2 yellow etc. I am currently working on the logic to solve the game, once I have that in a good state I can extend into a winforms app, into a web (MVC most likely) app and finally a mobile app (I have no knowledge about how to do this yet, but something I would like to try one day).\nOver the last few days I listened to .NetRocks where they discussed Progressive Web Apps . This sounds like a great challenge for me to aim for. A progressive Web App is a website that automatically detects various properties to give you an app that runs well whatever the state of your internet connection and whatever browser you are in.\nI have lots to learn but I can split it down into small sections and I can build something that demonstrates some of my skills.\n","date":"Feb 6, 2017","img":"","permalink":"https://www.funkysi1701.com/posts/2017/side-project-connect-4/","series":null,"tags":["Connect4","PWA","Website","Programming"],"title":"Side Project – Connect 4"},{"categories":null,"content":"I am currently working on source code that is over 5Gb in size. This is mostly due to a poorly thought out folder structure, there are code files, images and Excel files all jumbled together. I think a clear distinction should be made between source code and data.\nSource Code I will define source code as anything that is written in order to compile and run the project. If it is a webpage it will be all the HTML, CSS and Javascript or any file used to produce these. I would also include any configuration files and files used to build/deploy the website or project. Anything that is compiled from your source files can safely be ignored.\nData I would define data as anything that is added to the project during its life. So if you have an upload option, anything that is uploaded I would describe as data. The site should still function without (or very little) data.\nImages Images can fit into both groups. Any icons or images attached to the functionality of the project I would class as source code. However anything that is uploaded should be classed as Data.\nDatabases The database should also be classed as both. The data, anything that is inside a database table should normally be classed as data. Stored Procedures, Functions and Views are all Source Code and would benefit from version control.\nSource Control != Backup Source control is not an excuse not to backup things. Don’t just commit files to source control so you know you can restore them if you need to. Files in general in source control are there so you can see how they changed over time as the code base changed. Files in you backup are a snapshot of what the application was at a point in time and will include ALL the data.\nOne last point before I end. If you are hosting on a Cloud Computing platform like Azure it gives you an easy way to distinguish between Data and Code.\nAnything in your\nWeb App = Code Blob Storage = Data SQL = Data/Code Each project is unique and there will always be exceptions to these suggestions but I think this is a good goal to have. What do you think?\n","date":"Nov 24, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/what-should-be-in-source-control/","series":null,"tags":["Git","SourceCode","SourceControl","Programming","C-Sharp"],"title":"What should be in Source Control?"},{"categories":null,"content":"So recently I started working on a new codebase. I will be honest when I first saw it, it was a mess. Here are a few of the things I did to try and regain control.\nI was given access to the source code on Visual Studio Team Services. However this consisted of a single commit 3 months ago. When I looked at what was running on the production server it was clear that changes were being made live with no regard for source control.\nThe first thing I did was commit everything that was running live into source control.\nNext I created a SQL Server Data Tools (SSDT) project to keep track of all the database objects. Previously there was a folder with some stored procedures in it, but these did not match with what was currently running.\nI now had in source control the current state of the website and the database, so I knew I could get things back to this state if I made some bad changes.\nLets start by looking at the website code I had. There was no solution file, the only way to look at the website was to setup my local IIS to run what was in the website folder. I could then use Visual Studio to \u0026ldquo;open\u0026rdquo; my local IIS website and attach to process to debug it.\nNext I Looked at Default.aspx to see how the website worked. The majority of the website code was stored in the database stored procedures. After the tag there was a \u0026lt;% %\u0026gt; which contained a Response.Write(RunSP.RunStoredProcedure(Parameter1, Parameter2, \u0026hellip;) command, which executed a stored procedure and the results of the stored procedure was the html code including any javascript that the webpage needed to display. I will be honest I have never seen any code like it. My guess is that the developer was secretly a DBA and wanted to make any web page changes by just changing how the stored procedures work.\nThis meant that the website is not going to do anything without a backup of the database running, and meant my SSDT project was going to be vital. However the database was in a bad state, it consisted of a fair few broken objects and SSDT would not build.\nUsing find I went through each of the broken database objects to find where in the code they were being used. Luckily most were referenced in commented out code, so I just removed all the broken database objects. The database could now be built. However there was a dependency on the users table of another database. (This was the developers solution to sharing logins between websites) As I was using SSDT I added a database dependency, problem solved for now.\nNext I tried publishing my database. SQL CMD encountered a parsing error. The reason for this was my SPs contained javascript eg $(document), SQL CMD uses $(DatabaseName) as variables for different database so it was getting itself confused.\nMy solution was to use Find and Replace to replace all the $ with \u0026rsquo; + CHAR(36) + '\nSo I now have a SSDT project that builds and publishes but still no website project.\nTo get the website running from Visual Studio I started off creating a .Net 4 website project and added Entity Framework 5 and MVC 3 via nuget. I then copied all the website code into the new project, and with a bit of work I got it to build. Most of the work was relating to namespaces and referencing the correct one and moving the EF model from AppCode to a custom named folder. A bit of trial and error later I had a version of the website that could be run from Visual Studio.\nI have not deployed my new version of the website as it needs further testing. No automated testing or even a smoke test checklist currently exist.\nAs my source code is hosted on Visual Studio Team Services (VSTS), I can get VSTS to build each commit and check I haven’t broken the build. This is not that helpful at the moment, hopefully one day I will have automated tests that can be run here as well.\nWow, I feel like I have done loads with this code so far but there is loads more still to do. I need to understand more about the business processes behind the code with a hope to understand why some architectural decisions have been made. I want to refactor the code as much as is possible, I would like to remove much of the html/javascript from the stored procedures as I can’t see that there is any advantage to running a website like this. Please correct my if I am wrong.\n","date":"Nov 17, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/getting-control-of-a-codebase/","series":null,"tags":["Javascript","VSTS","SQL","Visual Studio","Programming","C-Sharp"],"title":"Getting control of a codebase"},{"categories":null,"content":" What is LINQ? LINQ is an acronym for Language Integrated Query, which describes where it’s used and what it does. The Language Integrated part means that LINQ is part of programming language syntax. In particular, both C# and VB are languages that ship with .NET and have LINQ capabilities.\nHow do I use LINQ in my C# code? To use LINQ the first thing you need to do is add the LINQ using statement.\nusing System.Linq; In your code you need a datasource, for this example I am going to use a simple array, but it can be anything eg SQL, XML etc\nint[] data = new int[10] { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 }; Next you need a LINQ query. (Note I know the Q in LINQ means query, so I have just written query query, if you are one of those people who hates seeing PIN number you might not like this blog post.) A LINQ query is very similar to a T-SQL query, so if like me you are good with databases this should make sense to you.\nIn T-SQL you can have:\nSELECT num FROM data WHERE num = 1 In LINQ this becomes:\nvar query = from num in data where num == 1 select num; Finally you need to do something with the query you have written. I am just going to print the results of my query to console.\nforeach (var num in query) { Console.Write(num); } What other SQL like syntax can I use? In T-SQL you can control ordering using ORDER BY, LINQ has a similar syntax orderby\norderby num descending In T-SQL you can use GROUP BY, to do something similar with LINQ\ngroup num by num.Type into type select type foreach (var type in query) { Console.Write(type.Key); foreach (var num in type) { Console.Write(num); } } JOINS So you thought joining tables was a SQL Server only thing. Think again you can do this in LINQ\nvar joinquery = from cust in customers join prod in products on prod.CustomerId equals cust.Id select new { ProductName = prod.Name, CustomerName = cust.CompanyName }; Conclusion There are loads more LINQ functionality that you can use. While writing this blog I found https://code.msdn.microsoft.com/101-LINQ-Samples-3fb9811b which has loads of examples of different queries that you can write with LINQ.\nThis has inspired me to use LINQ more in my code and learn more about the different queries that could be written.\n","date":"Oct 6, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/10/step30.jpg?w=515\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2016/linq/","series":null,"tags":["LINQ","SQL","Programming","C-Sharp"],"title":"LINQ"},{"categories":null,"content":" Next month I will celebrate ten years working at my current job, two weeks after that I will start a new chapter of my life at a new company. Lets take this opportunity to look back ten years at some of the great stuff I have achieved.\n2006 In 2006 I had no IT experience. If you were to ask me where Event Logs could be found I wouldn’t know, and the letters DNS meant nothing to me. I was quiet and shy but hard working. The idea of making IT my career hadn’t occurred to me, this was just a job.\nI started off life doing first line support and compiling health and safety files onto CD-ROMs (known by the company as Eurofiles) with HTML. At the time the company had a mix of Windows 2000 and Windows XP, using MS Office 2003. Server wise I can’t remember exactly but I think two domain controllers, a database server and a backup server using a tape drive. At this point in time I don’t believe I did a lot with the servers but I think the servers were running Windows Server 2005.\nAs time went on I learnt more and more about what the company did and how stuff worked.\nNagios One of my early achievements was setting up a nagios server. I have blogged about Nagios before, but it is a server monitoring system that runs on linux. I am extremely proud of what I have achieved here, no one else in the company had my knowledge of how nagios worked. Initially I even experimented with using a cheap mobile phone connected to the serial port (remember when PCs had these?) to send SMS messages to alert of down time. However this was abandoned when I burned through all the credit on this phone.\nNagios is still used today, and my current team have been shown how to extend the system as the business changes. My philosophy has always been if we have a problem that Nagios didn’t warn us about we need an extra nagios test. These days the nagios web interface is publicly available on the internet so can be checked from anywhere (assuming the office internet is up!) and a mobile app replaces the SMS idea we originally had.\n2011 At the end of 2010 the IT Manager resigned, then the Developer resigned and finally the Lead Developer resigned. I was alone. I was IT Manager. I negotiated my first pay rise since joining the company. I was IT.\nWow things were stressful back then. I had so much to learn but somehow I managed. I learnt how to Interview, yes I have employed some clangers along the way, but also some great staff. As well as internal staff I learnt to deal with contractors. We had contractors to help with our internal systems and also contractors to do development work. One thing I have learnt about contractors is that you will always have to chase them at some point to deliver what you are paying them for.\nServer Migration Around the start of 2011 our email server started to show its age. Exchange 2003 had a hard limit on the size of its information store and we were rapidly approaching this. Along with an IT contractor I worked to migrate to Exchange 2010, this was a huge project and caused all sorts of issues which we just worked through. Since then I have done other migrations so Windows Server 2012 and also virtualized many servers in 2013.\nBandwidth A major limit with our infrastructure has always been the internet connection coming in. For a while we tried to load balance three ADSL connections, but the upload speed was always a limiting factor. It was a major victory for the simplification of our network when we got a leased line installed into our head office, helped by a government grant we gave us free installation. Not content with this I did the same again for our second York office, including the free installation.\nLearning Development Since I started I have always been learning development stuff. But in the past few years I have learnt a massive amount helped largely by two main things. My boss passing lots of my responsibility onto others and dedicating lots of my time to development tasks, and also the opportunity to learn with our outsourced development team.\nFinishing off There are a few things that have been ongoing that we have wanted to change since I started which I can’t really take the credit for as they are not complete yet, but I am proud as they may be finished in the next few months.\nThe company stores calendar information in one giant excel spreadsheet, this is being replaced by outlook calendars.\nOur last Windows Server 2003 server and tape drive is being decommissioned. This is the only server that has been running for the entire time I have been here.\nThe way the company keeps track of work coming in and who does what is being reviewed. Over the years many people have tried to adapt the old database that one of the directors created in Access many years ago. I did a major overhaul recently to delete unused columns and added extra invoicing functionality. What is needed is a fresh system, maybe a CRM can do everything they want. I wish them lots of luck in doing this as I know it won’t be easy but it has to be done.\n2016 There are many other projects and pieces of work that I have done that I am proud of. In 2016 I will leave a stable IT department. All clients are on Windows 10 and using Office 2010. We have virtual servers running Server 2012 and are using some great services from Azure to run our SaaS websites. There are of course things I would have liked to achieve, and things I wish I had done differently but on the whole it feels like the right time to move on, especially now long running projects are starting to conclude.\n","date":"Sep 29, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/looking-back-ten-years/","series":null,"tags":["DevOps","ITAdmin","Servers","Nagios","2016","2006","Programming","C-Sharp","Career"],"title":"Looking back Ten years"},{"categories":null,"content":"So after five and a bit years of being an IT Manager here is some advice I have learned along the way in no particular order. On the whole I have enjoyed myself but it has been a real challenge at times.\n1. Figure out what plates are still spinning Being an IT Manager is all about keeping everything running all of the time. A bit like spinning 5 or 6 plates. You have plates for your servers and network infrastructure, you have plates for bespoke databases that you maintain, you have plates for your staff (including any external contractors), you have plates for any websites or apps that you develop. That is a lot of plates to keep spinning and that before you start thinking about what your boss wants you to deliver. Make sure you know what is happening with all these plates, which ones are happy, which ones are on the way to the floor and which ones you need to get the glue out and repair.\n2. Make it someone else’s problem If you can blame someone else do so. If your internet goes down it is your ISPs fault. If your website dies its your hosting company’s fault. Take responsibility for problems but if when something goes wrong you can pick up the phone and ask for help, it will make your life easier.\n3. Hire good staff Hiring poor staff wastes time and money and makes you look bad by others. Demand the highest salary band for new staff that you can afford and don’t agree to hiring anyone that you have doubts about. It is easy to bow to the pressure to get someone quickly but this will always result in worse problems in the long run. Once you have a good team do your best to keep them, and warn upper management of the problems if staff leaves (basically make it their problem not yours!).\n4. Learn, Learn, Learn You may or may not have the opportunity to go on training courses. Whatever your situation spend time learning new stuff that will benefit the company and yourself. You can learn a lot by reading online, you can petition for training from your managers, you can fund training yourself, you can ask for help from your different suppliers. The more you learn, the more you can do and the more useful you can be to the company, plus the more interesting you will find the job.\n5. Say No! Don’t be afraid to say no. You will always be asked to do the impossible and if something is impossible say so at the start. It wastes everyone’s time if you spend a lot of time trying to do the impossible. Always give your reasons for saying no, and if you always say no people will think you are unhelpful. A better way to say no is to come up with a better solution. No I can’t do it your way but here is a better solution.\n6. Don’t give estimates If you are asked how long something will take you don’t answer straight away or give an exaggerated estimate. Go away and spend some time thinking of everything that is involved before replying. There will always be something that you forgot to consider when first asked about it and looking at the different components will help plan out the work needed as well as provide an estimate.\n7. Know what to tell your boss, and what not to This is a hard one to get the balance right for. You need to tell your boss enough so that they appreciate all that you do, but too much and they will stop listening and accuse you of talking in technobabble. I have never got the balance right with this one. I have always aired on the side of not telling my boss enough, and hence they don’t realize that I saved the day on Sunday night as everything is working again on Monday. Do repeat yourself. If your server is running low on resources start asking for replacement hardware early, and increase the frequency and the panic in line with the problems it is causing.\n8. Understand the problems of the business Businesses need to make money. If the one you work for isn’t making enough money you will soon be looking for another. If you work for IT you will quickly start to see the problems of the business, think about what simple changes IT could make to improve things that would benefit the whole company. Some of your suggestions won’t go anywhere, but some may have a massive impact. I can think of a few changes that IT have spearheaded that I am very proud of, upgrading our internet connection, simplifying or automating processes and delivering new versions of software.\n9. Ask for help Don’t be afraid of asking for help. There are lots of places to look for help. Other departments could take more on, you could recruit extra help, you could hire external contractors. You can ask questions on support forums like ServerFault or StackOverflow , many software re-sellers or other suppliers are a good point of contact for questions about things they supply. Microsoft Support was also invaluable for a server issue.\n10. Think about disasters Write a disaster recovery plan or backup policy. Yes there will always be something more important that needs doing, but just stop for a moment to think how you would feel if everything died on your watch. The one thing you can rely on with technology is that it will fail at some point. A back of the envelope plan of action is better than no plan at all, even better is a detailed plan of what to do when each and every service you rely on fails. Plan additional services with an idea of adding extra redundancy. Always have multiple Domain Controllers, think about what data you could run from the Cloud. VMs could be replicated to the Cloud, and servers could be run from there.\n","date":"Sep 22, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/09/bhrzpww6aehdx1wvrrug.jpg?w=800\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2016/10-ways-to-survive-as-an-it-manager/","series":null,"tags":["ITAdmin","Backups","DevOps","Technology","Programming"],"title":"10 Ways to Survive as an IT Manager"},{"categories":null,"content":"This week I handed in my notice at a job I have had for almost ten years. In a few weeks time I will start a brand new job as a web applications developer.\nThis is a great achievement for me and a great chance to learn and expand my development skills.\nHowever moments after I handed my notice in, a huge wave of uncertainty enveloped me. I have read about this and I think I was suffering from an attack of Imposter Syndrome.\nDespite external evidence of their competence, those with the syndrome remain convinced that they are frauds and do not deserve the success they have achieved. Proof of success is dismissed as luck, timing, or as a result of deceiving others into thinking they are more intelligent and competent than they believe themselves to be.\nI started to convince myself I couldn’t do the job I had just accepted and that moments after I arrived on my first day I would be found out as a fraud and kicked out the door.\nMy logical brain was no match for the Imposter Syndrome. Having been kicked out the door of this new job, I would become penniless in no time with no employer (even my previous boss wouldn’t want me back) or recruitment agency willing to speak with me.\nOK lets see if we can fight off the Imposter Syndrome and look at a possible worse case scenario. I start at my new job, it is very hard work and a month or so later I have to move on.\nNo employer ever would sack me on my first day. My first day will be lots of learning how they do things, looking at procedures. I have lots of experience of learning new things, looking at procedures and if I put the work in I can make this new job a success. I have lots of good ideas and useful experience that no one at my new company has.\nI don’t know what the solution is to Imposter Syndrome other than to try and ignore it by thinking positively and the knowledge that I have done amazing things in the last ten years and I will do the same in the next ten.\n","date":"Sep 15, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/09/1429677066685.rendition-medium.jpg?w=598\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2016/imposter-syndrome/","series":null,"tags":["Newjob","Imposter Syndrome","Programming"],"title":"Imposter Syndrome"},{"categories":null,"content":"On September 8th 1966 a TV show called Star Trek first aired. Today 50 years later people around the world are still watching Star Trek.\nI was going to blog about the Star Trek story, but instead why not watch this video.\n","date":"Sep 8, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/01/07df3N8m.jpg","permalink":"https://www.funkysi1701.com/posts/2016/happy-50th-star-trek/","series":null,"tags":["StarTrek"],"title":"Happy 50th Star Trek"},{"categories":null,"content":"For a while now I have been sharing some of my blog posts on the Dzone website.\nThe Dzone website allows users to submit links to content and I have been submitting the content I have created on this website. This is how Dzone describes themselves:\nWith over 1 Million members, DZone.com is one of the web’s largest communities and publishers of technical content for software professionals. Developers from all over the world come to DZone for the latest and best content to hone their skills and advance their careers.\nWell this week I have been invited to join the MVB (Most Valuable Blogger) programme. The hope is that more of my readers will find my content from the DZone website.\nThe DZone team will soon start sharing my content on the DZone website and I hope this will result in even more people reading what I have to say.\nIf nothing else this is clearly telling me that my blog is making a difference, people are taking note of me and what I have to say. I need to keep going and keep writing articles and be more consistent.\n","date":"Aug 25, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/08/dzone_02.png?w=400\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2016/dzone/","series":null,"tags":["Blogging","DevOps","Programming","Social","DZone"],"title":"DZone"},{"categories":null,"content":"Last time I started looking at Amazon Web Services and how it differed from Azure. I am going to continue looking at what it can do.\nVirtual Machines Lets look at what you can do with Virtual Machines. I selected to create a new Virtual Machine (or as AWS calls them an EC2 Instance)\nFirst you choose a name for your VM and then the OS that runs on it. There are 5 main OSes to choose from Windows Server, Amazon Linux and a selection of the most common Linux flavours.\nYou can then download a certificate to secure your VM.\nLike Azure, AWS takes a few moments to create your VM. While I wait I can see that AWS has configured a firewall so only my current IP can connect to it.\nOnce the VM is ready you can download an RDP file. However to get the login details you need to decrypt the password using the certificate you downloaded when you created the VM.\nIt is interesting to compare the difference in security between Azure and AWS. Azure allows the resetting of passwords of VMs directly from its console, however I suspect that in AWS if you loose your certificate (AWS states they don’t keep a copy of this) you would have to recreate your VM.\nLike with the Websites the default name of the VM is much less user-friendly than what you get from Azure. However I suspect there are other options I haven’t spotted that may customise these.\nAzure Portal vs AWS Console I really like the Azure Portal. It feels like something that has been designed so you can easily access all the options for a specific Azure feature.\nThe AWS Console probably has all the same options as with Azure however I don’t think it looks half as good, and will take me a while looking through menus to find the equivalent options. Part of this is due to my unfamiliarity with AWS, so will get easier with time.\n","date":"Aug 4, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/amazon-web-services-pt-2/","series":null,"tags":["AWS","Azure","Amazon","Programming","Clouds","DevOps"],"title":"Amazon Web Services Pt 2"},{"categories":null,"content":"I am a big fan of Azure but I know zero about its biggest rival – Amazon Web Services or AWS.\nSo lets sign up for a free trial and see what it can do. The AWS free trial is available from https://aws.amazon.com/free/ and lasts for 12 months. From memory I think the Azure free trial lasted only one month.\nTo start you need to login with your amazon account and create an AWS account. This requires your name and address and your payment info (you will only get billed if use services not covered by your free trial).\nInterestingly AWS requires you to verify your identity via an automated phone call. (I don’t recall doing anything like this for Azure but please correct me if I am wrong.)\nOnce you are logged in you get a series of links displaying all the different services that are available. First impression is this is a simpler view to Azure’s portal with a similar amount of services. At the top right is an option to select which region you want to use, in Azure I use North Europe and West Europe, AWS has Ireland and Frankfurt.\nCreate a Web App First thing to try is setting up a website. I selected create a web app and I get a page asking me for its basic details (very similar to Azure, however AWS asked what language your code is written in, Azure handles all of these) AWS websites appear to support a host of different options similar to Azure.\nThe actual creation of your website takes a few moments (like on Azure). However the default URL for websites is similar to http://test.vjbbimyv7w.eu-central-1.elasticbeanstalk.com/ which is not quite as nice as the Azure equivalent http://test.azurewebsites.net Azure has a host of command lines available via powershell. AWS has a similar command line interface called AWS CLI, including the option to deploy from git to your website.\nAWS Toolkit for Visual Studio is an extension that allows for the publishing of websites to AWS. (Just like you can publish to Azure)\nAs I learn more about AWS I will continue to blog about it. Amazon Web Services Pt 2 ","date":"Jul 21, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/07/aws-300x169.png?resize=300%2C169","permalink":"https://www.funkysi1701.com/posts/2016/amazon-web-services/","series":null,"tags":["AWS","Azure","Amazon","Programming","Clouds"],"title":"Amazon Web Services"},{"categories":null,"content":"I hate MS Access and especially developing with it as you can’t do any thing clever with it.\nWrong, Wrong, Wrong!\nThere are a few clever things I have been able to script to make developing with it passable. I still would rather use Visual Studio but this improves the experience a fair bit.\nSource Control Access files are binary (I use the ADP/ADE file format but I assume other Access file formats have the same problem) and so you can’t diff them to see what has changed. This is bad.\nHowever there is a solution to this. A tool called Access SVN and can be downloaded from https://accesssvn.codeplex.com/ , this gives you a way to extract to text files all the forms and reports that are in Access. Before every commit I would manually run this tool on my ADP file and extract to text files, then I would commit these text files to source control and could easily see what had changed in each commit.\nDespite the name Access SVN, the tool is not tied to subversion, you can use any source control system, I use git.\nAlso included in this tool is a way to do this with the command line, so you can make this a build step on your build server. I have not used this extensively yet, but the syntax is fairly simple\nasvn.exe e \u0026#34;path to Access file\u0026#34; \u0026#34;path to txt files\u0026#34; \u0026#34;*.*\u0026#34; The filter at the end . allows you to specify what to extract so you could extract all forms/reports beginning with D with \u0026ldquo;.D\u0026rdquo;. I had trouble using . because the names of my forms/reports contain characters not allowed in a windows file name. I am sure there is a way round this but I haven’t had chance to look into it further yet.\nTesting MS Access Surely testing is not possible with MS Access? I would have agreed with that statement the other day until I found a neat way of testing if a feature is enabled.\nFirstly a bit of background. I develop using MS Access 2003 because the design view is far easier to use, however because it is out of support all my users use MS Access 2010. MS Access 2010 has a feature called Tabbed Documents which allows all forms and reports to open in new tabs so you can easily switch between them. This feature can only be enabled in MS Access 2010 and has no effect if opening with MS Access 2003.\nIf you use Access SVN on your Access file with tabbed documents turned on and off you will see UseMDIMode: 0 and UseMDIMode: 1 show up in the Database properties file. UseMDIMode: 0 means that tabbed documents is turned on.\nIn powershell I can now write a test to see if UseMDIMode: 0 can be found in the database properties file\nGet-Content \u0026#34;General\\Database properties.dbp.txt\u0026#34; | Select-String \u0026#34;UseMDIMode: 0\u0026#34; -quiet If the test passes True will be returned, if it fails null will be returned.\nOn my build server I scripted the extraction of Database properties.dbp.txt from the ADP file with asvn.exe before running this test. While not strictly needed as Database properties.dbp.txt should be in source control, it is possible that someone could forget to extract the text files from the ADP, with this step you are always testing what is enabled in the binary file.\nMS Access Connection Strings While developing with MS Access I often swap the database connection to point to my local machine or a build server. I always try and remember to only ever commit with this set to the live database to avoid obvious problems.\nThe other day I found on stackoverflow a way to script this. I love this! I can include this step in my deployment process and it will overwrite what ever the connection string is in source control with what your production environment needs.\nAll you need to run this step is, (note it is spaces between the parameters not commas)\ncscript connect.vbs Project.adp \u0026#34;ServerName\u0026#34; \u0026#34;DatabaseName\u0026#34; The contents of connect.vbs can be found on the stackoverflow article . It is also possible to pass username and password if your environment requires this.\nCompiled ADE The last clever thing I do with MS Access is convert my ADP file into the compiled ADE version. To manually do this there is an option in the tools menu.\nTo automate this I run\ncscript createADE.vbs \u0026#34;path to ADP\u0026#34; \u0026#34;path to ADE\u0026#34; The contents of createADE came from this forum post , the only change I made was to comment out some of the echo statements so it would run silently as part of my build process. It should be noted that cscript and wscript are almost identical and either will run these scripts however in a command line environment cscript is preferable, and wscript should be used in a windows environment.\nI am quite surprised at how much I have managed to do in terms of scripting the build and deployment process for MS Access. I still don’t like developing with Access but this has definitely improved things.\n","date":"Jul 7, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/clever-things-ms-access/","series":null,"tags":["Database","Access","DevOps","Programming","Source Control"],"title":"Clever things with MS Access"},{"categories":null,"content":"The periodic table lists all the chemical elements and groups them together based on some key properties. Today I found an article about the periodic table of DevOps .\nI am not going to discuss every element but I thought I might go through some that I have heard of or used.\nGithub – The repository of lots of open source software. My Github can be found at https://github.com/funkysi1701 Amazon Web Services – The second most popular cloud computing provider (not sure why this isn’t number 3?) Git – The distributed source control system that everyone uses these days. Azure – The number one cloud provider, I have used this a lot, mostly with websites but also with some of their other features like Traffic Manager. Bitbucket – like Github but allows private repositories. I have used this extensively for work based projects that I don’t want to be public. Google Cloud Platform – don’t know much about this one, but no surprise that google wants a piece of the cloud computing pie. Selenium – This is a product I want to play about with as allows front end testing with a browser. Rackspace – Before we made the jump to Azure we made use of some Rackspace servers. Subversion – The first source control system that I used, but been using git so long now not sure I can remember how it worked. Visual Studio – The IDE from Microsoft that I use to write code. I am a big fan as it does everything I could want. TeamCity – The continuous integration software that I have been using to automate my deployments. MSBuild – This is used by Visual Studio to build your software and can also be used by your deployment scripts. Trello – A website that allows you to create a board of ideas or things to do. Slack – Brings all your communication together in one place. It’s real-time messaging, archiving and search for modern teams. New Relic – A software analytics tool suite used by developers, ops, and software companies to understand how your applications are performing. Useful but find myself favouring Application Insights (part of Azure) more now. Nagios – Yay nagios is on the list! My favourite server monitoring system. Splunk – This application can be used to search, monitor and analyse all your log files to find out what is happening. Don’t currently use it but I have tried it out in the past. What is your favourite DevOps tool? Why not leave a comment below?\n","date":"Jun 30, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/06/https-xebialabs.com-assets-files-infographics-periodic-table-of-devops-v2.png?resize=1024%2C572\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2016/periodic-table-devops/","series":null,"tags":["Chemistry","DevOps","Programming","Tools"],"title":"Periodic Table of DevOps"},{"categories":null,"content":"One of the features of git is the ability to tag a point in my change history with a tag. For a while now I have been manually tagging my code whenever I do a release, so I can easily work out what has changed by doing a diff between two tags.\nNow that I am automating my release process with TeamCity I am thinking about how to manage my tags better.\nTeamCity has a setting called VCS Labeling which comes in very handy. Configuring it is fairly simple as it only has three settings.\nVCS root to label: This is obviously the url to your git repository Labeling pattern: This is the text of the label to be added. Label successful builds only: Do you really want to add a tag if the build failed?\nA tag needs to have a unique name, so adding a tag just called deployed won’t work. When I used to add tags manually I used the naming convention of deployedyyyymmdd. While this naming convention is possible with TeamCity I use something a bit more complex to provide more information about what has been deployed.\nTeamCity provides lots of parameters that can be used in your build steps and also in the Labeling pattern box. I started off using deployed-build-%system.build.number% as my tag which just marks git with the TeamCity build number.\nWhen I run a TeamCity deployment I don’t always use the same configuration options, I deploy locally, to a test server or to production and sometimes I just deploy the frontend or the backend. How cool would it be to include this information in the tag text?\nWell the next step was to change my Labeling pattern to deployed-build-%system.build.number%-%ServerName%-%DatabaseName%-%FrontEndPath%, this adds the backend database config settings and the path the frontend was deployed to. Now when looking at git you can see commits marked with multiple tags, one for each deployment that succeeded and the tag will indicate the settings used during that deployment.\nNow I will never forget to add the tag after a release as the adding of a tag is part of the deployment process, if the deployment fails the tag won’t be added. I can test my deployment to test and git will show if this has been successful, and when I deploy live this will also show up.\nHow do you use tags? Do you mark successful builds with a tag? Why not let me know or leave a comment below.\n","date":"Jun 16, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/automatic-git-tagging/","series":null,"tags":["DevOps","Programming","Source Control","Git","TeamCity"],"title":"Automatic Git Tagging"},{"categories":null,"content":"That’s right this is the one hundredth post that I have written on this blog.\nSo what have I learned in the past 100 posts?\nIt is easier to write a blog before you become a parent. More recently I am increasingly finding it difficult to find the time to blog. It used to be that I could write on an evening, but now James is around I often prefer to play with him, or more often stop him crawling where he shouldn’t. I like my job. The inspiration for most of this site is my day job and as you can read, I do a wide variety of different things, but I have a lot more to learn as well. Finding your niche is hard. 100 posts in and I am still not sure what my niche is. I started out with the broad niche of IT and what I do, I then considered something about IT and fatherhood but I don’t think that topic is really me. My current thinking is maybe DevOps especially as my role these days fits squarely between Development and Operations. Its time for a refresh. I have been meaning to change the theme of this site for some time and I feel after 100 posts now is as good as time as any. I want to emphasise my skills and what I am learning and increase the emphasis on DevOps, which I think will be my niche. Watching visitor numbers is addictive. Every day I look at how many people have looked at my blog, but I have yet to see a pattern between what I write and how many reads I get. Is my writing getting better? I don’t know. Are more people reading? Probably not. Will I keep going? Yes So what is next?\nHopefully a refreshed look in the next few months. Hopefully regular posts. If there is something you want to see on here drop me a message via any of the social media links or put a comment below.\nWhat is my favourite post?\nMaybe User Groups and F# which proved very popular and got me to start going to user groups something I still enjoy today. I also like Coding Myself Into A Corner which got me to start thinking more if I was giving myself future problems. But there are many others I like such as James and Becoming a father ","date":"Jun 2, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/i-m-100-blog-posts-old/","series":null,"tags":["100","Blogging"],"title":"I’m 100 blog posts old"},{"categories":null,"content":"My latest podcast can be found http://www.trekmate.org.uk/brothers-tng-s4-e3-review-the-battle-bridge/ and feedback can be left on the trekmate forums http://forum.trekmatefamily.com/2016/05/brothers-tng-s4-e3-review-the-battle-bridge/ Data jeopardizes an emergency mission to save an ill child when he gets a signal from his creator.\nToday James and Lou discuss (Brothers) and what we think of it, with tonight’s guest host Simon Foster, Funky Si.\nCredit: Main Title Theme (TV Edit) was arranged by Dennis McCarthy and composed by Jerry Goldsmith and Alexander Courage ","date":"May 26, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/05/Noonian_Soong_2367.jpg","permalink":"https://www.funkysi1701.com/posts/2016/brothers-tng-s4-e3-review-battle-bridge/","series":null,"tags":["StarTrek","Trekmate","Podcast"],"title":"Brothers, TNG S4 E3 Review, The Battle Bridge"},{"categories":null,"content":"Last year you may remember me talking about playing with a Raspberry Pi. Well since then my Raspberry Pi has been sat on a desk collecting dust.\nThis week I attended Leeds Sharp and the topic was Running Windows on Raspberry Pi and this has inspired me again to do something with a Pi.\nBut first what did I learn.\nHere are a couple more of pics of last nights @LeedsSharp https://t.co/QvlsYjNFvB #RaspberryPi #MSIoT pic.twitter.com/60o4wiiPSv\n\u0026mdash; Richard Tasker 🇬🇧 (@ritasker) April 29, 2016 I had heard that a cut down version of Windows 10 could be installed on the newer Raspberry Pi’s, but I hadn’t really understood how cut down the version of windows is. Having now seen it demonstrated the OS consists of a single page with a few menu options.\nThe real power of Windows 10 IoT is when you connect remotely to it. There are a couple of ways to do this, PowerShell (check out https://ms-iot.github.io/content/en-US/win10/tools/CommandLineUtils.htm for a few commands), and of course connecting Visual Studio to your Pi.\nWhen I had previously played with a Pi, it had been with bash scripts and linux commands. The beauty of installing Windows IoT is that you can write c# code, something I do in my day job so theoretically I should find it easier.\nThe demonstration at Leeds Sharp was pretty impressive. If you are a fan of the Big Bang Theory you may recall Sheldon playing a Theremin. Well it is actually possible to construct a Theremin from a couple of sensors and a Raspberry Pi. The code for which is on github .\nNow that I have been inspired what shall I do?\nMy Raspberry Pi won’t support Windows 10 IoT, so I need to buy the latest version. I am thinking of buying a kit so I can play about with a breadboard, LEDs and resistors. Maybe not build a robot straight away but certainly try doing something that connects to the GPIO pins.\nIf you have any suggestions leave a comment below.\n","date":"May 5, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/windows-10-raspberry-pi-3/","series":null,"tags":["C-Sharp","Programming","RaspberryPi","Visual Studio"],"title":"Running Windows on Raspberry Pi"},{"categories":null,"content":"Where I work we use a really old fashioned way of recording where in the country employees are: Excel!\nFor years I have been trying to persuade staff to use calendars in Exchange. Outlook is great for looking at one or two people’s calendars at once but quickly gets unmanageable for looking at ten or more peoples availability.\nRecently I have started looking into how easy it is to query this information to give a custom view.\nMicrosoft provide an API to query exchange information called Exchange Web Services or EWS. I have only used EWS with my Exchange 2010 setup, but the documentation mentions working with Exchange 2007 and older or Exchange online.\nHere are the basics of what I have tried. Fire up Visual Studio and from nuget install EWS.\nInstall-Package Microsoft.Exchange.WebServices I started off with a simple Console App to see how it all worked, and then extended it to a MVC website. I found querying exchange directly was slow, but it is easy enough to cache information in a database.\nTo start you need to create an Exchange Service object, by specifying the version of Exchange you are using.\nExchangeService service = new ExchangeService(ExchangeVersion.Exchange2010); Next you need to pass the URL you are using to access Exchange.\nservice.Url = new Uri(\u0026#34;mail.example.com\u0026#34;); To access information from exchange you need to pass some Exchange credentials, ideally a username and password that has access to view all the calendars you want to look at.\nservice.Credentials = new WebCredentials(\u0026#34;username\u0026#34;,\u0026#34;password\u0026#34;); Next pass the email address of the user who owns the calendar you want to look at.\nservice.ImpersonatedUserId = new ImpersonatedUserId(ConnectingIdType.SmtpAddress, \u0026#34;me@example.com\u0026#34;); My particular exchange server has a self signed SSL certificate which is not going to be trusted by remote clients. The following line ignores this validation check and makes my program connect.\nSystem.Net.ServicePointManager.ServerCertificateValidationCallback = (sender, certificate, chain, sslPolicyErrors) =\u0026gt; true; Now that we have connected to exchange we just need a few lines to look for events in the calendars.\n// Initialize the calendar folder object with only the folder ID. CalendarFolder calendar = CalendarFolder.Bind(service, WellKnownFolderName.Calendar, new PropertySet()); // Set the start and end time and number of appointments to retrieve. CalendarView cView = new CalendarView(startDate, endDate); // Limit the properties returned to the appointment\u0026#39;s subject, start time, and end time. cView.PropertySet = new PropertySet(AppointmentSchema.Subject, AppointmentSchema.Start, AppointmentSchema.End); // Retrieve a collection of appointments by using the calendar view. FindItemsResults\u0026lt;Appointment\u0026gt; appointments = calendar.FindAppointments(cView); Now that you have an appointments object you can loop through each element with a foreach loop. In my case I assign each elements Subject to a variable, which I can then do what I like with (display in a console window, save to a database, display in an MVC website.)\nforeach (Appointment a in appointments) { if (a.Subject != null) { subject += a.Subject.ToString(); } } My website queries a SQL database which I can easily populate with a console app that runs at regular intervals throughout the day.\nThere is a lot more I want to do with this project as this is only the basics of what you can do with Exchange Web Services. So expect more blog posts on this subject as I expand its functionality and learn new ways of doing things.\n","date":"Apr 21, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/exchange-web-services/","series":null,"tags":["EWS","MVC","Exchange","Programming"],"title":"Exchange Web Services"},{"categories":null,"content":"Last year I blogged about Team City , well I have been looking at it again recently. In that time they have even changed their logo!\nLets start with thinking about what I want my Continuous Integration server to do.\nCheck out my code from source control (usually master but all feature branches would be even better) Configure specific setting for build Build my code Build my databases Run any unit tests (Optional) Run deployment to Azure Test/Live site There are probably other things I want to achieve but I will start with these six.\nChecking out code from source control is something Team City does out of the box, so I can safely say I have done this now. It even monitors a branch for changes and initiates a new check out. Team City allows you to create specific build steps so in theory you can have multiple builds for every variation of settings that you want for your code. I have not tried this yet apart from building with the default config, but I don’t expect it will be too difficult. I have managed to get my code to build with Team City, it took a bit of tweaking the different build steps but wasn’t too difficult. Team City has a visual studio build agent which takes you solution file and does what it needs to. The one problem I have found with this step is that I get errors with my tests if I select a Debug config instead of Release. Databases are always the problem part of the deployment. So far I have manually deployed my databases but I intend on revisit this step. A stackoverflow post suggests that I can run SQL code via Team City in the following way by creating a command line executable: Command executable: c:\\Program Files\\Microsoft SQL Server\\100\\Tools\\Binn\\sqlcmd.exe Command parameters: -S [ServerName] -i [PathToSQLScript] I have yet to try this but I am hopefully that it will just work. Dropping a database and restoring a back and then running different SQL scripts is all possible from TSQL, so I should be OK. Watch this space for more details.\nRunning the unit tests got me stuck for a while. I tried setting it up using VSTest or MSTest neither worked mainly because a config file wasn’t being copied with the test binaries. When I tried using NUnit it just worked. The tests that failed gave me a few config settings to change. I have powershell scripts that deploy to Azure websites, I think that these could form the basis of a deployment to Azure. Again the difficult step here may end up being deploying all the different databases to Azure. This is also the riskiest step as I need to connect to live servers which is why I will leave this to last, at the very least I could generate scripts that do a full deployment. That’s it for now. Once I have this all working I will revisit again with details of the database steps as I am expecting a few challenges to overcome. What have you used a CI Server for? Are there other things I want to achieve from a project like this? Why not contact me or leave a comment below\n","date":"Mar 24, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/revisiting-teamcity/","series":null,"tags":["CI","Programming","DevOps","Source Control"],"title":"Revisiting Team City"},{"categories":null,"content":" Model View Controller or MVC is a software architectural pattern for implementing user interfaces on computers. It divides a given software application into three interconnected parts, so as to separate internal representations of information from the ways that information is presented to or accepted from the user.\nI have been trying to get my head around the concept of MVC for a while, hopefully writing this article will help solidify my understanding of it.\nOne of the core concepts of MVC is the ability to separate concerns so you can concentrate your energies on one aspect of the application.\nModel This is the data. If your application uses a database the model often mirrors what you have in the database and concerns itself retrieving information from the database.\nView This concerns itself with displaying the data to the user. Typically this is the html pages of your application.\nController This concerns itself with actually doing things and deals with user interaction. Typically it will get data from the view and send data to the model.\nThe three concerns can be developed in isolation as they do not depend on each other, for larger development teams you can even divide up development much easier that a traditional app.\nThe basic concept of MVC I get and understand, however I find myself getting bogged down in the details.\nThe database doesn’t matter. I need to remember this and not get sidetracked in writing custom methods to connect to the database which end up unmanageable. I know SQL, so can easily write SQL commands to copy data into the format I need for my app. The app I am currently working on involves a large amount of existing data, and I need to concentrate on the MVC parts and worry about the database later.\nIn previous attempts I have tried to build my model against many tables, but instead I can write a query against many tables and insert that into one table which the Model can then use.\nChanging the model often results in an error informing you that the context has changed since the database was created. The easy solution to this in my case is to drop the database and allow entity framework to recreate the database each time. As long as my database contains no new data, I won’t loose anything.\nOne of the core advantages of MVC is the ability to test it or even use test driven development (TDD). I haven’t really dabbled with testing yet as I am still trying to get my head around the fundamentals, but once I have made some progress with my app I want to test, so next time I get asked to add a new feature I have no fear about breaking stuff.\nFor the first time I have got an app with a working Authentication system from the start. And it is remarkably easy to implement with one keyword. Adding [Authorize] to the top of your controller is all that is needed. Building the actual Authentication system is relatively easy from Visual Studio, as it has templates for Azure AD, Forms Based, Open Auth like google/twitter etc.\nIt is still very early days for my MVC app and my understanding of it, but I feel I have turned the corner and can actually build something with it now, rather than be stuck in a downward spiral of confusion.\nWhat do you think about MVC why not leave a comment below? For more info about MVC I have been looking at http://www.asp.net/mvc which has more information and tutorials.\n","date":"Mar 17, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/model-view-controller-mvc/","series":null,"tags":["Programming","MVC"],"title":"Model View Controller (MVC)"},{"categories":null,"content":" I have just bought myself a new laptop, but it is not just any laptop it is a Microsoft Surface Book .\nI think this is probably the first time I have bought myself a top of the range laptop and after a few days of use I am loving it.\nThe Surface Book is the latest in Microsoft’s Surface line of tablets but the first to feature a keyboard and be more like a traditional laptop. The keyboard is detachable from the keyboard so you can use the Surface Book like a tablet.\nAs it is a top of the range laptop, it was not cheap, so if price is a significant factor in your laptop choice this is not the machine for you. Microsoft are trying to compete with Apple’s Macbook range.\nSo what did I get for my money:\nWindows 10 Pro 16Gb RAM 512Gb SSD (Formatted Size nearer 474Gb) 6th Generation Intel Core i7 Processor Surface Pen 13.5-inch touch screen I have only had the device for a few days but these are my thoughts so far. My primary reason for buying this is for doing development work with Visual Studio and SQL Server.\nI really like the Surface Pen. This is a feature that I didn’t think I would use much. The Pen connects via bluetooth and allows interaction with the touch screen. The pressure sensitive screen allows all sort of touch actions to be performed.\nI especially like the choice of keyboards that are offered when the physical keyboard is detached. You can have either a on screen keyboard which you can type with pen or fingers, an on screen keyboard that splits in half or you can write with the pen. This is an amazing feature. My handwriting is not good but most of the time it understands my scrawl. With this option you can hand write a tweet or fill in a form on a webpage. For speed I would not recommend this form of input for large amounts of text, but for browsing the web of when you are just clicking on links it is great.\nWindows Hello – I can unlock windows just by looking at my screen. How cool is that! It was really easy to setup, it just takes a photo of your face and next time you login all you need to do it look at the screen. Note if you are working in tablet mode, make sure the tablet is the correct way up.\nIt’s not all brilliant though. Detaching the screen is fiddly and sometimes takes a few moments to do. A few times I have felt I needed another hand but I am sure the more I do this the easier I will find it to do.\nBattery life isn’t great especially when running off the tablet only. This is due to the machine having two batteries, one in the base and one in the screen so with keyboard attached you have much longer use times. Actually I am finding the battery is lasting longer now that I have used it for a few days.\nThe screen has a very high resolution 3000 x 2000 but using clever zooming technology everything is still readable and not tiny. However I use Remote Desktop a lot and this caused me a problem. When RDPing the remote session used the host screen resolution which made everything tiny on my servers. The solution to this can be found on SuperUser and involved using Microsoft’s Remote Desktop Connection Manager, a cool bit of software for managing multiple RDP sessions. This is actually an improvement on the way I usually work, but until I found the answer this was annoying.\nAnother minor annoyance with the keyboard is that you can’t press Ctrl-Alt-Del with one hand. Normally this isn’t a requirement but if the screen locks while I have my son on my lap I am stuck, but I won’t blame the surface book for this.\nWhen I got my Surface Book I was also given a Microsoft Wireless Display Adapter. I am not a fan of this bit of tech as I can’t get it to work. I briefly got my old laptop to connect but my Surface Book keeps telling me NO!\nOverall I like the Surface Book. It is certainly the nicest laptop I have ever used. Now that I have the docking station I can connect two monitors, have a wired internet connection and it becomes a proper work horse. What does everyone else think? Better than Apple’s range of laptops?\n","date":"Mar 10, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/02/en-INTL-PDP0-Surface-Book-CR9-00001-P2.jpg?w=780\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2016/surface-book-review/","series":null,"tags":["Laptop","Hardware","Technology","Microsoft","Windows"],"title":"Surface Book Review"},{"categories":null,"content":"One of my plans is to create new MVC Webapps for my companies databases. Once I publish these I will need to secure them so only staff have access.\nThe traditional way to do this would be insert membership tables into my database. The user then has to remember another username and password and I have to secure the storage of these credentials. Lots of work for everyone.\nThere is a better way by using Azure Active Directory. You have probably heard of Active Directory, if you are a SysAdmin you probably use it all the time to manage your corporate users and computers. Azure Active Directory is an extension of this into the Cloud.\nI have blogged in the past about using Azure but this is the first time I have tried connecting my internal domain to Azure. There is a Virtual Lab which helped me try out some of these ideas.\nThe first thing I did was to create a new Directory in Azure. I did this via the old portal (manage.windowsazure.com ) it might be possible via the new portal but I don’t know how yet.\nClick New, select App Services \u0026gt; Active Directory \u0026gt; Directory and select Custom Create. Select Create new directory, give it a name and a domain name and select a region from the drop down. Then add a Global Admin for this directory.\nThere is a tool called Azure Active Directory Connect . Download and Install this with express settings on one of your domain controllers. You need to specify a domain admin account to access your domain and the Azure Global Admin account you just created.\nAt this point I went to bed so I am not sure how long it tool to sync all the domain information but by morning it was all showing in the users list on Azure.\nAll my user accounts are showing with a @contoso.onmicrosoft.com, it is possible to use custom domains but I haven’t figured out that step yet. I made a change in my Active Directory and a while later that change was showing in Azure AD.\nSo now what? Open up Visual Studio and see if I can use Azure to Authenticate.\nI selected to create a new MVC web project and clicked the change authenticate option. One of the options was Work and School Accounts, I then selected Cloud Single Organization and entered contoso.onmicrosoft.com. I then ran this app and it authenticated using Azure using my domain password. Really impressed at how easy that was.\nThe app then shows up on Azure in the old portal. In Applications you can see a list of which users have access to your app and configure few other app related settings.\nThis is a long way off being useful in my actual app, but it shows that the basics of what I am trying to do does work. Anyone done anything similar with Azure AD? How did you get on?\n","date":"Mar 3, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/03/arch.png?w=600\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2016/trying-out-azure-active-directory/","series":null,"tags":["Active Directory","Azure","Authentication","Azure Active Directory","DevOps","Programming"],"title":"Trying Out Azure Active Directory"},{"categories":null,"content":"I spend an awful lot of my working life adding new features and improvements to a legacy database system.\nA legacy system is an old method, technology, computer system, or application program, “of, relating to, or being a previous or outdated computer system.” Often a pejorative term, referencing a system as \u0026ldquo;legacy\u0026rdquo; often implies that the system is out of date or in need of replacement.\nMy particular legacy system is Microsoft Access. We use Microsoft Access to provide the front end for all our internal databases. The particular technology we use is ADP files which are only supported in Access 2000, 2003 or 2010.\nMicrosoft has already dropped support for Access 2000 and 2003, and support for 2010 is due to be dropped in 2020.\nSo why am I coding myself into a corner? Its simple for every feature or improvement I create in Access, I increase the amount of features I need to create or port to a new front end.\nThis is a very depressing thought, everything I create will need creating again. I am not saving myself work but increasing the amount of work I need to do again.\nThe bad news is that my employers like almost all businesses demand results and like a good employee I have been delivering them. I have been promised the mythical “when we are quiet” you can work on rebuilding the database front end. I know this will likely never happen so what are my options.\nDo nothing I have warned my managers that this work needs doing and it is up to them to give me the resources I need. This is not an acceptable option. Firstly I am not future proofing the business, 2020 will be here before we know it. Also I am not developing myself as a developer, as the only experience I am getting is with Legacy technology that has expired or will do very soon. No one wants to employ someone who only has legacy experience.\nDo something I need to keep delivering results and doing what is asked of me. Like any good engineer I should be multiplying my estimates by four, meaning that I have some time that can be used for looking at the bigger picture. This is a win-win option really. Employer gets a solution that is future proof, Employee gets valuable experience in up to date technology.\nWhat do you think? Have you been tied to legacy technology? Why not leave a comment below.\n","date":"Feb 25, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/coding-myself-into-a-corner/","series":null,"tags":["Programming","Databse","Access"],"title":"Coding myself into a corner"},{"categories":null,"content":" Episode Title: The Best of Both Worlds Star Trek Type: TNG Original Air Date: 18 June 1990 \u0026amp; 24 September 1990 Written By: Michael Piller Directed By: Cliff Bole The Borg threaten the Federation, when they kidnap Captain Picard and destroy a fleet of 39 Starships. Luckily some original thinking from Commander Riker and Lieutenant Commander Shelby saves the Federation from assimilation.\nThoughts: I love this episode! It is probably my favourite episode in the whole of trek. I can’t think of a single scene which I don’t like.\nIt is a two parter that actually works best as a two parter. The moment that Riker orders the Enterprise to fire on the Borg is great. At the time some thought that Patrick Stewart might leave the show and be replaced by Riker/Shelby. I am very glad that didn’t happen but it is interesting to ponder what that might be like.\nThe Borg soundtrack in this episode is great. Ron Jones really captures the spooky threat that the Borg are. I was always disappointed that some of that didn’t continue in First Contact and when the Borg return in Voyager.\nThe episode has a cracking pace. From the moment the Borg first attack the Enterprise to the moment the Borg ship explodes the show doesn’t slow down.\nI am Locutus… of Borg. Resistance… is futile. Your life, as it has been… is over. From this time forward… you will service… us. Commander Shelby is a great character that enhances the episode and introduces conflict between characters, particularly Riker.\nI have heard that Riker’s thoughts about his career was a mirror to writer Michael Piller’s thoughts about leaving the show. Luckily for us he stayed.\nIn a future episode of DS9 Worf comments that he felt there was nothing that they couldn’t do during this time. I have to agree the tag team of him and Data is a great one. If I want rescuing it would be Data and Worf that I would want.\nMy good friends at Trekmate have just reviewed part one, why not have a listen?\n","date":"Feb 18, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/02/Picard_kidnapped_by_the_Borg.jpg","permalink":"https://www.funkysi1701.com/posts/2016/star-trek-episode-review-the-best-of-both-worlds/","series":null,"tags":["StarTrek"],"title":"Star Trek Episode Review The Best of Both Worlds"},{"categories":null,"content":"Today I did a clone of one of my git repositories and it took ages to download. Looking into what got downloaded it was easy to see why. The .git folder was over 500Mb in size.\nI know how this has happened. This repository was created in 2013 and has been used by me as a dumping ground for lots of things related to the code which never should have been committed.\nSince 2013 I have learnt a lot more about coding and git so the current version of the files in git isn’t too bad. But git keeps the history of changes for every file so bad practices like this are kept.\nWhat can I do about this? Well what does google suggest? I found this blog post .\nIt suggests ways of listing all the large files that are stored in git and a way to remove them. As I am the only person that regularly commits to this repository I see no problem with giving it a go.\nI will summarise the steps here.\ngit clone url Now you need all the remote branches so there is a bash script to run\n#!/bin/bash for branch in `git branch -a | grep remotes | grep -v HEAD | grep -v master`; do git branch –track ${branch##*/} $branch done Another bash script then lists the top 10 large files\n#!/bin/bash #set -x # Shows you the largest objects in your repo’s pack file. # Written for osx. # # @see http://stubbisms.wordpress.com/2009/07/10/git-script-to-show-largest-pack-objects-and-trim-your-waist-line/ # @author Antony Stubbs # set the internal field spereator to line break, so that we can iterate easily over the verify-pack output IFS=$’\\n’; # list all objects including their size, sort by size, take top 10 objects=`git verify-pack -v .git/objects/pack/pack-*.idx | grep -v chain | sort -k3nr | head` echo “All sizes are in kB. The pack column is the size of the object, compressed, inside the pack file.” output=“size,pack,SHA,location” for y in $objects do # extract the size in bytes size=$((`echo $y | cut -f 5 -d ‘ ‘`/1024)) # extract the compressed size in bytes compressedSize=$((`echo $y | cut -f 6 -d ‘ ‘`/1024)) # extract the SHA sha=`echo $y | cut -f 1 -d ‘ ‘` # find the objects location in the repository tree other=`git rev-list –all –objects | grep $sha` #lineBreak=`echo -e “\\n”` output=“${output}\\n${size},${compressedSize},${other}” done echo -e $output After running the script you will see details of your largest files. I had *.msi and *.exe files in the mix. To remove them run the following, where filename is the path to the file that needs removing.\ngit filter-branch –tag-name-filter cat –index-filter ‘git rm -r –cached –ignore-unmatch filename’ –prune-empty -f — –all To reclaim the disk space run the following commands\nrm -rf .git/refs/original/ git reflog expire –expire=now –all git gc –prune=now git gc –aggressive –prune=now Now push your changes back to the remote server.\ngit push origin –force –all git push origin –force –tags Now if you do a clone it will be much smaller than before and you can get back to coding much quicker, without having to wait.\n","date":"Feb 11, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/my-git-repository-is-too-large/","series":null,"tags":["Git","Source Control","Programming"],"title":"My git repository is too large!"},{"categories":null,"content":"Like many DBAs I spend a lot of time maintaining my SQL Server backups.\nFrom SQL Server I maintain both full backups and transaction log backups. I have often restored my full backups but until recently I have never restored a transaction log backup. All backup strategy’s are only as good as the last time you tested the restore process.\nSo what is a transaction log backup? A transaction log backup contains all the transaction log records generated since the last full backup and is used to allow the database to be recovered to a specific point in time (usually the time right before a disaster strikes). Since these are incremental, if you want to restore the database to a particular point in time, you need to have all the transaction log records necessary to replay database changes up to that point in time.\nHow to do the restore. First right click on Databases in SQL Management Studio and select restore database. You should then get a screen similar to this.\nIn source click the \u0026hellip; to allow you to select your backup files.\nNow normally I have only ever selected one file here, the *.bak file. Instead select the *.bak and all the *.trn files as well. After SQL Server has chugged for a few minutes (time will depend on number of transaction files and server/disk speed etc) the restore plan section should fill up with files.\nIn the destination database box, type in the name of the database you want to restore. I recommend using a different name to avoid overwriting the original database, appending Test or a datetime to the name is what I usually do.\nOn my test server I need to untick the take tail-log backups option off the options screen before I can execute the restore.\nNow you can either check the tick boxes in the restore plan section or (more fun) click the timeline button to select at what point in time you want to restore to.\nYou can either select the point in time with your mouse or specify the exact point in the time textbox. Alternatively you can just select the most recent point, probably the most likely option when disaster strikes.\nNow that I have tried doing this on my test server I feel much more confident that when disaster does strike I can get things restored quickly and painlessly.\nHow often should you run transaction backups? The answer to this question depends on how critical your data is. Until very recently I ran mine ever 15 minutes, I have increased this to every 5 minutes, but I have seen recommendations of running it every minute . The more critical your data the more often you should run them.\n","date":"Feb 4, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/transaction-log-backups/","series":null,"tags":["Database","SQL","Programming","Backup"],"title":"SQL Transaction Log Backups"},{"categories":null,"content":" I spend a lot of my time creating new features that simplify my companies business processes.\nA good example of this is an invoicing system I created. Instead of users working off different spreadsheets and copying and pasting data between various different programs the user can click a couple of buttons and everything is done and they can move on to the next task.\nI am currently expanding this system so more of the companies work can be invoiced quickly and simply.\nI like working on this kind of problem. I get to discuss with the different departments involved, finding out how they work and what could make their lives better.\nI then get to look at what structures already exist in the database and get to rip out anything that makes no sense (Today I had a Customer table which has a primary key called ClientId – its now a Client table with PK ClientId) and add new structures to store new information.\nI then can build a user interface so the users can interact with the data.\nBut the most important part of the process comes next. This is when I show the users what I have built and how it will make their lives better, they can then feed back to me questions and comments. How do I do x?, what about y?, have you thought about z?\nAfter this very valuable feedback I can go back and tweak what I have done making it better and better until the users think it is suitable for what they need.\nNow why did I call this post development annoyance? I find it intensely annoying that after spending weeks of my time working on creating something to be used. Users either make do with my solution and never tell me the one thing that really annoys them or worse they go back to the time consuming process and never feedback the one or two changes I need to make to complete the project.\nNow I am not perfect, like many IT people my people skills are somewhat lacking but apart from asking people what can you do to get the feedback you need to complete the project?\nI do not have the knowledge that other departments have so I consider development a team effort that requires everyone to contribute otherwise it will not finish.\nThe second outcome where users return to their old ways is the most annoying. It could well be months or even years before I find out that what I created is not being used any more. By that time my memory of what I did and more importantly why I did what I did has been lost and forgotten.\nSo users please, please tell me what works and what doesn’t and what things I can do to improve things. I want my solutions to constantly improve and get better and I can only do that with your help.\n","date":"Jan 29, 2016","img":"","permalink":"https://www.funkysi1701.com/posts/2016/development-annoyance/","series":null,"tags":["Agile","SQL","Programming","Project"],"title":"Development Annoyance"},{"categories":null,"content":" Episode Title: Arena Star Trek Type: TOS Original Air Date: 19 January 1967 Teleplay: Gene L Coon Story By: Frederic Brown Directed By: Joseph Pevney The episode begins with the Enterprise in orbit of Cestus III. Commodore Travers requests a tactical team join the away team. When the away team beam down Cestus III has been destroyed.\nSpock detects non-human life signs, obviously the aliens that destroyed the outpost. They attack the Enterprise, stranding the away team who are under heavy fire. Sulu is instructed to do whatever he can to protect the Enterprise and leaves orbit.\nKirk finds the armoury and a grenade launcher, after firing the enemy withdraws.\nAfter beaming back aboard the Enterprise they pursue the unknown enemy into uncharted space. One of the survivors of Cestus III explains that the enemy attacked without warning.\nKirk is determined to destroy the enemy ship before it reaches home and the Enterprise prepares for battle.\nAn unknown solar system scans the Enterprise as they continue their pursuit of the enemy ship. The enemy ship comes to a complete stop. The Enterprise prepares to attack but before they can they also come to a complete stop. The Metrons have stopped both ships because both ships were on a mission of violence. The captains of both ships will be placed on a planet where the winner can destroy the other.\nCaptain Kirk and the enemy captain appear on a desert planet. The enemy is a reptilian like species called the Gorn. The Gorn has superior strength to humans but Kirk manages to escape his first encounter.\n\u0026#34;This is Captain James Kirk of the Starship Enterprise. Whoever finds this, please get it to Starfleet Command. I’m engaged in personal combat with a creature apparently called a Gorn. He’s immensely strong. Already, he has withstood attacks from me that would have killed a Human being. Fortunately, though strong, he is not agile. The agility and I hope the cleverness, is mine.\u0026#34; The Metrons said that the planet contained materials to allow the building of a weapon, however at first glance Kirk sees nothing that could help him defeat the much strong Gorn captain. He does find some diamonds, too small to be used as a weapon.\nKirk pushes a huge boulder over a cliff onto the Gorn captain, which appears to knock him out, however he survives and Kirk barely escapes. Kirk then finds Sulphur, which triggers a memory.\nThe Metrons allow the Enterprise to watch the last stages of Kirks struggle. They see Kirk discover Potassium Nitrate. The Gorn accuses the Federation of trespassing into their space.\nKirk starts constructing a weapon using the diamonds, sulphur and potassium nitrate to make a gunpowder explosion to fire the diamonds at the Gorn captain. Just in time he applies a spark to his weapon and his opponent lies defeated.\nKirk refused to kill the Gorn captain as they may have been just defending themselves. The Metron announces that Kirk has displayed the advanced trait of mercy and there may be hope for humanity after all.\nThoughts: I like this episode very much. It has some classic Star Trek ideas in it.\nWe have a super powerful alien race (the Metrons) that think there is hope for humanity.\nWe have Kirk using raw materials to construct a weapon using simple chemistry.\nWe have an enemy race that appears destructive but may have just been defending themselves.\nWe have the classic location of Vasquez Rocks, somewhere that if I ever visit the US I have to visit.\nWe have the very cheesy Gorn mask, yes its not realistic but the story is strong enough that it doesn’t really matter. Enterprise brought back the Gorn but as a CGI character and I think I may prefer the rubber mask.\nInterestingly the weapon Kirk builds would likely not work. The US show MythBusters proved that the bamboo wasn’t strong enough to hold the explosion and the weapon would likely wound Kirk as much as it would wound the Gorn.\nScore: 10/10 Redshirt Count: Cestus III Outpost, 1 Red Shirt and 1 yellow shirt from away team.\n","date":"Jan 21, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/01/Gorn.jpg?w=662\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2016/star-trek-episode-review-arena/","series":null,"tags":["StarTrek","Gorn","Arena"],"title":"Star Trek Episode Review Arena"},{"categories":null,"content":" Over the Christmas break the city of York experienced the worst flooding disaster it has seen in recent years. If you want to help donations are being taken at York Flood Appeal .\nLuckily I wasn’t affected but it was really sad seeing many of my favourite streets full of water. Thousands of people had to be evacuated from their homes when the water was at its highest.\nYork has its phone exchange located in the centre of the city and as the flood waters rose, this building flooded. BT who run the exchange worked flat out trying to restore phone and broadband services to York and the surrounding area, but this lead to at least 24 hours or more of downtime for users.\nDuring this time most of the city wasn’t able to take card payments from customers (due to an internet connection being required), cash machines were not working for the same reasons and the most worrying thing access to emergency services was not available.\nAnyone would think the city phones were down #yorkfloods #bthardatwork pic.twitter.com/ZavLMsG2Js\n\u0026mdash; Simon Foster (@funkysi1701) December 28, 2015 I am in no way critical of the hard working engineers that worked on the BT exchange to restore services to the city. However I have to ask what was contained in the BT disaster recovery plan.\nAs an IT Manager myself backups and disaster planning is something I need to think about and plan for. Recent investment in our internet connection meant that we are now on a leased line so BTs downtime would not have affected us in terms of network connectivity, I am uncertain about phone but I believe as a business we could have continued running during this time if we needed to.\nPlanning for a disaster is not something I ever make enough time, however I definitely want to spend more time on it and I do have the basics of a plan in my head which is probably more than BT has.\nBT is a huge company they run almost the entire country’s phone lines and according to wikipedia have operations in 170 countries. I think that they can afford to have someone plan for disaster recovery.\nThe York Exchange is located next to a river that often floods. What could the company have done to avoid this problem?\nRelocate some or all of its services to a location away from the river. Add the ability to reroute services through an alternative exchange. Many towns and villages surrounding York use the BT exchange surely some of these could have been configured to use other exchanges. In these days of terror alerts and bomb scares. Imagine if York was London and all it took to take down communication ability of the capital was one Exchange to be offline?\nI am sure BT are looking at what they did right and what they did wrong and if you are reading this have a look at your own disaster recovery policy. Don’t get caught out like BT did.\n","date":"Jan 14, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/01/flood.jpg?resize=1024%2C1024\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2016/disaster-planning/","series":null,"tags":["Disaster Recovery","BT","Outage","Flood","Phone","Internet"],"title":"Disaster Planning"},{"categories":null,"content":" In 1966 a TV show called Star Trek first aired in the USA. Since then 726 episodes have been made and 12 films have been released. 2016 marks the 50th anniversary and Star Trek is as alive as ever with a new film due out this summer and a brand new TV series in the works for broadcast in 2017.\nBut what can I do to celebrate? With that many episodes I don’t have enough time to re-watch every episode so how about a re-watch of the top 50 episodes.\nStar Trek has five live action TV series so all I need to do is pick my top 10 from each.\nStar Trek The original Star Trek series first aired on September 8th 1966. It lasted only three years and 79 episodes. This series featured the iconic characters Kirk, Spock and Bones on a five-year mission to explore space.\nThe City on the Edge of Forever The Devil in the Dark Balance of Terror The Trouble with Tribbles Arena This Side of Paradise Amok Time Space Seed Mirror Mirror The Deadly Years Star Trek: The Next Generation The first spin-off series set 100 years after Kirk featured a new Enterprise continuing to explore space. This series aired between 1987 and 1994 for 178 episodes.\nBest of Both Worlds Yesterdays Enterprise The Offspring The Measure of a Man Disaster All Good Things… Tapestry The Inner Light Cause and Effect Unification Star Trek: Deep Space Nine The second spin-off series which ran at the same time as TNG was set on a space station so instead of exploring adventures arrived via a stable wormhole. This series aired between 1993 and 1999 for 176 episodes.\nEmissary Duet The Way of the Warrior The Visitor Sacrifice of Angels Waltz Trials and Tribble-ations In the Pale Moonlight Far Beyond the Stars Siege of AR 558 Star Trek: Voyager The third spin-off series ran after TNG and featured a female captain and a starship stranded in a remote part of the galaxy journeying home. This series aired between 1995 and 2001 for 172 episodes.\nScorpions Year of Hell Caretaker Message in a Bottle Eye of the Needle Heroes and Demons Jetrel Timeless Pathfinder Riddles Star Trek: Enterprise The most recent spin-off ran after Voyager and was set 100 years before Kirk. This series aired between 2001 and 2005 for 98 episodes.\nBroken Bow The Andorian Incident Shuttlepod One In a Mirror Darkly Twilight Damage Azati Prime Regeneration Impulse Zero Hour I am not going to review each episode here, I will save that for future blogs. With 50 episodes that is one a week to get them all reviewed during Star Trek’s birthday year. We will see if I manage to keep to that schedule.\n","date":"Jan 7, 2016","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2016/01/07df3N8m.jpg?w=512\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2016/top-50-star-trek-episodes/","series":null,"tags":["StarTrek","Voyager","DeepSpaceNine","Spock","Kirk","Enterprise"],"title":"Top 50 Star Trek episodes"},{"categories":null,"content":"Wow! What an amazing year 2015 has been and with 2015 about to end I thought it would be good to look back at what happened in this amazing year.\nJanuary\nI started the year with a declaration that 2015 would be my year of code. Well I have had a few breaks from programming but I have learnt loads this year. I have flitted a bit concentrating on different areas of programming but I think that has broadened my knowledge. I consider myself to be a programmer now , which I didn’t at the start of 2015.\nFebruary\nA first for 2015 was appearing on a podcast . The Trekmate podcast Ten Forward were the lucky ones that got to experience my first attempt at talking to the world. This continued throughout the year with appearances on Upper Pylon 2 , The Battle Bridge and even the Sci-Fi Waffle podcast.\nIt was a sad time for Trekkies at the end of February when Leonard Nimoy passed away.\nMarch\nIn March I talked about my recent efforts with Azure Traffic Manager to help prevent downtime. I also spent a lot of time improving databases which led me to consider ways to deploy it better.\nAlso in March I attended an IT networking event called ITBoss . Free food and drinks and a discussion with fellow IT Managers.\nApril\nApril was a bit of a mixed bag. I celebrated my second wedding anniversary but I also said goodbye to my car due to financial reasons.\nI played about with a Raspberry Pi , mostly with the camera . I really must make some time to play with it more.\nMay\nIn May I went to a C# User Group called Leeds Sharp, was great fun and I really need to make more time and go back in 2016. It did result in one of my most popular blog posts .\nUsually developers don’t enjoy finding bugs in their software but I had great fun when I realised I had a Stack Overflow in one of my databases, it was easy to fix once I worked out what was going on.\nIn May the UK went to the polls as we had another General Election.\nJune\nIn June I celebrated my 50th Blog Post . Amazing to think I had already written 50 Blog posts, even more now!\nLeeds Sharp had a coding challenge, to solve Sodoku puzzle programatically. Didn’t get it finished but made a good stab at it.\nJuly\nIn July it was SysAdmin Day and I celebrated with my staff the amazing work we do keeping everything working.\nMicrosoft launched Windows 10 in July, the latest operating system is being offered for free for a year.\nAugust\nIn August everything changed as my son James was born. I took time out from blogging and work during this time to spend with James.\nBefore James was born I stepped down from my involvement with the technical team at St Michael le Belfrey.\nSeptember\nIn september it was back to work, where I started planning how to add internet connections to reduce the chance of outages.\nOctober\nIn October I took James to a dads group . This was the first time Dad spent time with James without Mummy and it was awesome.\nI also celebrated my first birthday after becoming a father, was great to have a card from James. James also got the chance to spend time with my family in Nottingham.\nNovember\nIn November the countdown to Christmas began with James appearing in his first Nativity as Jesus and dressed up as an Elf at a Christingle service.\nExciting Star Trek news was announced this month, a new TV series is to be aired in January 2017. No other details yet but very exciting.\nDecember\nDecember is all about Christmas and we had a great Christmas time as a family.\nSadly York was hit by Floods just after Christmas, luckily my family was not affected but many others have been. Hopefully by the time you read this the cleanup will be well underway.\nBefore Christmas I spent my time counting down the days with the Advent of Code What will 2016 bring?\nI don’t know. There will be lots more time with James as he continues to learn and develop. I will continue to learn and develop my coding skills. Watch this space as I keep blogging about it.\n","date":"Dec 31, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/12/151218181153-2015-year-in-review-biggest-stories-wrap-up-orig-00025316-large-169.jpg?w=460\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2015/look-back-2015/","series":null,"tags":["Blog","James","Goals"],"title":"A look back at 2015"},{"categories":null,"content":"I use SQL Server Management Studio all the time for writing queries, getting data and general SQL development.\nI have enjoyed seeing the improvements that each new version of SQL Server Management Studio (SSMS) introduced. One great improvement was intellisense.\nThis feature saves typing and reduces errors by automatically suggesting tables, column names or other database objects.\nA common query that I get asked to write is provide a spreadsheet that gives the information that satisfies certain criteria. This is easy to do in SSMS, you can write the query, click execute and the rows that satisfy the criteria are displayed. These rows can then be easily copy/pasted into Excel or other spreadsheets.\nA common data item that gets stored in databases is addresses and addresses often contain line breaks to make the data display better. In the earlier versions of SSMS, when you copied and pasted these line breaks were ignored and the data displayed the same in SSMS as it did in Excel. However in the more recent version, theses line breaks got copied across breaking your spreadsheet and making it hard to see what data corresponded with what.\nNow I don’t know if this should be described as introducing a bug or fixing one. I can easily argue both sides. If your data contains a line break and you copy this data it should include the line break in the destination, but if it does that it displays badly in Excel.\nThe fix I have been using until recently is to use the following TSQL command in my queries.\nSELECT REPLACE(REPLACE(@str, CHAR(13), \u0026#34;), CHAR(10), \u0026#34;) This command replaces any line breaks with an empty string. Both Char(10) and Char(13) are needed because you can have different types of line breaks. This is great if you are writing the script from scratch but isn’t great if your are running a stored procedure or your query has a lot of columns.\nThe answer to this is to use Visual Studio to run your SQL query. In Visual Studio you can write and run queries via Server Explorer and the results produced don’t contain line breaks. I have only just discovered this solution, but so far it has worked and is very easy to do, plus as I do most of my development in Visual Studio anyway it saves me having to open SSMS to test my queries.\n","date":"Dec 3, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/2015/sql-server-management-studio/","series":null,"tags":["Database","Programming","SQL"],"title":"SQL Server Management Studio"},{"categories":null,"content":" As you have probably heard Star Trek is coming back to TV. I recently joined with the Sci Fi Waffle podcast to discuss.\nSci Fi Waffle: Episode 12 – STAR TREK IS BACK!!\nIn this episode we discuss the great news that Star Trek IS BACK!!.\nShawn and I are joined again by James Roberts and TrekMates News feed Editor Simon Foster to speculate about the announcement of a new Star Trek TV series due to debut in January 2017.\nJames can be found on our sister podcast The Battle Bridge and Simon can be found on Twitter @funkysi1701 To listen go http://www.trekmate.org.uk/sci-fi-waffle-episode-11-star-trek-is-back/ , to leave feedback go http://forum.trekmatefamily.com/ ","date":"Nov 12, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/11/3f448330857e4ca7ba40ab5465a3c2cb62cdb98c.png?resize=620%2C260","permalink":"https://www.funkysi1701.com/posts/2015/star-trek-is-back-in-2017/","series":null,"tags":["StarTrek","podcast"],"title":"Star Trek is back (in 2017)"},{"categories":null,"content":"I have been asked to investigate adding a fail over internet connection to one of our offices.\nCurrently this office is connected via a wireless link with our head office. Unfortunately we have recently experienced some poor performance with this connection, this has now been corrected but like all technology there is the chance of it failing in some way in the future and causing loss of business because of it.\nInternet Connection This is one of my first considerations. I need a reliable internet connection and there are a lot of options to consider, lets look at a few.\nADSL Connection This is your standard internet connection that most homes have, you get around 20Mb/s download but only 2Mb/s upload for a fairly low price per month. Speeds are dependent on area and the quality and distance from your exchange, some rural areas have speeds much lower than this. As this connection is for an office I want the best connection I can afford so would rather not go down this option if I can avoid it.\nFTTC (Fibre to the Cabinet) This is what ISPs typically mean when they say superfast broadband. The cables that have been laid to your exchange cabinet have been upgraded to fibre optic cables which are capable of much faster transfer speeds (eg 40Mb/s download and 10Mb/s upload). This is my ideal choice, however BT Openreach are responsible for upgrading the country to these cables and they are not very quick about it. A few years back when investigating this FTTC had almost reached our office, but when I investigated today nothing seems to have progressed. Both FTTC and ADSL require a phone line into the office to run on.\nLeased Line This is where your ISP connects a dedicated line to your office which provides you a much faster connection (upload and download speeds are the same) but this is very expensive. We have this for our head office as this is where our servers are located and the faster speed benefits the entire company. We do not want to do this for every branch office as it will be too expensive.\nSpur from a Leased Line It could be possible to branch or spur off the leased line to our second office. This would be one leased line that terminates in two locations instead of one. However due to the distances involved I do not think this will be possible in my case and may end up costing as much as a second leased line.\nISP As well as the type of connection I need to decide who will provide it. There are hundreds of ISPs out there but I like to use ISPs that have a good reputation or I have used in the past and have been happy with. I do not stick with one ISP for all my connections as I like to have some connections that continue to work should that ISP be experiencing problems. ISPs that I would recommend are YDS, Eclipse and PlusNet\nMoney Off While I am talking about internet connections the government has been offering a scheme offering businesses money off upgrading their connections to a superfast connection (either FTTC or Leased Line) More information can be found on your councils website, If you have an office in York have a look here for more details. We got £3000 off installation so worth investigating if you qualify.\nVPN Connection Offices require access to far more resources than just the internet. This particular office requires access to our email and database servers as a bare minimum. One way to access a remote offices network resources is through a VPN (Virtual Private Network) connection. So my next consideration is how to establish a VPN link between our offices.\nSoftware Windows Server includes RRAS (Routing and Remote Access Service) which you can use to configure a VPN connection. One important thing to note is that the server needs at least two network connections, one on the internet with a public IP and one on the internal network.\nHardware The more expensive routers often have settings that allow the configuration of a VPN connection.\nI have used both of these methods in the past. RRAS is a pain to work with but has evolved with the newer versions of windows server, so may not be as bad as I remember. Using a router to do VPN introduces its own set of problems and there is no guarantee of avoiding RRAS, you may still need it to authenticate the VPN provided by your router.\nMultiple Connection Handling Internet connectivity is being provided in two ways, the primary way is via a Wireless link and the second way is via one of the options above.\nThere are lots of options to deal with multiple connections, do I want to load balance both connections and use them all the time, do I want to fail over onto the second connection only if the first one fails, or do I want it to be a manual process or switching over to the backup network if problems occur.\nAs you can see there are lots of different technologies to consider before I can add resiliency to this office and I haven’t even started to think about if an additional server will be required.\n","date":"Sep 24, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/08/14091d1357164106-internet-connection-drops-every-couple-minutes-cable-sxchu-internet1.jpg?resize=660%2C252\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2015/adding-internet-connection-resiliency/","series":null,"tags":["Fibre","VPN","Internet"],"title":"Adding Internet Connection Resiliency"},{"categories":null,"content":"For the past few months I have been deploying changes to my companies database every couple of weeks or so. Over a weekend when the database was not being used I would make a backup of the database, load up visual studio, deploy the database changes and copy a couple of front-end files.\nMy weekends and evenings have recently become a bit more precious to me since the arrival of my son James. At suitable times to run the database upgrade I am either, asleep, about to fall asleep or be spending time with James.\nThis entire deployment can be set to run at a time of my choosing using SQL Server Jobs, (not sure why I never thought of doing this before). I still need to check everything is working after it has run. The job can send me an email if it encounters any problems so I can still ensure the company has the minimum of problems but has more frequent changes.\nUnmanaged Database Deployment OK so what do I need to do to set this up.\nTest all your code changes are working correctly on a backup of the database and everything is committed to source control. Backup everything that is going to be changed so it can be rolled back in case of a problem. I don’t rely on the daily backup jobs for this, I do my own. Maybe I am a bit paranoid, or maybe I am just being cautious. BACKUP DATABASE DBName TO DISK=\u0026#39;E:\\SQL Backups\\Filename.bak\u0026#39; I would include in the backup file name the date and time of the deployment for easy reference later on. For the front-end files these are backed up daily and only ever change during a deployment so I am going to rely on the day to day backups, plus they are in source control so in a worse case scenario I can look there.\nFrom Visual Studio create a deployment script using the publish option and save as a file on your database server. Create a SQL Server Job and give it a descriptive name. In steps create a step called backup and enter the T-SQL code above. Set this to stop on failure and continue on success. Create a second step called upgrade of type operating system with the following code. sqlcmd -S (local) -i \u0026#34;E:\\SQL Backups\\deploy_test.sql\u0026#34; The -S parameter is the name of your SQL Server instance and the -i parameter is the path to the sql script you generated from Visual Studio above. Set this job to quit reporting success or failure.\nIn schedule create a date time you want the job to run, make sure this is a one time job as you don’t want it to try and run again possibly causing problems. In notifications I have set to email on completion, you can set it to email on failure only. I would prefer to know the outcome of the job regardless. Using SQL Server jobs to copy files over the network is possible but is not trivial to setup so I am going to use windows task scheduler for this. Create a job in task scheduler to copy all the ADPs or ADEs that have been changed. If the SQL job fails you will need to copy the old versions of these back but that is simple enough to do. This process can be expanded to check code out of source control and do even more automated deployments but for now this is good enough for my purposes.\n","date":"Sep 10, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/09/deploy.jpg?resize=263%2C300\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2015/unmanaged-database-deployment/","series":null,"tags":["SQL","Programming","Backups","Deployment"],"title":"Unmanaged Database Deployment"},{"categories":null,"content":"I am pleased to announce the birth of James David Martin Foster weighing 9 lb 12 on 19th August 2015.\nMother and Baby (and Father) are doing well.\nI am not a baby person, I hate children and I run a mile from other people’s babies.\nBut James performed some magic on me and now I will do anything for him. If you have had children you probably know what I am talking about and if you haven’t you are probably like me 24 hours before James arrived.\nDuring the pregnancy I had been living in denial that I would become a father, despite the wife speaking about nothing else and our home filing with baby equipment. It was only minutes before I first saw my son that I changed.\nDue to James’s large size, the birth wasn’t easy. After about an hour of pushing the midwife called in a team of doctors and nurses to help and I think it was at this point that it started to hit me. I am not an emotional man, if anything I tend to repress my feelings but I completely and utterly failed to do that on this occasion. By the time the baby had been placed on my wife’s chest tears of joy were streaming down my face and this continued for some time. Moments after birth James had a tight grip on my finger. His tiny hands were like miniature versions of my own hands.\nI have been asked what it felt like holding James in my arms for the first time, this happened on the following day. I am not sure if I can put it into words, but a few adjectives come to mind, amazing, great, fantastic, humbling. James depends on me (and his mother) for everything and his existence has put a new spin on everything I do. I want to be there for him, help him learn stuff, provide for him and love him.\nI have been working very hard recently, spending my free time writing this blog, learning stuff and doing other work. The reasons for this hard work are now clear, I work so I can provide for James and so I can spend my free time with him.\nBefore James arrived I had it in my head that his mother would do the majority of the caring for him. This is not going to happen now that I have met James. I want to cuddle him, change his nappies, dress him, hold his hand, take him to school, watch TV with him, stroke his head when he is ill, tell him about my life experiences, introduce him to wine, tell him about girls, help him move home, etc.\nI am not saying being a parent is going to be easy. I am pretty clueless about what is in store for us, but already I am worrying if he is OK, trying to calm him down when he is upset and being woken in the early hours.\nAnyway only time will tell if I continue being this positive about fatherhood, but so far I am loving it. Normal blog posts will hopefully resume over the next few weeks (or when I have time).\n","date":"Sep 3, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/07/james5.jpg?resize=300%2C225\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2015/baby-magic-and-becoming-a-father/","series":null,"tags":["Father","Baby","Family"],"title":"Baby Magic and Becoming a Father"},{"categories":null,"content":"I recently saw this blog post by Brent Ozar that I thought I might discuss.\nBrent listed 13 questions to ask about a database before you start working with it. I am going to go through these 13 questions and expand on them based on my experiences.\nIs this database in production now? I think it should go without saying that the first thing you should find out is if your database is in production. If its not in production you can do what you like and no one will notice. I know what databases are in production and which aren’t where I work so I can answer this one.\nIf this goes down, what apps go down with it? What apps are running on what databases is a good second question. I have at least one database which has multiple front end apps. At first glance you would think that that these two apps are not connected but they are, I need to be careful with both of these apps to make sure they don’t break each other. I know what apps run off what databases.\nWhen those apps go down, is there potential for loss of life or money? This is a difficult question so I will split it into two. Loss of life, my databases don’t control life support machines or nuclear weapons so my first instinct would be to say no. However it is not that straightforward, what if your app allowed contractors to know the location of dangers inside a property they were working in. Once your app goes down, they could have an accident due to lack of information. Loss of money, this one is more straightforward. Time is money in the business world so any time that your app is down and your employees are not able to work is a loss of money. If your database is linked to an eCommerce site, the loss of money could be extremely high. I know what affect downtime will have on my users and business.\nHow sensitive are these apps to temporary slowdowns? Similar to the previous question, a slowdown can be as serious as downtime for some applications. Luckily most of my apps are internal only so are not seriously affected by slowness.\nWhen was the last successful backup? I manage the backup schedule for all my databases so I know exactly when each one was last backed up. When ever I do anything to a production database I will run a backup so I can roll back in case of problems. As part of developing changes I run my changes on a backup of the data. I can script all my changes and repeatedly run them against a backup until I am sure no problems will occur.\nWhen was the last successful restore test? More important than a backup is testing restoring your databases. If you can’t restore data then your backup is useless. I try to test restoring my backups at least weekly so I know that I can rely on my backups.\nIs everyone okay losing data back to the last successful backup? If disaster strikes you could loose all data between now and the time of your last backup. But all is not lost transactional backups can be scheduled throughout the day, in my case the most data we could loose is 15 minutes. This could be configured to be more or less frequent depending on your data. But remember the previous question and make sure you test a restore of your transactional backups, if you can’t restore from them you will be forced to restore from the last successful backup.\nWhen was the last successful clean corruption test? Corruption can be a killer if it is not found quickly. If you need to restore to the last backup before corruption occurred this could result in a significant amount of data loss. To check for corruption you need to run DBCC regularly.\nDo we have a development or staging environment where I can test my changes first? If the answer to this is no, then your next job is to setup a development or staging area. Having a development environment makes development a lot easier and I don’t think I could manage to do all the changes I have done recently without one.\nIs there any documentation for why the server was configured this way? I really wish we had more documentation about configurations as it would make finding out why thing were setup the way they are. So unfortunately the answer to this question is No.\nWhat changes am I not allowed to make? Depending on what your app does, where it is hosted, how quick the changes are needed and many other factors will all restrict what changes you can make. Historical decisions on the database can also affect what changes can be made, if the database has been structured in a certain way, it may be very difficult to restructure it in a more efficient way.\nWho can test that my changes fixed the problem? This is an interesting question, from experience the best people to speak to about problems with the database are the users. If they can show you how to reproduce a problem, you should be able to fix this problem, after that you can probably get them to verify that it has been fixed.\nWho can test that the apps still work as designed, and that my changes didn’t have unintended side effects? This is an extension of the previous question. The main users of your app should be your first port of call to find out if the app works as expected. However exploring side affects and undesired features is something that I would test as part of the development process. It has taken a while but I have constructed a detailed check list that can be used for testing so I know that most bugs can be found before release.\n","date":"Jul 20, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/07/uf010206.jpg","permalink":"https://www.funkysi1701.com/posts/2015/things-to-know-before-working-on-your-databasethings-to-know-before-working-on-your-database/","series":null,"tags":["Backups","SQL","Programming","Disaster Recovery"],"title":"Things to know before working on your database"},{"categories":null,"content":"Last night I went to a Code Dojo at Leeds Sharp (the coding user group I have started going to). A Code Dojo is a programming challenge that people work on usually in pairs.\nThe challenge that we worked on was to code a solution to the puzzle game Sudoku. The code we worked with can be found on github.\nI don’t know much about Sudoku but the game goes like this. You start with a 9×9 grid, and the idea is to fill in all the missing numbers.\nEach Row can only use each digit 1 to 9 once, each column can only use each digit 1 to 9 once and each subgrid (3×3) can only use each digit once.\nI worked with Richard one of the Leeds Sharp organisers, we started by trying to loop through each empty cell and insert values in a brute force attack, but this approach didn’t get us very far (literally as the application crashed!)\nThe approach that we needed to use was to loop through calculating possible values.\nIt was only when I sat down at this dojo did I realise how much I still have to learn, but while talking with Richard about different approaches and different coding structures I started to learn.\n","date":"Jun 26, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/06/2000px-Sudoku-by-L2G-20050714.svg_.png?resize=660%2C660\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2015/sudoku-challenge/","series":null,"tags":["Dojo","C-Sharp","LeedsSharp"],"title":"Sudoku Challenge"},{"categories":null,"content":"In the UK today is Fathers Day. Happy Fathers Day Dad!\nI thought I might talk about how he encouraged my interest in technology, especially as in about 8 weeks I will become a parent and will have someone to encourage and inspire.\nI am going to sound really old but when I was born there was no internet, no mobile phones and computers more often appeared in sci-fi than in your home.\nThe first computer I remember us owning was a Lynx. A quick google reveals there were a number of different models, I have no idea what we had but here is what I found.\nIn March 1983 the Lynx 48K was released, in September this was upgraded to 96K. In December 1983 a new Lynx was released with 128K. To put these numbers in perspective on my desktop is an image file that is 123K! The CPU in this computer was rated at between 4 and 6Mhz. (Modern PCs have CPUs around the 1.5Ghz or 1500Mhz)\nThe Lynx we had, didn’t have a disk drive or a mouse. Programs were loaded from audio tapes (Audio tape was a form of data storage that came before CDs where data was stored on magnetic tape, most often used for music) The tapes we had would often get stuck and data wouldn’t load. The answer to this was to fast-forward/rewind the tape or hit the tape on your leg. For some reason this is a memory I still have of my dad hitting a tape on his leg to get a game or something else to load on the lynx. We also had a joystick for playing some of the games. I believe we also had an early thermal printer.\nMy memory of the Lynx is in black and white. The reason for this is that the Lynx was connected to an old black and white TV, it was a special treat when we connected the main colour TV to the Lynx.\nWe had a wide range of games for the Lynx, I can remember Jet Set Willy, and Diggerman. But there were also programs which my father had created. The Lynx had a form of BASIC where you can write you own programs and my father spent many hours writing programs for me and my sister. (We actually never realized this at the time)\nWith that as my introduction to computers is it any wonder that I do what I do now? Now what should I do for my child to encourage them in a similar way?\nAs a family we never owned a games console. Not sure if this was a financial or moral choice but it is something I will have to think about. I want my child to be knowledgeable about computers and able to compete in the competitive job market, but I also want them to know about science and nature and to enjoy their childhood. Lots of technology choices to make over the next few years.\n","date":"Jun 21, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/06/PRODPIC-2926.jpg","permalink":"https://www.funkysi1701.com/posts/2015/fathers-day/","series":null,"tags":["Father","BASIC","1980s","Lynx"],"title":"Fathers Day"},{"categories":null,"content":"Today is my day off, but I wake up and have a quick look at nagios to see if there is anything I need to worry about. Yes there is, SQL Server has run out of disk space on its data disk.\nI race downstairs and VPN onto the server to find out what has happened. One of my monitoring databases has had runaway log growth and is over 80Gb is size.\nBACKUP LOG [DBName] WITH TRUNCATE_ONLY DBCC SHRINKFILE(\u0026#39;DBName_Log\u0026#39;) Free disk space is back to normal, all users will be unaware of the problem and everything is fine again. I create a daily job that runs the above code, that way it should stay a manageable size.\nNext I need to find out why it happened and to prevent it happening again in the future (Next time I have a day off I want to lie in!)\nI check the SQL logs and notice\nBACKUP LOG WITH TRUNCATE_ONLY or WITH NO_LOG is deprecated. The simple recovery model should be used to automatically truncate the transaction log.\nThen I remember what I have done to cause this issue. I have a separate disk for my backup files and earlier in the week I noticed this disk was filling up, a large amount of space was taken up by transactional backup files. I thought I don’t need to backup the transactions for this non critical database, I will just do a full backup at the start of everyday.\nHowever what I forgot is that a transactional backup keeps the log file under control, once this backup was stopped the log file grew uncontrollably. The answer, change the database from FULL mode to SIMPLE. This is my understanding of how backups work in FULL mode. A full backup is done at the start of the day which resets the log file, then changes in the database are stored in the log file, this is backed up into a transactional backup and the log file gets reset. If you have regular transactional backups throughout the day the log file doesn’t grow too much, however with no transactional backups your log file contains an entire days worth of changes and so for a monitoring database this could be quite large.\nIn SIMPLE mode you can’t do transactional backups and the log doesn’t grow uncontrollably. This shouldn’t be used for production databases as if there is a problem you could loose data.\n","date":"Jun 12, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/2015/runaway-sql-log-growth/","series":null,"tags":["ITAdmin","Backups","SQL"],"title":"Runaway SQL Log growth"},{"categories":null,"content":"Every developer uses source control, it is a great tool for keeping track of changes to your code, seeing who has done what.\nHowever I keep messing up, my use of it. I am fairly disciplined when creating new features, all my changes will get committed to source control and when I am happy I will deploy these changes to the live system. But as soon as there is a bug, especially one where the client is chasing for a fix, I will deploy a fix as soon as humanly possible on to the live system.\nAt first glance there is nothing wrong with what I have described but what happens the next time I deploy a new feature. Yes the bug the client was complaining about gets deployed with the new feature as the fix was never committed into source control. The client gets angry as the bug he was screaming about is back again.\nEvery change you make to the system MUST be committed to source control. If it isn’t that change will look like it never existed. I have worked with source control for over 5 years why do I keep making this rookie mistake over and over.\nTroy Hunt has a blog post about the 10 commandments of using source control. His number two commandment is “If it’s not in source control, it doesn’t exist” He talks more about code you have written being not saved into source control, but the principal is the same for my example as his.\nThere are ways to automatically deploy from source control, however most of the time you don’t want your live database being rebuilt because you fixed a typo. Additional steps will need to be implemented and there is still the chance that you might want to bypass these steps to fix the urgent problem. The only way past this problem is for you and everyone on your team to be disciplined and only ever commit to source control first, and only after that deploy live (either automatically or manually)\n","date":"Mar 16, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/2015/source-control-fail/","series":null,"tags":["Git","Programming"],"title":"Source Control Fail"},{"categories":null,"content":"I have spent most of the day tweaking my Azure websites. Lots of fun!\nLast week unfortunately Azure had some problems and many websites that were running in the North Europe data centre were unavailable for several hours. And you guessed it my websites were hosted here.\nAll hosting providers are going to have downtime from time to time and this is just something you have to take on the chin. The important thing to do in times like these is communicate with your customers about what is going on and that you are doing everything you can to restore service.\nHowever Azure has some amazing features that you can configure to help manage when downtime occurs.\nAzure is Microsoft’s global cloud platform. And it really is global, there are data centres in North Europe, West Europe, Brazil, Japan, two more in Asia and five in the US. In the event of problems it is highly unlikely that more than one of these would go down at once. If all of these are unavailable, I expect the planet earth is facing some kind of cataclysmic event and the fact that my website is down is not a priority.\nTo take advantage of these multiple data centres, Azure has something called a Traffic Manager.\nTraffic Manager has various settings but I am using it in failover mode. This means that if one website goes down, the next one is used.\nAll you need to do is create a traffic manager, add two or more websites to it (called endpoints) and choose a page that needs to be monitored so Azure knows which websites are up and which are down.\nIf you are using SSL or custom domain names, there are a few extra steps you need to do. Your custom domain name needs pointing at the traffic manager, not the individual websites. The websites themselves have three domain names, the traffic manager address, the azure address and the custom domain name. The SSL certificate can then be assigned to each website that you have added to the traffic manager.\nThat was easy wasn’t it, and now if a website goes down traffic manager will use the next one. While testing this, the transition to the next website was almost immediate. I did notice that if you had a browser showing the website open during a problem you sometimes got an error page, I think this was probably due to browser caching, reopening a tab or browser fixed this issue.\n","date":"Mar 12, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/2015/azure-traffic-manager/","series":null,"tags":["Azure","DevOps","ITAdmin"],"title":"Azure Traffic Manager"},{"categories":null,"content":"My desktop is always a mess. I constantly download files there and forget all about them.\nEvery now and then I copy files into sub directories, so my desktop looks sane for a day or two before it gets out of control again.\nWhy don’t I write a script that I can schedule to do this for me. Then my desktop will always be tidy.\nI have written a few simple batch scripts, but of course the best scripting language out there at the moment is PowerShell. Lets use that.\nWindows provides a nice little utility for writing scripts called the Windows PowerShell ISE, so let\u0026rsquo;s start by loading that up.\nPS has lots of help included to help you, just run Get-Help [name of ps command]\nTo move files you can use Move-Item which works very similar to copy, specify source and destination. In my case I moved files based on their file extension.\nMove-Item *.pdf folder\nNow all I need to do is schedule this script to run either every day or so, or maybe every time I login or switch my computer on.\nPowerShell can do lots more interesting things which hopefully I will blog about soon.\n","date":"Mar 11, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/2015/tidying-my-desktop/","series":null,"tags":["PowerShell","Desktop","Programming","ITAdmin"],"title":"Tidying my desktop"},{"categories":null,"content":"Last week I blogged about the Game of Life.\nWell it took some searching through dead hard drives and old USB storage but I found the program I wrote, and better than that I have figured out how to turn it back into source code.\nThe file date is November 2004 and the source code has no comments so I don’t know what was going through my head when I wrote it, or even what some of it does.\nTo view the source code download Life.src , to download the finished program download Life .\nI am actually surprised I was expected the code to be much worse than it was, I am not saying it is good, but I can see more than one method and I can’t see any code that obviously is repeated. I can see some terrible variable names. Note to future self don’t use tmp676_674!\nAssuming that I wrote this in 2004 what would I have been doing then? It was before I started working in IT. (Hard to imagine I know!) It may even have been before I started living with Keith, before he recruited me for my first IT role. I think I was probably working as an administrator for Defra at this point.\nIf we put 2004 in technical terms, it was before I joined facebook, Office 2003 and XP would have been installed on my PC, and probably felt brand new.\n","date":"Feb 21, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/02/Life.jpg","permalink":"https://www.funkysi1701.com/posts/2015/source-code-for-game-of-life/","series":null,"tags":["Java","Programming","SourceCode"],"title":"Source Code for Game of Life"},{"categories":null,"content":"You may have noticed that my blog has a new home. It can now be found on its own domain http://www.funkysi1701.com . This change not only gives me more flexibility to make any change I like, it also feels good to have a website again. If you had followed my blog, do keep following it at the new location.\nI have also been playing about with giving the site a new look, hope you all like it. I have gone for a more image focussed front page. No doubt in a few weeks I will be bored and change the look again.\nI have recently been reading about blogging and one suggestion is to have a clear theme, or idea of what the blog is about. I am not sure my blog has a clear theme.\nWhen I started blogging I was more interested in blogging than thinking about what I was blogging about. The vague theme I worked off was to talk about what I do, to answer the question, \u0026ldquo;What do you do?\u0026rdquo;\nI think this is a fairly good theme, but it could probably do with some refinement. The best themed blogs are specialist or about something specific. As I am generally learning about development, rather than doing development I think this should be part of what I concentrate my blogs about. A minority of my blogs can continue to be about the other things I spend my time doing, as it’s always good to have a break from the main “meat” of the blog every now and then.\nThe people who read my blogs are split between technical people and non-technical people, so I would like to continue to explain what I do in non-technical terms. This is an essential skill for a developer (or SysAdmin) as you will always need to explain what you are doing, or discuss requirements. It is not an easy skill and like development itself, the more I do it the better at it I will become.\nSo watch this space for the blogs to come. I know I will be working with Azure over the next few weeks so maybe some blogs about that, maybe some more SQL blogs or maybe about what I learn next.\n","date":"Feb 15, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/02/unnamed.png","permalink":"https://www.funkysi1701.com/posts/2015/blog-theme/","series":null,"tags":["Theme","Blogging","WordPress"],"title":"Choosing a Blog Theme"},{"categories":null,"content":"The podcast I recorded the other week has been released, why not have a listen by heading over to trekmate.org.uk and comment on their forums\nThis week Sina, Chris and Paul welcome Simon Foster, Trek Mate’s News Editor, to do discuss his Desert Island Trek choices. Paul becomes Firbob, or AntiFirbob, or\u0026hellip;something like that. Boo hiss Firbob. Every one listen and decide – Simon or Firbob?\nYou can reach Simon here:\n@funkysi1701 Complaints to @firbob1\nYou can reach the rest of the gang here:\n@queenkatblue , @chriscantfly , or facebook.com/TenForwardPod ","date":"Feb 13, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/02/Spock_and_Horta_mind_meld1.jpg","permalink":"https://www.funkysi1701.com/posts/2015/ten-forward-episode-135-anti-firbob-is-back-or-simons-desert-island-trek/","series":null,"tags":["StarTrek","Podcast"],"title":"Ten Forward Episode #135 – Anti Firbob is Back or Simon’s Desert Island Trek"},{"categories":null,"content":"Many, many years ago I tried to learn Java. I didn’t get very far but one of the things I built was The Game of Life.\nUnfortunately I can not find the code I wrote, probably a good things as it is probably amusingly bad.\nNow you are probably asking what is the Game of Life ? The game of life, sometimes called cellular automation, is a simulation of cells being born and dying. It is sometimes called Conway’s Game of Life after the British mathematician who devised it in 1970.\nThe game has several rules:\nAny cell with less than three neighbours will die on the next turn Any cell with three neighbours lives on to the next turn Any cell with more than three neighbours will die on the next turn Any dead cell with three live neighbours will be born on the next turn Now I had not thought about this game until the other day when I came across a blog and a github page which sparked my interest again. This version is written in javascript, but the blog mentions a desire to build this program in many different languages. I want to build this with visual studio in C# as it should provide a good training exercise.\nThere are several problems I need to overcome to achieve this, I am currently trying to work out how to \u0026ldquo;draw\u0026rdquo; the cells. My current thinking is that maybe I can code it using characters to display the cells and then replace with pixels once I have the logic correct. Well lots for me to learn!\n","date":"Feb 11, 2015","img":"","permalink":"https://www.funkysi1701.com/posts/2015/game-of-life/","series":null,"tags":["Java","Programming"],"title":"Game of Life"},{"categories":null,"content":" The Great British Bake Off and The Great British Sewing Bee are very popular TV shows. Each week the contestants take part in a baking or sewing challenge and the judges choose the best and the worst, with the worst having to leave. This is very entertaining as you watch the drama between the contestants and the reactions as the judges tell them they are rubbish.\nCould this simple format be applied to computer programming? I think it could.\nThe only developers I know argue about the best way of tackling different problems, despite being friends. I am going to assume that this is typical behaviour and not unique to them. So for this TV show, we will split the contestants into pairs, that way we would get the added drama of how well they work together.\nI am still undecided if the pairs stick together each week with one pair leaving or if we randomize the pairs with one person leaves. The second option would cause there to be a group of three every other week, not sure if this would be a benefit or a curse for that group.\nMy friends who are experienced programmers have agreed to be the judges, they both have experience of hiring and firing people, know what to look for in good code, and never agree so should be entertaining enough.\nOn the bake off the contestants don’t really know how well they have done until the judges cut into their cake and take a bite. For the code off we could not compile the code until the judges run it, would make coding extra difficult as if the contestants are anything like me, I run my code every five minutes to see what affect that change has made.\nSo what challenges can we come up with?\nWeb Design week Stored Procedure week Mobile App week Loops Logic problems Bugs weeks Performance issues I think we are onto a winner here. What does everyone else think?\n","date":"Feb 7, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/02/logo1.jpg","permalink":"https://www.funkysi1701.com/posts/2015/great-british-code-off/","series":null,"tags":["Programming"],"title":"Great British Code Off"},{"categories":null,"content":"I am lazy, I won’t try and deny that. When my alarm goes off in the morning, I will snooze it for twenty minutes or so before getting out of bed.\nIn my work my laziness continues. Remote Desktop (or RDP) is probably my number one laziness tool. For those that don’t know RDP allows you to connect to another computer and access it like you were sat in front of it. So I can be sat at my desk and RDP into any other computer in the office including any server. However this laziness tool does sometimes require a bit of effort sometimes, powering on the target computer, logging on locally, making sure the user account you are using is allowed to use RDP.\nWriting a script is another example of a laziness tool. I often get asked to do tedious and long-winded task, because I am lazy I will go out of my way to learn how to write a script to do this, so that I can run this script and do this long-winded task in a matter of seconds. There are loads of different types of scripts from database SQL scripts, to PowerShell scripts that can do almost anything on your server.\nPowerShell is something Microsoft are really pushing at the moment, you can even write scripts to create new user accounts, so no longer will you have to remember to tick that tickbox for every new user account. And because PowerShell is part of almost all MS technologies you can link Exchange to Active Directory and you don’t even have to remember the right syntax as PowerShell has a built-in help command to tell you how to run that useful command.\nAs I am now moving into a more developer role can I continue to be lazy? I sure can. Code should be written once and reused as often as possible and this is one of the features of OOP (Object Oriented Programming).\nIt can be as simple as creating a master page so you don’t need to recreate the same code on every one of your webpages. Or every time you find yourself rewriting the same code again, you plug it into a method so it can be called again and again.\nBut of course the ultimate way to be lazy is of course get yourself some staff and spend all day getting them to do everything. If you are lucky they may even try and adopt some of these lazy ideas.\n","date":"Feb 4, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/02/bill-gates-quote.jpg","permalink":"https://www.funkysi1701.com/posts/2015/laziness/","series":null,"tags":["PowerShell","RDP","Lazy","ITAdmin"],"title":"Laziness"},{"categories":null,"content":"For a while now I have been listening to podcasts. For those that don’t know a podcast is an audio file that you can download and listen to, usually about a topic you are interested in. The topics that interest me are obviously Star Trek, Science and Technology and IT stuff. I mainly listen to them in the car when I am driving, but sometimes it will be at home when doing jobs around the house.\nI think the first podcast I listened to was Trekmate . Trekmate as the name suggests is about Star Trek, it’s a few guys talking about Star Trek. It’s very similar to those conversations you have in the pub with your mates. Trekmate do lots of podcasts, one for almost every flavour of Star Trek out there, I don’t keep up with all of them, but the ones I listen to I really enjoy.\nMy favourite podcast that is work related is RunAs Radio . If you are a SysAdmin or have an interest in servers or IT you should listen to this. The host Richard Campbell interviews IT experts about a particular piece of technology, one week it will be Exchange, the next Security, and then SQL Server. Often I will learn something that I will want to try out, some weeks I won’t learn something specific but will give be an insight into which way IT for businesses is going. I think it was probably only after listening to RunAs Radio that are started having more of a long term plan for my IT infrastructure at work.\nAs I have blogged about before I am learning Development so obviously I listen to .NET Rocks . This is hosted by Carl Franklin and Richard Campbell (yes the same guy that does RunAs Radio) who talk to developers about code, learning and IT from a more Developer point of view. This is released 3 times a week so is a bit hit and miss sometimes, but most weeks there will be something relevant to me or worth listening to.\nA few days ago I did something for the first time, I was a guest on one of the Trekmate pods, TenForward. I reviewed 5 Star Trek episodes. This was a bit of a leap for me as an introvert I don’t like to be the center of attention and I certainly don’t like how my voice sounds recorded. Hopefully when released people will like it and I will do lots more of these in the future. Once it has been released I will put a link up here so you can all have a listen.\n","date":"Jan 27, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/01/128900.png","permalink":"https://www.funkysi1701.com/posts/2015/podcasts/","series":null,"tags":["ITAdmin","Programming","StarTrek","Podcast"],"title":"Podcasts"},{"categories":null,"content":"So I have been working through the challenges on the freecodecamp website and one of the unique things that this site does is give you the chance to program with another person.\nI am an introvert and if you were to ask me to pick up the phone and ring someone I will procrastinate a lot before doing it (if I do it at all), I also am not good at speaking with strangers but I have been enjoying the pair programming I have done so far.\nI have only pair programmed a couple of times so far, so I am no expert. The first time was with someone from North America, it was weird to begin with talking to your computer and hearing an american voice respond, but you soon forget that as you concentrate on the problems you are working on.\nThe second time was with someone from the Netherlands. This time we tackled some trickier problems. Both times I took more of a back seat but I think contributed useful stuff and I certainly learnt a lot from the other person.\nI think that is the key thing about pair programming, you learn so much about how to look at a problem from another angle, or learn about a function you haven’t heard of before. If you are reading this and it was you that I pair programmed with a big thank you for spending the time with me, I learnt a lot.\nI was trying to pair program again tonight but technical problems prevented me. One problem I have found is finding a partner, was hoping to do some yesterday but no one else was around to partner with.\nI would be interested to learn what companies use pair programming. I imagine a pointy haired boss would say why pay two people to do one job, when you could get them working on different things. However if you want to build up your team and give them loads more skills then pair programming would be great. I also think there are probably some problems that are crying out for pair programming.\n","date":"Jan 19, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/01/PairProgrammingInAction.jpg","permalink":"https://www.funkysi1701.com/posts/2015/pair-programming/","series":null,"tags":["PairProgramming","Programming","FreeCodeCamp"],"title":"Pair Programming"},{"categories":null,"content":"I got excited earlier this week as I started playing around with the javascript I have been learning.\nI thought I would try and create useful javascript that could be applied to an online filestore. Somewhere that you could upload, delete, edit files and folders and various options to do with your files would popup via javascript or jquery code.\nSo far I have created a static HTML page which lists a few dummy files. I have added a button which adds a new row into my table and I have used jquery to highlight any selected files and also when the mouse passes over.\nIf you want to follow my progress I have put my files on github . It’s very basic stuff and I have already got a few lines I would like to refactor but not bad for an evenings work. I have lots of feature ideas to add to this page which should help me learn the basics.\nThe trouble with learning all this javascript is that I am starting to get all my programming languages confused. Earlier this week I started writing a for loop in javascript, only problem was I was working on an MS Access ADP (Don’t ask!) which requires the code in VBA. That is never going to work. Oh well.\nIf anyone knows good ways to stop myself getting my programming languages all confused do let me know.\n","date":"Jan 16, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/01/logo_JavaScript.png","permalink":"https://www.funkysi1701.com/posts/2015/javascript-progress/","series":null,"tags":["JavaScript","Programming","SourceCode"],"title":"Javascript progress"},{"categories":null,"content":"The first program anyone writes in a new language is often really simple and just displays the text “Hello World!”, below are a few examples from languages I have knowledge of.\nConsole.WriteLine(\u0026#34;Hello, world!\u0026#34;); \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello, world!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; php echo \u0026#39;Hello, world!\u0026#39;; ?\u0026gt; MsgBox(\u0026#34;Hello, World!\u0026#34;) SELECT \u0026#39;Hello, world!\u0026#39; Or maybe\nSELECT * FROM table WHERE name = \u0026#39;Hello World!\u0026#39; \u0026#39;Hello, world!\u0026#39; console.log(\u0026#39;Hello, world!\u0026#39;); PRINT \u0026#34;Hello, world!\u0026#34; Turns out I know quite a few programming languages. And yes there have been times where I need to stop and think about which language I am writing in.\n","date":"Jan 10, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/01/512px-HelloWorld.svg_1.png","permalink":"https://www.funkysi1701.com/posts/2015/hello-world-2/","series":null,"tags":["Languages","Programming","SourceCode"],"title":"Hello World!"},{"categories":null,"content":"There is no escaping the fact that we are in 2015 now. My new years resolution is to do more coding.\nOver Christmas I thought I would have a go at http://www.codecademy.com I completed the javascript course and I was just wondering what I should try next when I saw a tweet about http://www.freecodecamp.com Since then I have been addicted to doing the exercises that they have, almost everyday I have made a bit of progress.\nBut this site is not just about teaching you how to code, it has an active forum where you can ask questions and understand more about what it takes to become a developer.\nBefore I started these exercises I didn’t know much javascript or jquery but I am starting to learn quite a few commands that hopefully I can put into use at some point. Whatever development I end up doing I am sure I am going to need javascript at some point.\nOne of the tips I have learnt is that you need to practise daily. Did you ever use flash cards to help pass exams? Well there is an electronic version of them now called Anki. Just add a question/answer pair to your deck of cards and review them each day, the ones you find difficult you review more often than the ones you have learnt.\nHopefully I can keep this up and later in the year I will be well on way to writing lots of useful code.\n","date":"Jan 9, 2015","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/01/AK29YaTf_400x400.png","permalink":"https://www.funkysi1701.com/posts/2015/my-year-of-code/","series":null,"tags":["JavaScript","Programming","JQuery","FreeCodeCamp"],"title":"My Year of Code"},{"categories":null,"content":"As today is New Years eve I thought it would be a good chance to look back over the last year.\nAt the start of the year I finally got agreement for the installation of a 30MB leased line. This was an amazing achievement as I had been really struggling to keep everything working on our ADSL connections and the complexity of the connections was starting to cause problems itself.\nHowever it was not a smooth process, the site survey revealed a substantial installation cost would be required, however the Council was about to launch a voucher scheme that would give us £3000 off. So most of this was just waiting around for the Council to get things going. By April we had it installed and I have been very happy with the ISP we chose, York Data Service (YDS).\nAlso during April this year was the end of support for Windows XP and Office 2003. This didn’t impact us too much, mostly due to my constant nagging of the directors to replace our oldest machines. We now only have a couple of machines still running XP and no machines running Office 2003. Most machines are now running Windows 7, I have one Director running Windows 8 for presentations and a few surveyor tablets running windows 8 as well. Like many businesses I am waiting for Windows 10 next year before doing any more substantial replacement of OS.\nIn May it was decided that I would move more into development work, so I got a new desktop PC and Visual Studio. I am very happy with my new machine (subject to dell fixing a problem later today) So far I have only dipped my toe into development work, this is largely due to needing more time to build up others to do what I do.\nIn June I made another huge achievement. I got agreement for a new server. This was our first Server 2012 server and allowed us to make huge strides towards virtualizing our services. Once we had this server up and running I was able to decommission two of our Windows Server 2003 machines well before it goes out of support next year.\nI am a huge fan of this new server, I have only used a fraction of its memory. Hyper V is really great and I hope to make progress next year into HyperV replication.\nAlso during 2014 we made huge progress with our clouds software. Within the last few weeks it is running on the Azure platform which will not only make it far easier to expand and create new features it will provide us with cost savings. I am looking forward to learning more about Azure as I get deeper into development.\nWow I achieved a lot didn’t I, on top of this I hired a new IT person and trained him up (still more to do), I have also fixed lots of printers, websites, servers (had a server motherboard die!), and made hundreds of changes to our internal databases.\nI don’t know what is in store for 2015. I expect a lots of the same with hopefully more focus on developing in Visual Studio, training myself and my department and getting the last few tweaks into our internal servers.\nI would also like to thank everyone that I have worked with over the last 12 months (you know who you are), as you made a lot of what I have just described possible.\n","date":"Dec 31, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/12/2014-Numbers-Happy-2014-Wallpaper-New-Year-Image-1024x768.jpg","permalink":"https://www.funkysi1701.com/posts/2014/looking-back-at-2014/","series":null,"tags":["Goals","Programming","Azure","ITAdmin"],"title":"Looking back at 2014"},{"categories":null,"content":"I saw this tweet on twitter.\nHe’s making a database, He’s filtering twice SELECT * FROM customers WHERE behaviour = Nice Santa Clause is Coming to town. This started me thinking surely in a normalized database structure behaviour wouldn’t be stored in the customer table, so I propose the following change.\nHe’s making a database, He’s filtering twice SELECT * FROM customers WHERE EXISTS (select * from behaviour where behaviour.CustomerId = customers.Id and behaviour.Type = Nice) Santa Clause is Coming to town. Happy Christmas everyone, hope you all have restful holidays.\n","date":"Dec 24, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/12/Merry-Christmas-Hat.gif","permalink":"https://www.funkysi1701.com/posts/2014/happy-christmas/","series":null,"tags":["SQL","Christmas"],"title":"Happy Christmas"},{"categories":null,"content":"Moving house, lack of internet and lack of inspiration has caused a lack of posts recently but hopefully more to come. Been doing some filing and found an old software developer quiz. Thought I would have a go.\nMany thanks to Keith for originally writing the quiz.\nQuestions:\nWrite a function that determines if a string starts with an upper-case letter A-Z Write a function that determines the area of a circle given the radius Add up all the values in an array of integers Given a table called \u0026ldquo;Nodes\u0026rdquo; with the following structure and sample data (see below after questions): …where ID is the primary key,andParentID references ID, complete the following: write a stored procedure to return all nodes beneath a given node ID describe how you might write a query to return all nodes at any depth below a given node ID (i.e. recursively) Write a function to get the prime numbers up to 1,000,000 You’ve been given the following code to review (below table)– what comments would you give back to the developer? ID ParentID Name Type Depth 1 NULL My Documents Folder 0 2 1 My Pictures Folder 1 3 1 My CV Document 1 4 2 Photo of me Document 2 CREATE PROCEDURE GetNode @NodeId INT AS DECLARE @ID INT, @ParentID INT, @Name NVARCHAR(255) DECLARE @Type NVARCHAR(20), @Depth INT SELECT @ID = ID FROM Nodes WHERE ID = @ID SELECT @ParentID = ParentID FROM Nodes where ID = @ID IF (EXISTS(SELECT NULL FROM Nodes WHERE ID = @ID AND Name = NULL)) SELECT @Name = \u0026#39;\u0026#39; ELSE SELECT @Name = Name FROM Nodes WHERE ID = @ID SELECT @Type = Type FROM Nodes WHERE ID = @ID SELECT @Depth = Depth FROM Nodes WHERE ID = @ID SELECT @ID, @ParentID, @Name, @Type, @Depth My Answers:\nstatic bool GetUpper(string var) { if (char.IsUpper(var[0])) { return true; } else { return false; } } static double AreaOfCircle(int radius) { double area = 0; area = Math.PI * radius * radius; return area; } static int SumArray() { int[] MyArray = new int[10] { 1, 2, 5, 12, 4, 9, 8, 18, 9, 6 }; int Sum = MyArray.Sum(); return Sum; } create procedure getnodes ( @node int ) select * from dbo.nodes where parentid = @node For a recursive query I would write something along the lines of, but it would need to be customised depending on the depth, eg more joins for higher depths\nselect * from dbo.nodes n1 join dbo.nodes n2 on n1.id = n2.ParentId join dbo.nodes n3 on n2.id = n3.ParentId where n1.parentid = 4544054 static void prime() { Console.WriteLine(\u0026#34;Prime: 1\u0026#34;); for (long i = 3; i \u0026lt;= 1000000;i++ ) { bool isprime = true; for (long j = 2; j \u0026lt;i; j++) { if(i%j==0) { isprime = false; break; } } if (isprime) { Console.WriteLine(\u0026#34;Prime: \u0026#34;+i); } } } No Brackets around parameters, @NodeId parameter never used, select @id = id from dbo.nodes where id = @id is pointless as same id that is passed it being set, Name = NULL should be Name is NULL, no from specified in last query. There are probably more issues as well.\n","date":"Dec 5, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/12/Quiz-e1354812026198.jpg","permalink":"https://www.funkysi1701.com/posts/2014/software-developer-quiz/","series":null,"tags":["Programming","Quiz",""],"title":"Software Developer Quiz"},{"categories":null,"content":"On Friday night I had the urge to fire up Visual Studio and tomorrow (Monday) I move house.\nWhat’s with that? Why am I in a programming mood when I have boxes to pack and shelves to take apart?\nFor the last few weeks I have been too busy with other things to do much programming or when I had the time I felt more like relaxing than learning anything new.\nHow do I get into the programming zone, what was it about this particular weekend that made me want to? Was it just the fact I had no time to concentrate on it, or is there some more useful factor that controls my desire to code?\nI can think of a few factors that I should make note of.\nPeace and quiet – Friday night I had the place to myself so I could concentrate and do whatever I wanted. Ideas – Before I started I had some ideas, why don’t I try doing x. Once I have a coding idea I need to investigate it and see where it goes. Break from routine – Friday night was the start of a whole week away from work, this gives me more time to forget about the daily stress of the office and think about other things. Goals – Work annoyed me on Friday and if I want to achieve my goal of working in development more then I need to work at it. I am not sure how I can put these into practise so that I spend more time on development but we will see. One thing I want to achieve in my new home is a quiet place to concentrate and think without distractions, I have a few options which I will investigate over the next few weeks.\nAnyway need to get back to packing boxes.\n","date":"Nov 2, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/11/keep-calm-and-get-in-the-zone.png","permalink":"https://www.funkysi1701.com/posts/2014/in-the-zone/","series":null,"tags":["Programming","Zone","Visual Studio"],"title":"In the Zone"},{"categories":null,"content":"I listen to the dot net rocks podcast and they ask this question every show. (If you don’t listen then you should!)\nI don’t know what my answer would be as there are so many gadgets and cool things that I like the sound of.\nI might get a surface pro 3, it’s a tablet with attachable keyboard that’s more powerful than my current laptop. But at £600 it’s expensive so maybe not. Maybe instead I would install a SSD into my existing laptop to increase disk performance.\nI’d like a new car with lots of gadgets built-in. A friend gave me a ride in his new BMW and it’s nice. Bluetooth so I don’t have to have an audio cable to listen to podcasts. Parking sensors so no chance the wife will prang it. Built in satnav etc\nWhile we are on the subject of cars eventually I’d like a google self driving car as I think Google is a better driver than me. Think about it I drive to a friends I have a drink I sleep in the back seat as my car drives me home. Once all cars are automated I believe the roads will be much safer.\nThe last thing I would spend on is an intensive training course on development, I would really like to get further in dev and more training would really help. Specifically something that would help solve my current dev problems. I am sure it is possible for me to learn dev with self-study but that requires a discipline I don’t think I have, I easily eat my time up with my current job, wife and other interests.\nI recently had £6500 to spend on improving my employers IT infrastructure. That was really hard. It sounds like a lot of money, but it really isn’t.\nOnce I started looking at the problems that I wanted to solve and what hardware I wanted I quickly used it all up and that was before I factored in license and software costs. In the end I just had to order what I wanted and hope that on a future date I could spend on other areas. I got a super powerful server that I could virtualize almost all our existing physical servers with it I needed licences, which left me just enough to get a NAS for file storage.\n","date":"Oct 26, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/10/itunes_dnr.jpg","permalink":"https://www.funkysi1701.com/posts/2014/if-you-had-5000-to-spend-on-technology-what-would-you-get/","series":null,"tags":["Servers","Spending"],"title":"If you had £5000 to spend on technology what would you get?"},{"categories":null,"content":"I have done a few interviews from both sides of the table, I’m not very good at either but thought I would have a go at answering my favourite questions.\nDescribe an IT disaster and what you did to turn it around?\nThe question is all about the turn it around bit. I’ve had lots of answers that emphasise the problem rather than what the candidate did to turn things around.\nMy answer would be: on a Saturday I was rearranging the server room and when I came to turn on our main file server and pdc it wouldn’t boot. My plans for the weekend went out the window it was all about getting this server back up. I tried the usual unplug everything and reconnect still nothing. I rang a friend to get a second opinion and between the two of us we formulated a plan of action. I then rang my director to tell her that I was having problems and what I was going to try. (keeping people informed is an essential skill)\nSo the server in question was getting power but nothing was happening during boot. The motherboard had died. What I needed to do was connect the raid card to another server so I could copy the data from it to our Nas drive (luckily I had an upgrade plan and this failure had just accelerated it)\nA few hours later the data was copying and I could breathe again. I told my director that I had fixed things but there would be minor issues on Monday.\nDescribe your strengths and weaknesses?\nIt wouldn’t be an interview without a strengths and weaknesses question. As an interviewer I want at least one weakness and I want it to actually be a weakness.\nMy strengths are my problem solving skills, I can look at a problem and investigate what is going on and find a solution. If its a technology I haven’t used before I can read up about it and find out how it works and then use it to solve the issue.\nMy weakness is my interpersonal skills, I am much better at analysing a computer issue than figuring out why one member of my staff is not performing. This is an area that is improving over the last few years I have taken on more responsibility over people and I am learning more about getting the best out of them.\nTechnical Questions\nI have never included technical questions in one of my interviews until recently and I think it is a good way to gauge ability. I often come out of an interview unsure how well that person would perform under the stresses of my job. If I have something down on paper it is a good start.\nBut the personality that comes through during the interview must also be considered as most knowledge can be taught.\nA good technical question I have used in the past is write a paragraph explaining DHCP. This question illustrates what their knowledge is like for an essential technology, but it also indicates what their writing style is like. Could the passage they have written be given to a Director or Client?\nGood IT people need both technical knowledge and the ability to communicate at all levels, I still struggle to know if someone is good or not and even sometimes if I am any good.\n","date":"Oct 23, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/10/how-not-to-ace-the-technical-interview.jpg?w=550\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2014/interview-questions/","series":null,"tags":["ITAdmin","Interview","Technical"],"title":"Interview questions"},{"categories":null,"content":"Let’s continue looking at a database schema for storing details of every Star Trek Episode. If you are new to databases, a schema is just the design of the structure of the database.\nWe have three tables, Episode, EpisodeWriter and Writer. See my last post for more details of these. It has been suggested that a slight change to this structure would enable storage of more of the creative staff.\nLets rename Writer and call it Credit, and rename EpisodeWriter and call it EpisodeCredit. Now any creative staff member involved in an episode can be stored in the Credit table. Lets alter EpisodeCredit and add an extra column called CreditType. CreditType is just a text field that stores the role that creative person had on that episode, it can be anything from Director, Writer, Actor, Science Consultant etc.\nWe now have the ability to store information relating to the episode in the episode table and any creative people in the credit table. What else can we add to the database? How about a table that can be used to record when an episode was last watched. I am probably weird but sometimes I want to watch a Star Trek episode that I like but I haven’t watched in ages.\nThe last watched table is really simple and just have a datetime field and episodeId. This can be further expanded to have a userId field if you wanted to keep track of what episodes your friends had been watching.\nAnother idea could be to tag episodes with certain themes or topics like Klingon episodes or meaning of life episodes or Kirk talks a computer to death episodes. Again this is fairly simple table containing TagName and EpisodeId.\n","date":"Oct 21, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/10/c285ab50-86a9-44c3-a0ce-d790acae7db1.jpg","permalink":"https://www.funkysi1701.com/posts/2014/to-boldly-go-where-no-sql-has-gone-before-part-2/","series":null,"tags":["StarTrek","SQL","Database","Programming"],"title":"To boldly go where no SQL has gone before Part 2"},{"categories":null,"content":"My last post proved quite popular so I am wondering if I can combine a post about IT and Star Trek.\nYears ago I used to have lists of Star Trek episodes, which included such information like original air date, production number, episode title and brief description.\nOne thing that was hard to keep track of was how many episodes were written by a specific person. This is because episodes are written by multiple people. A column called writer would then need to contain multiple people, another option would be to have columns called writer1, writer2 etc. This wouldn’t help either as you wouldn’t know which column a specific writer had been saved in.\nThe relationship between writer and episode is known as a many to many relationship. An episode can have many writers and a writer can have written many episodes. To achieve this structure in a SQL database you will need three database tables as it is not possible to create a many to many join between two tables. The first table will contain all the episodes, the second table will contain all the writers, the third table known as a junction table, will contain the relationship between the two.\nLet’s do an example so we can see how this would work. Gene Roddenberry creator of Star Trek wrote the pilot episode ‘The Cage’. So Gene would be added to the writers table with an id of 1 and The Cage would be added to the episode table with an id of 1. In the junction table, it has two columns episode and writer, so we would enter 1 and 1 into these columns.\nSelect * from Episode e Join EpisodeWriter ew on e.id = ew.EpisodeId Join Writer w on w.id = ew.WriterId But if Gene Coon and Gene Roddenberry had writing credits on The Cage we would need to add Gene Coon to the writing table and add an extra entry to the episodewriter table.\nI am going to do more posts based on this as I expand the database structure to include other information, I may go on to create stored procedures for bringing back certain information or I may use this as an example to talk about coding a user interface.\n","date":"Oct 12, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/10/enterprise.jpg","permalink":"https://www.funkysi1701.com/posts/2014/to-boldly-go-where-no-sql-has-gone-before/","series":null,"tags":["StarTrek","SQL","Database","Programming"],"title":"To boldly go where no SQL has gone before"},{"categories":null,"content":"As a break from my usual topics I am going to talk about what I did over the weekend.\nI am a huge star trek fan, so me and my wife Laura went to Destination Star Trek, a convention held in London. This is a picture of us in costume, my wife created a tribble costume (A tribble is an alien race that just consists of a ball of fur) and I am wearing an original series captains uniform. We had a great time.\nThere are several things that made this trip special.\nFriends: After the last convention I remember thinking how much better the convention would be with a group of friends. Shortly afterwards I started listening to the trekmate podcast, eventually this lead to me helping out with their website and making friends via twitter with many of the hosts. This weekend was the first time I met up with them and it was great to put faces to names (or twitter names) Writers and Directors: Star Trek has a great philosophy. In the future human beings will put their differences aside and work together to explore the galaxy, this is one reason why Star Trek is so popular even after almost 50 years. This philosophy has been created by the writers and other creative staff over the years. My favourite Star Trek film is Wrath of Khan which was directed by Nicholas Meyer, this weekend I got my DVD signed by him and was able to tell him how I still enjoyed that film over 30 years after it was made. He said that he really appreciated me saying that. Listening to his talk later on, it was fascinating to hear his insights into making my favourite film. It is the writers and directors that make our favourite characters who they are, the actors themselves aren’t allowed to ad lib, all they do is flesh them out with a bit of emotion or a mannerism, it is the writer that create our favourite lines or situations that we remember. Seven of Nine: During my teenage years I had lots of posters of Seven on my walls. Seven of Nine was the ex-borg that walked around the ship in a skin tight catsuit, but she was also highly intelligent and continued to explore what it was to be human. It was great to have my photo taken with her, even if it was rushed due to the large number of people that wanted their picture taken. Trivia: On the Friday of the convention, Laura took part in the beginners trivia challenge, she won. On Saturday I took part in the intermediate trivia challenge, I did really badly only got 3 correct, On Sunday I took part in the captain level challenge and I won. I really wasn’t expecting that, after I did so badly the day before. Thanks Marc Stamper and Destination Star Trek for organising that it was great fun, and also fun to meet the other trekkies who I had been competing against via twitter. Party!: On Saturday night we went to a party, we had Romulan Ale to drink, we had some of the actors performing songs (James Darren was particularly good), and some of the actors mingled with fans, it was great to see the actors relaxing and if you were lucky you could chat to them, I asked Vaughn Armstrong how he was enjoying the music, he loved it. This is just a taste of what I got up to, there was lots more, like seeing alien costumes, listening to talks, sitting on a mockup of the bridge. Only one way to end this post, Live Long and Prosper!\n","date":"Oct 5, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/10/DST3_Logo.jpg","permalink":"https://www.funkysi1701.com/posts/2014/destination-star-trek/","series":null,"tags":["StarTrek"],"title":"Destination Star Trek"},{"categories":null,"content":"Like many systems administrators I watched as the news came out yesterday about the latest version of windows. What would it be like? What would it be called? Would it be better or worse than previous versions?\nI was shocked when the name of the new operating system was revealed to be Windows 10. What really!? At first I though someone had tweeted a photoshopped image of the press conference, but no it was really going to be called Windows 10.\nWindows 10 is the next operating system to be released after windows 8, many had been calling it windows 9 or windows Threshold.\nMicrosoft has never had a fixed naming convention for its OSes, so should we really be that surprised at its new name.\nWindows 1.0 (1985) Windows 2.0 (1987) Windows 3.0 (1990) Windows 95 (1995) Windows 98 (1998) Windows ME (2000) Windows XP (2001) Windows Vista (2007) Windows 7 (2009) Windows 8 (2012) Windows 10 (2015) Windows 11 (2021) One reason Microsoft may have gone with with Windows 10 is because they wanted to signify that the coming Windows release would be the last “major” Windows update. What does it mean to be the last major update?\nIf you look at the list of OSes above you will see that between XP and Vista was a huge 6 years, this gap is/has caused a huge headache for IT professionals (me included). Software was written for XP because that is all that was available, when XP moved out of support earlier this year it caused loads as problems as people tried to get this software to work on more modern OSes.\nIf windows 10 is around for a long time we may end up with a similar situation to XP, in that loads of software will run only on it and cause problems if you try and port it to whatever comes next. I am only a trainee developer so I don’t know what is involved with building an OS, but I would imagine there must be things that are very difficult or almost impossible to fix without ripping them out and starting again which is what happens when a new version of an OS is written.\nOnly time will tell if Windows 10 really is the last version of windows and what will happen to the OS in the future. What ever the case I am downloading a preview so I can have a look (I want my start menu back!)\n","date":"Oct 1, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/10/windows_10_fullwidth.jpg?w=800\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/2014/windows-9-or-do-i-mean-10/","series":null,"tags":["Windows","Microsoft","ITAdmin"],"title":"Windows 9 or do I mean 10"},{"categories":null,"content":"I want to learn Development and eventually move into a development role. But what is the difference between development and what I already do?\nPut simply Development or programming is the creation of new programs, websites or databases.\nIT Operations or System Administration is the administration of existing servers, websites or databases.\nI have been doing System Administration since I started my job back in 2006. What you do as part of this role can be very simple like setting up new users or computers to something very complex like upgrading the version of Exchange that the company uses for its emails.\nAs part of my role I have done a lot of tasks that could be described as development. I have created databases and built new database structures like tables, views and stored procedures. I have also assisted the development department with testing, setting up visual studio, building websites and databases from code.\nCan I describe myself as a developer? I would say yes, I have done plenty of work that is development work.\nCan I get a job as a developer? Possibly but it would depend on the role. I have many of the skills that developers use, but I have limited knowledge in many areas. What I need to do is concentrate my efforts onto the development work I do and delegate as much administration work as I can so my knowledge can increase.\nA Buzz word in IT at the moment is DevOps. I have lost count of the number of times .NetRocks or RunAsRadio have mentioned it.\nDevOps is the integration of IT Development and Operations, it emphasises the need for the two departments to work closely together to achieve common goals. In big companies the two departments can pull against each other if you are not careful, but in my case as I do both roles, it would be very difficult for the development side of myself to blame the operations side of myself for a problem.\nThis I think puts me in a good position, I just need to learn more development and I will make a valuable addition to someone’s development team.\n","date":"Sep 27, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/09/DevOps-Cloud.jpg","permalink":"https://www.funkysi1701.com/posts/2014/what-is-the-difference-between-dev-and-ops/","series":null,"tags":["DevOps","Operations","Programming","ITAdmin"],"title":"What is the difference between Development and Operations?"},{"categories":null,"content":" You may not have heard of Nagios but it has saved my bacon quite a few times.\nNagios is an open source server monitoring application that runs on many linux flavours.\nI can’t remember exactly when I first installed nagios but I am guessing it was sometime in 2007/8. My boss gave me a book about it (which I never read) and told me to create a system to monitor the companies servers.\nNagios is not simple to set up. It relies on setting up various Hosts and services. Hosts are usually physical servers that you want to monitor and services are all the services you want to monitor. As this is a linux program all these can be configured by editing the right config file\nNagios is very flexible and can be expanded easily with the use of plugins, if you want to monitor something there is usually a plugin available. If you have a dell server running openmanage software there is even a plugin that allows the temperature of your server to be monitored.\nIf you want to monitor windows servers the use of nsclient++ is a real advantage. This is a simple client that runs as a service on your windows server. This allows nagios to track memory, cpu, disk space, performance and services, in fact almost everything that you would want to monitor.\nOver the years I have kept a close eye on Nagios and added extra checks as new services were added or problems encountered. A few years ago I dabbled with sending alerts out via SMS message and once I got a smart phone found an app to keep track of Nagios 24/7.\nBut recently I have started wondering if Nagios is the best way to monitor modern servers like 2012 or remote services like Azure. I want something that is easy to expand as your IT infrastructure expands. Something that relies on running on a linux OS requires your IT staff have a knowledge of linux and you keep that server maintained and updated.\nMy question is: Is Nagios still the best way to monitor my servers?\n","date":"Sep 24, 2014","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/09/nagios.png","permalink":"https://www.funkysi1701.com/posts/2014/i-love-nagios/","series":null,"tags":["ITAdmin","Monitoring","Servers","Nagios"],"title":"I love Nagios"},{"categories":null,"content":" My real name is Simon Foster but my on-line persona is Funky Si a nickname I was given during my university days which has somehow stuck. I am a Developer working in the North of England. I have been working in IT departments since 2006 and have a wide range of experiences. More recently in the Software Development space but also SysAdmin, Server Infrastructure, Databases and DevOps.\nIn my spare time I have created Pwned Pass a Xamarin Forms mobile app that makes use of the Have I Been Pwned? API to allow searching for breached emails and passwords. This is available from the Google Play store.\nCertifications Tech I have used .Net C# SQL Server Blazor Javascript jquery Azure Azure DevOps AWS Terraform Powershell git plus many others\u0026hellip; Recommendations Simon is an excellent developer, always looking for new methods and technology to improve existing solutions, along with his own skills. He is open to new ideas, though not afraid to share his thoughts and feedback and guide others to the best solution possible. It’s worth noting that Simon has vast knowledge on hardware and infrastructure which really does compliment his skill as a developer. - Alasdair Thomson (Head of Business Data Solutions at NPD Travel Retail)\nSimon worked for my company for a number of years and was a very valued member of staff, and later on management. He is an intelligent and conscientious worker who gives over and above on a regular basis. He leads by example and was respected by his peers and the directors of the company. I would not hesitate to recommend Simon both in terms of the quality of his work and his ethics and values in life in general. - Alison Davies (CEO at Eurosafe UK)\nIf you want a developer who really understands DevOps, then Si is your man. With a mixture of operations, support, development and management experience, he can bring a balanced skillset to a team, as well as excellent knowledge of Azure, .NET, SQL and the web. - Keith Williams (Development Manager at IRIS for Cascade HR and KashFlow Payroll)\n","date":"Jan 1, 0001","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2014/09/1922276.jpg","permalink":"https://www.funkysi1701.com/posts/about/","series":null,"tags":null,"title":"About Me"},{"categories":null,"content":"On 19th August at 9.39 pm James David Martin Foster arrived weighing 9 lb 12.\nMy initial thoughts on fatherhood can be found here . More baby photos can also be found on my instragram .\nOn 11th November 2017 at 22:58pm Edward Leonard Alan Foster arrived weighing 11 lb\n","date":"Jan 1, 0001","img":"https://storageaccountblog9f5d.blob.core.windows.net/blazor/wp-content/uploads/2015/07/james5.jpg?resize=300%2C225\u0026ssl=1","permalink":"https://www.funkysi1701.com/posts/funky-si-the-next-generation/","series":null,"tags":null,"title":"Funky Si: The Next Generation"},{"categories":null,"content":"Privacy Policy Last updated: March 31, 2022\nThis Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your information when You use the Service and tells You about Your privacy rights and how the law protects You.\nWe use Your Personal data to provide and improve the Service. By using the Service, You agree to the collection and use of information in accordance with this Privacy Policy. This Privacy Policy has been created with the help of the Privacy Policy Template.\nInterpretation and Definitions Interpretation The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.\nDefinitions For the purposes of this Privacy Policy:\nAccount means a unique account created for You to access our Service or parts of our Service.\nCompany (referred to as either \u0026quot;the Company\u0026quot;, \u0026quot;We\u0026quot;, \u0026quot;Us\u0026quot; or \u0026quot;Our\u0026quot; in this Agreement) refers to Funky Si's Blog.\nCookies are small files that are placed on Your computer, mobile device or any other device by a website, containing the details of Your browsing history on that website among its many uses.\nCountry refers to: United Kingdom\nDevice means any device that can access the Service such as a computer, a cellphone or a digital tablet.\nPersonal Data is any information that relates to an identified or identifiable individual.\nService refers to the Website.\nService Provider means any natural or legal person who processes the data on behalf of the Company. It refers to third-party companies or individuals employed by the Company to facilitate the Service, to provide the Service on behalf of the Company, to perform services related to the Service or to assist the Company in analyzing how the Service is used.\nUsage Data refers to data collected automatically, either generated by the use of the Service or from the Service infrastructure itself (for example, the duration of a page visit).\nWebsite refers to Funky Si's Blog, accessible from https://www.funkysi1701.com\nYou means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.\nCollecting and Using Your Personal Data Types of Data Collected Personal Data While using Our Service, We may ask You to provide Us with certain personally identifiable information that can be used to contact or identify You. Personally identifiable information may include, but is not limited to:\nEmail address\nFirst name and last name\nUsage Data\nUsage Data Usage Data is collected automatically when using the Service.\nUsage Data may include information such as Your Device's Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that You visit, the time and date of Your visit, the time spent on those pages, unique device identifiers and other diagnostic data.\nWhen You access the Service by or through a mobile device, We may collect certain information automatically, including, but not limited to, the type of mobile device You use, Your mobile device unique ID, the IP address of Your mobile device, Your mobile operating system, the type of mobile Internet browser You use, unique device identifiers and other diagnostic data.\nWe may also collect information that Your browser sends whenever You visit our Service or when You access the Service by or through a mobile device.\nTracking Technologies and Cookies We use Cookies and similar tracking technologies to track the activity on Our Service and store certain information. Tracking technologies used are beacons, tags, and scripts to collect and track information and to improve and analyze Our Service. The technologies We use may include:\nCookies or Browser Cookies. A cookie is a small file placed on Your Device. You can instruct Your browser to refuse all Cookies or to indicate when a Cookie is being sent. However, if You do not accept Cookies, You may not be able to use some parts of our Service. Unless you have adjusted Your browser setting so that it will refuse Cookies, our Service may use Cookies. Flash Cookies. Certain features of our Service may use local stored objects (or Flash Cookies) to collect and store information about Your preferences or Your activity on our Service. Flash Cookies are not managed by the same browser settings as those used for Browser Cookies. For more information on how You can delete Flash Cookies, please read \u0026quot;Where can I change the settings for disabling, or deleting local shared objects?\u0026quot; available at https://helpx.adobe.com/flash-player/kb/disable-local-shared-objects-flash.html#main_Where_can_I_change_the_settings_for_disabling__or_deleting_local_shared_objects_ Web Beacons. Certain sections of our Service and our emails may contain small electronic files known as web beacons (also referred to as clear gifs, pixel tags, and single-pixel gifs) that permit the Company, for example, to count users who have visited those pages or opened an email and for other related website statistics (for example, recording the popularity of a certain section and verifying system and server integrity). Cookies can be \u0026quot;Persistent\u0026quot; or \u0026quot;Session\u0026quot; Cookies. Persistent Cookies remain on Your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close Your web browser. Learn more about cookies: Use of Cookies by Free Privacy Policy.\nWe use both Session and Persistent Cookies for the purposes set out below:\nNecessary / Essential Cookies\nType: Session Cookies\nAdministered by: Us\nPurpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services.\nCookies Policy / Notice Acceptance Cookies\nType: Persistent Cookies\nAdministered by: Us\nPurpose: These Cookies identify if users have accepted the use of cookies on the Website.\nFunctionality Cookies\nType: Persistent Cookies\nAdministered by: Us\nPurpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.\nFor more information about the cookies we use and your choices regarding cookies, please visit our Cookies Policy or the Cookies section of our Privacy Policy.\nUse of Your Personal Data The Company may use Personal Data for the following purposes:\nTo provide and maintain our Service, including to monitor the usage of our Service.\nTo manage Your Account: to manage Your registration as a user of the Service. The Personal Data You provide can give You access to different functionalities of the Service that are available to You as a registered user.\nFor the performance of a contract: the development, compliance and undertaking of the purchase contract for the products, items or services You have purchased or of any other contract with Us through the Service.\nTo contact You: To contact You by email, telephone calls, SMS, or other equivalent forms of electronic communication, such as a mobile application's push notifications regarding updates or informative communications related to the functionalities, products or contracted services, including the security updates, when necessary or reasonable for their implementation.\nTo provide You with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information.\nTo manage Your requests: To attend and manage Your requests to Us.\nFor business transfers: We may use Your information to evaluate or conduct a merger, divestiture, restructuring, reorganization, dissolution, or other sale or transfer of some or all of Our assets, whether as a going concern or as part of bankruptcy, liquidation, or similar proceeding, in which Personal Data held by Us about our Service users is among the assets transferred.\nFor other purposes: We may use Your information for other purposes, such as data analysis, identifying usage trends, determining the effectiveness of our promotional campaigns and to evaluate and improve our Service, products, services, marketing and your experience.\nWe may share Your personal information in the following situations:\nWith Service Providers: We may share Your personal information with Service Providers to monitor and analyze the use of our Service, to contact You. For business transfers: We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company. With Affiliates: We may share Your information with Our affiliates, in which case we will require those affiliates to honor this Privacy Policy. Affiliates include Our parent company and any other subsidiaries, joint venture partners or other companies that We control or that are under common control with Us. With business partners: We may share Your information with Our business partners to offer You certain products, services or promotions. With other users: when You share personal information or otherwise interact in the public areas with other users, such information may be viewed by all users and may be publicly distributed outside. With Your consent: We may disclose Your personal information for any other purpose with Your consent. Retention of Your Personal Data The Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.\nThe Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.\nTransfer of Your Personal Data Your information, including Personal Data, is processed at the Company's operating offices and in any other places where the parties involved in the processing are located. It means that this information may be transferred to — and maintained on — computers located outside of Your state, province, country or other governmental jurisdiction where the data protection laws may differ than those from Your jurisdiction.\nYour consent to this Privacy Policy followed by Your submission of such information represents Your agreement to that transfer.\nThe Company will take all steps reasonably necessary to ensure that Your data is treated securely and in accordance with this Privacy Policy and no transfer of Your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of Your data and other personal information.\nDisclosure of Your Personal Data Business Transactions If the Company is involved in a merger, acquisition or asset sale, Your Personal Data may be transferred. We will provide notice before Your Personal Data is transferred and becomes subject to a different Privacy Policy.\nLaw enforcement Under certain circumstances, the Company may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).\nOther legal requirements The Company may disclose Your Personal Data in the good faith belief that such action is necessary to:\nComply with a legal obligation Protect and defend the rights or property of the Company Prevent or investigate possible wrongdoing in connection with the Service Protect the personal safety of Users of the Service or the public Protect against legal liability Security of Your Personal Data The security of Your Personal Data is important to Us, but remember that no method of transmission over the Internet, or method of electronic storage is 100% secure. While We strive to use commercially acceptable means to protect Your Personal Data, We cannot guarantee its absolute security.\nChildren's Privacy Our Service does not address anyone under the age of 13. We do not knowingly collect personally identifiable information from anyone under the age of 13. If You are a parent or guardian and You are aware that Your child has provided Us with Personal Data, please contact Us. If We become aware that We have collected Personal Data from anyone under the age of 13 without verification of parental consent, We take steps to remove that information from Our servers.\nIf We need to rely on consent as a legal basis for processing Your information and Your country requires consent from a parent, We may require Your parent's consent before We collect and use that information.\nLinks to Other Websites Our Service may contain links to other websites that are not operated by Us. If You click on a third party link, You will be directed to that third party's site. We strongly advise You to review the Privacy Policy of every site You visit.\nWe have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.\nChanges to this Privacy Policy We may update Our Privacy Policy from time to time. We will notify You of any changes by posting the new Privacy Policy on this page.\nWe will let You know via email and/or a prominent notice on Our Service, prior to the change becoming effective and update the \u0026quot;Last updated\u0026quot; date at the top of this Privacy Policy.\nYou are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.\nContact Us If you have any questions about this Privacy Policy, You can contact us:\nBy email: simon@funkysi1701.com ","date":"Jan 1, 0001","img":"https://www.funkysi1701.com/","permalink":"https://www.funkysi1701.com/posts/privacy-policy/","series":null,"tags":null,"title":"Privacy Policy"},{"categories":null,"content":"Using the data supplied by Troy Hunt and his Have I been pwned? website Pwned Pass allows you to check to see if any password has appeared in a data breach.\nFor more details about Have I been pwned? check out haveibeenpwned.com and www.troyhunt.com.\nPwned Pass is a simple Xamarin app that allows you to type in a password and tells you if it has been used in a data breach.\nTroy Hunt of Have I Been Pwned? recently added a new API to his website which allows you to search his extensive database of pwned passwords, over 306 million of them. I have simply created a Android frontend to this API.\nIt should be noted:\u0026nbsp;Do not send any password you actively use to a third-party service – even mine! I don’t log anything that you type into my app and this app makes use of the\u0026nbsp;k-anonymity feature to avoid transmitting passwords.\nAs well as using Troy’s new API I also take advantage of his existing API’s. You can search his extensive database of email addresses to see if you have been affected by a data breach all from my android app.\nPwned Pass is now available from the Google Play Store.\n","date":"Jan 1, 0001","img":"","permalink":"https://www.funkysi1701.com/posts/pwned-pass/","series":null,"tags":null,"title":"Pwned Passwords"}]